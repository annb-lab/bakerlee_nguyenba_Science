{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "import numpy.linalg as l\n",
    "import csv\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import scipy as stats\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import t\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import sem\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.special import binom\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import OrderedDict \n",
    "import seaborn as sns\n",
    "import time\n",
    "from random import random\n",
    "from random import randint\n",
    "from random import randrange\n",
    "from random import gauss\n",
    "from random import sample\n",
    "from matplotlib.lines import Line2D\n",
    "from textwrap import wrap\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import OrderedDict\n",
    "import gzip\n",
    "from itertools import compress \n",
    "from itertools import groupby\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from statsmodels.stats import diagnostic\n",
    "import statistics\n",
    "from scipy.stats import gaussian_kde\n",
    "import mpl_scatter_density\n",
    "# see https://github.com/astrofrog/mpl-scatter-density\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import Normalize \n",
    "from scipy.interpolate import interpn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Say, \"the default sans-serif font is Arial\"\n",
    "plt.rcParams['font.sans-serif'] = \"Arial\"\n",
    "# Then, \"ALWAYS use sans-serif fonts\"\n",
    "plt.rcParams['font.family'] = \"sans-serif\"\n",
    "# Set conditions for legend\n",
    "plt.rcParams['legend.title_fontsize'] = 7\n",
    "plt.rcParams['legend.fontsize'] = 7\n",
    "\n",
    "# Set conditions for labels\n",
    "plt.rcParams['xtick.labelsize'] = 7\n",
    "plt.rcParams['ytick.labelsize'] = 7\n",
    "plt.rcParams['axes.labelsize'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCUS_LIST = ['BUL2','FAS1','MKT1','NCS2','PMA1','RHO5','SCH9','WHI2','AKL1','RPI1','HSL7','SPT7','FRS1']\n",
    "\n",
    "floci = ['BUL2','FAS1','MKT1','NCS2','PMA1','RHO5','SCH9','WHI2','RPI1','AKL1']\n",
    "ploidies = ['hap','hom']\n",
    "envts = ['37C','4NQO','gu','salt','suloc','YPDA']\n",
    "tps = [7,14,28,42,49]\n",
    "\n",
    "flocnum = 10\n",
    "fwt_res = ['L883','G588','D30','H71','S234','G10','P220','L262','E102','S176']\n",
    "fmut_res = ['F883','A588','G30','L71','C234','S10','S220','S262','D102','P176']\n",
    "\n",
    "bell = 'chr07_584201_+T_YGR045C_CDS\\xa0←_coding\\xa0(96/363\\xa0nt)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define York regression without correlation function\n",
    "\n",
    "# Some notes from a while back:\n",
    "# Using York et al 2004, we can come to a more accurate estimate of the line of best fit that allows for error\n",
    "# in both Xi and Yi.\n",
    "# A key assumption of this estimation is that, were it not for error, all points would line up exactly linearly.\n",
    "# Our data doesn't really allow this assumption, but let's just let it go as a starting point.\n",
    "# We will term the observed points Xi and Yi respectively, and the least-squares-adjusted points xi and yi, which is\n",
    "# their expected values.\n",
    "\n",
    "# The model uses the equation y = a + bx, so a is the y-intercept and b is the slope.\n",
    "# a and b also will have errors on them, sigma_a^2 and sigma_b^2.\n",
    "# The general process is this:\n",
    "# 1) Use known errors in Xi and Yi to properly weight points for the calculations, along with finding any ri for\n",
    "# correlated error.\n",
    "# 2) Choose an approximate initial value of b (e.g., what normal linear regression spits out for slope).\n",
    "# 3) Use these values to find intermediate terms that are useful, like Bi, Ui, Vi, Wi, X_bar, and Y_bar.\n",
    "# 4) Iterate to find the actual b within some threshold (and the corresponding intermediate terms).\n",
    "# 5) Calculate a, then figure out xi and yi adjusted values, then figure out the errors, etc.\n",
    "\n",
    "# This may be the appropriate ri calc? Though note this was used for a ∆s plot in a past life, so not sure.\n",
    "# # Get the ri values now, which cannot be assumed to be zero in this plot\n",
    "#for i in np.arange(len(est_table)):\n",
    "#    est_table['ri'][i] = -1*np.sqrt(est_table['Xerror'][i]**2/(est_table['Xerror'][i]**2+fitness_table['1stderr'][i]**2))\n",
    "\n",
    "\n",
    "def yorkreg_nocorr(x,y,xerr,yerr,n):\n",
    "    res = linregress(x,y)\n",
    "    \n",
    "    b = res.slope\n",
    "\n",
    "    # Now prepare a table to store estimates\n",
    "    est_table = pd.DataFrame(columns=['Xi','Yi','Xerror','Yerror','wXi','wYi','alpha',\n",
    "                                     'Wi','Ui','Vi','Bi','xi','yi','ui','vi','ri'])\n",
    "    est_table['Xi'] = x\n",
    "    est_table['Yi'] = y\n",
    "    est_table['Xerror'] = xerr\n",
    "    est_table['Yerror'] = yerr\n",
    "    # assume correlation between x and y points is zero\n",
    "    est_table['ri'] = 0\n",
    "\n",
    "    # We want to make sure we're using consistent data - that things that are NaN in y are NaN in x too, and vice versa.\n",
    "    # So do that:\n",
    "    est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Yi'] = np.nan\n",
    "    est_table.at[est_table.loc[(est_table['Yi'].isnull())].index,'Xi'] = np.nan\n",
    "\n",
    "    # Now determine the weights of all these Xi and Yi\n",
    "    if est_table['Xerror'].sum() == 0 and est_table['Yerror'].sum() == 0:\n",
    "    #if len(est_table.loc[est_table['Xerror'] != 0]) > 0 and len(est_table.loc[est_table['Yerror'] != 0]) > 0:\n",
    "        est_table['wXi'] = 100000000\n",
    "        est_table['wYi'] = 100000000\n",
    "    else:\n",
    "        est_table['wXi'] = 1/(est_table['Xerror']**2)\n",
    "        est_table['wYi'] = 1/(est_table['Yerror']**2)\n",
    "        \n",
    "\n",
    "    # Calculate alpha (sqrt of the product of wXi and wYi)\n",
    "    est_table['alpha'] = np.sqrt(est_table['wXi']*est_table['wYi'])\n",
    "\n",
    "    mybs = [b]\n",
    "\n",
    "    # time to iterate b calculation until get to \"convergence\"\n",
    "    for q in np.arange(n):\n",
    "\n",
    "        # Calculate this Wi term, which is quite elaborate. Here right now, not including any ri term, though prob ought to.\n",
    "        est_table['Wi'] = (est_table['wXi']*est_table['wYi'])/(est_table['wXi']+(b**2)*est_table['wYi']-2*b*est_table['ri']*est_table['alpha'])\n",
    "\n",
    "        # Want to make sure we aren't using Wi values for points that won't exist on the plot\n",
    "        est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Wi'] = np.nan\n",
    "\n",
    "        # Calculate X_bar and Y_bar from Xi, Yi, and Wi\n",
    "        X_bar = np.nansum(est_table['Wi']*est_table['Xi'])/np.nansum(est_table['Wi'])\n",
    "        Y_bar = np.nansum(est_table['Wi']*est_table['Yi'])/np.nansum(est_table['Wi'])\n",
    "\n",
    "        # Calculate Ui, Vi, and Bi using X_bar, Y_bar, Xi, and Yi\n",
    "        est_table['Ui'] = est_table['Xi'] - X_bar\n",
    "        est_table['Vi'] = est_table['Yi'] - Y_bar\n",
    "        est_table['Bi'] = est_table['Wi']*((est_table['Ui']/est_table['wYi'])+(b*est_table['Vi']/est_table['wXi'])-((b*est_table['Ui']+est_table['Vi'])*(est_table['ri']/est_table['alpha'])))\n",
    "\n",
    "        b_array_top = est_table['Wi']*est_table['Bi']*est_table['Vi']\n",
    "        b_array_bottom = est_table['Wi']*est_table['Bi']*est_table['Ui']\n",
    "\n",
    "        b_new = np.nansum(b_array_top)/np.nansum(b_array_bottom)\n",
    "        #print(b_new)\n",
    "        mybs = np.append(mybs,b_new)\n",
    "        b = b_new\n",
    "\n",
    "    # get the y-intercept now\n",
    "    a = Y_bar - b*X_bar\n",
    "    \n",
    "    # calculate adjusted values of xi where xi = X_bar + Bi\n",
    "    est_table['xi'] = X_bar + est_table['Bi']\n",
    "    \n",
    "    # calculate x_bar and ui\n",
    "    x_bar = np.nansum(est_table['Wi']*est_table['xi'])/np.nansum(est_table['Wi'])\n",
    "    est_table['ui'] = est_table['xi'] - x_bar\n",
    "    \n",
    "    # get the sigma^2_b\n",
    "    sigma2_b = 1/np.nansum(est_table['Wi']*est_table['ui']**2)\n",
    "    \n",
    "    # get the total sum of squares to report\n",
    "    S = np.nansum(est_table['Wi']*(est_table['Yi'] - b * est_table['Xi'] - a)**2)\n",
    "    \n",
    "    return b, a, sigma2_b, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a version of the above function that returns a and S for a slope set to 1\n",
    "# Again, assumes no correlation\n",
    "\n",
    "def york_slope1(x,y,xerr,yerr,n):\n",
    "    \n",
    "    # set slope = 1\n",
    "    b = 1\n",
    "\n",
    "    # Now prepare a table to store estimates\n",
    "    est_table = pd.DataFrame(columns=['Xi','Yi','Xerror','Yerror','wXi','wYi','alpha',\n",
    "                                     'Wi','Ui','Vi','Bi','xi','yi','ui','vi','ri'])\n",
    "    est_table['Xi'] = x\n",
    "    est_table['Yi'] = y\n",
    "    est_table['Xerror'] = xerr\n",
    "    est_table['Yerror'] = yerr\n",
    "    \n",
    "    # assume correlation between x and y points is zero\n",
    "    est_table['ri'] = 0\n",
    "\n",
    "    # We want to make sure we're using consistent data - that things that are NaN in y are NaN in x too, and vice versa.\n",
    "    # So do that:\n",
    "    est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Yi'] = np.nan\n",
    "    est_table.at[est_table.loc[(est_table['Yi'].isnull())].index,'Xi'] = np.nan\n",
    "    \n",
    "    # Now determine the weights of all these Xi and Yi\n",
    "    if est_table['Xerror'].sum() == 0 and est_table['Yerror'].sum() == 0:\n",
    "    #if len(est_table.loc[est_table['Xerror'] != 0]) > 0 and len(est_table.loc[est_table['Yerror'] != 0]) > 0:\n",
    "        est_table['wXi'] = 100000000\n",
    "        est_table['wYi'] = 100000000\n",
    "    else:\n",
    "        est_table['wXi'] = 1/(est_table['Xerror']**2)\n",
    "        est_table['wYi'] = 1/(est_table['Yerror']**2)\n",
    "        \n",
    "    # Calculate alpha (sqrt of the product of wXi and wYi)\n",
    "    est_table['alpha'] = np.sqrt(est_table['wXi']*est_table['wYi'])\n",
    "\n",
    "    # Calculate this Wi term, which is quite elaborate. Here right now, not including any ri term, though prob ought to.\n",
    "    est_table['Wi'] = (est_table['wXi']*est_table['wYi'])/(est_table['wXi']+(b**2)*est_table['wYi']-2*b*est_table['ri']*est_table['alpha'])\n",
    "\n",
    "    # Want to make sure we aren't using Wi values for points that won't exist on the plot\n",
    "    est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Wi'] = np.nan\n",
    "\n",
    "    # Calculate X_bar and Y_bar from Xi, Yi, and Wi\n",
    "    X_bar = np.nansum(est_table['Wi']*est_table['Xi'])/np.nansum(est_table['Wi'])\n",
    "    Y_bar = np.nansum(est_table['Wi']*est_table['Yi'])/np.nansum(est_table['Wi'])\n",
    "\n",
    "    # Calculate Ui, Vi, and Bi using X_bar, Y_bar, Xi, and Yi\n",
    "    est_table['Ui'] = est_table['Xi'] - X_bar\n",
    "    est_table['Vi'] = est_table['Yi'] - Y_bar\n",
    "    est_table['Bi'] = est_table['Wi']*((est_table['Ui']/est_table['wYi'])+(b*est_table['Vi']/est_table['wXi'])-((b*est_table['Ui']+est_table['Vi'])*(est_table['ri']/est_table['alpha'])))\n",
    "\n",
    "    a_calc = Y_bar - b*X_bar\n",
    "    best_a = a_calc\n",
    "\n",
    "    # calculate adjusted values of xi where xi = X_bar + Bi\n",
    "    est_table['xi'] = X_bar + est_table['Bi']\n",
    "\n",
    "    # calculate x_bar and ui\n",
    "    x_bar = np.nansum(est_table['Wi']*est_table['xi'])/np.nansum(est_table['Wi'])\n",
    "    est_table['ui'] = est_table['xi'] - x_bar\n",
    "\n",
    "    # get the sigma^2_b\n",
    "    sigma2_b = 1/np.nansum(est_table['Wi']*est_table['ui']**2)\n",
    "\n",
    "    # get the sum of total least squares, S\n",
    "    S = np.nansum(est_table['Wi']*(est_table['Yi'] - b * est_table['Xi'] - a_calc)**2)\n",
    "    best_S = S\n",
    "\n",
    "    for atest in np.arange(-1,1,0.001):\n",
    "        # get the sum of total least squares, S\n",
    "        testS = np.nansum(est_table['Wi']*(est_table['Yi'] - b * est_table['Xi'] - atest)**2)\n",
    "        #Sdivn = S/(len(temp)-2)\n",
    "        if testS < best_S:\n",
    "            best_S = testS\n",
    "            best_a = atest\n",
    "    \n",
    "    return best_a, best_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a version of the above function that returns a and S for a slope set to 1\n",
    "# Again, assumes no correlation\n",
    "\n",
    "def york_slopeanyb(x,y,xerr,yerr,n,myb):\n",
    "    \n",
    "    # set slope = myb\n",
    "    b = myb\n",
    "\n",
    "    # Now prepare a table to store estimates\n",
    "    est_table = pd.DataFrame(columns=['Xi','Yi','Xerror','Yerror','wXi','wYi','alpha',\n",
    "                                     'Wi','Ui','Vi','Bi','xi','yi','ui','vi','ri'])\n",
    "    est_table['Xi'] = x\n",
    "    est_table['Yi'] = y\n",
    "    est_table['Xerror'] = xerr\n",
    "    est_table['Yerror'] = yerr\n",
    "    \n",
    "    # assume correlation between x and y points is zero\n",
    "    est_table['ri'] = 0\n",
    "\n",
    "    # We want to make sure we're using consistent data - that things that are NaN in y are NaN in x too, and vice versa.\n",
    "    # So do that:\n",
    "    est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Yi'] = np.nan\n",
    "    est_table.at[est_table.loc[(est_table['Yi'].isnull())].index,'Xi'] = np.nan\n",
    "\n",
    "    # Now determine the weights of all these Xi and Yi\n",
    "    if est_table['Xerror'].sum() == 0 and est_table['Yerror'].sum() == 0:\n",
    "    #if len(est_table.loc[est_table['Xerror'] != 0]) > 0 and len(est_table.loc[est_table['Yerror'] != 0]) > 0:\n",
    "        est_table['wXi'] = 100000000\n",
    "        est_table['wYi'] = 100000000\n",
    "    else:\n",
    "        est_table['wXi'] = 1/(est_table['Xerror']**2)\n",
    "        est_table['wYi'] = 1/(est_table['Yerror']**2)\n",
    "        \n",
    "    # Calculate alpha (sqrt of the product of wXi and wYi)\n",
    "    est_table['alpha'] = np.sqrt(est_table['wXi']*est_table['wYi'])\n",
    "\n",
    "    # Calculate this Wi term, which is quite elaborate. Here right now, not including any ri term, though prob ought to.\n",
    "    est_table['Wi'] = (est_table['wXi']*est_table['wYi'])/(est_table['wXi']+(b**2)*est_table['wYi']-2*b*est_table['ri']*est_table['alpha'])\n",
    "\n",
    "    # Want to make sure we aren't using Wi values for points that won't exist on the plot\n",
    "    est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Wi'] = np.nan\n",
    "\n",
    "    # Calculate X_bar and Y_bar from Xi, Yi, and Wi\n",
    "    X_bar = np.nansum(est_table['Wi']*est_table['Xi'])/np.nansum(est_table['Wi'])\n",
    "    Y_bar = np.nansum(est_table['Wi']*est_table['Yi'])/np.nansum(est_table['Wi'])\n",
    "\n",
    "    # Calculate Ui, Vi, and Bi using X_bar, Y_bar, Xi, and Yi\n",
    "    est_table['Ui'] = est_table['Xi'] - X_bar\n",
    "    est_table['Vi'] = est_table['Yi'] - Y_bar\n",
    "    est_table['Bi'] = est_table['Wi']*((est_table['Ui']/est_table['wYi'])+(b*est_table['Vi']/est_table['wXi'])-((b*est_table['Ui']+est_table['Vi'])*(est_table['ri']/est_table['alpha'])))\n",
    "\n",
    "    a_calc = Y_bar - b*X_bar\n",
    "    best_a = a_calc\n",
    "\n",
    "    # calculate adjusted values of xi where xi = X_bar + Bi\n",
    "    est_table['xi'] = X_bar + est_table['Bi']\n",
    "\n",
    "    # calculate x_bar and ui\n",
    "    x_bar = np.nansum(est_table['Wi']*est_table['xi'])/np.nansum(est_table['Wi'])\n",
    "    est_table['ui'] = est_table['xi'] - x_bar\n",
    "\n",
    "    # get the sigma^2_b\n",
    "    sigma2_b = 1/np.nansum(est_table['Wi']*est_table['ui']**2)\n",
    "\n",
    "    # get the sum of total least squares, S\n",
    "    S = np.nansum(est_table['Wi']*(est_table['Yi'] - b * est_table['Xi'] - a_calc)**2)\n",
    "    best_S = S\n",
    "\n",
    "    for atest in np.arange(-1,1,0.001):\n",
    "        # get the sum of total least squares, S\n",
    "        testS = np.nansum(est_table['Wi']*(est_table['Yi'] - b * est_table['Xi'] - atest)**2)\n",
    "        #Sdivn = S/(len(temp)-2)\n",
    "        if testS < best_S:\n",
    "            best_S = testS\n",
    "            best_a = atest\n",
    "    \n",
    "    return best_a, best_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a comprehensive list of barcodes\n",
    "\n",
    "# import filenames to work off of\n",
    "mypath = os.path.join('C:\\\\','Users','bakor','Dropbox','CRISPR','CRISPR_10xmer_BFA_data','20210117_6_selection_coefficients')\n",
    "filenames = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "# create a comprehensive list of barcodes, using the tech replicate-separated files\n",
    "bc_list = []\n",
    "#for f in np.arange(1):\n",
    "for f in np.arange(len(filenames)):\n",
    "    thisfilechar = filenames[f][len(filenames[f])-5:len(filenames[f])-4]\n",
    "    if thisfilechar == '1' or thisfilechar == '2':\n",
    "        templ = pd.read_csv('CRISPR_10xmer_BFA_data/20210117_6_selection_coefficients/'+filenames[f],sep='\\t')\n",
    "        templ.drop(templ.tail(8).index,inplace=True)\n",
    "        bc_list = bc_list + templ['Lineage'].tolist()\n",
    "    \n",
    "bc_list = list(OrderedDict.fromkeys(bc_list))\n",
    "\n",
    "bigt = pd.DataFrame()\n",
    "bigt['BC'] = bc_list\n",
    "\n",
    "# Map wells onto BCs\n",
    "\n",
    "bcwmap = pd.read_csv('20200901_simplemap_BCtorealwell.csv').drop(columns=['BC','BC-rc'])\n",
    "bcwmap = bcwmap.rename(columns={'BC_noATs':'BC'})\n",
    "bcwmap = bcwmap.rename(columns={'real_well':'Well'})\n",
    "\n",
    "bigt = pd.merge(bigt,bcwmap,on='BC',how='left')\n",
    "\n",
    "# Map on the full genotype info we have\n",
    "ginfo = pd.read_csv('20200107_gt4.csv')\n",
    "ginfo = ginfo.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "ginfo['full_g_bin_10'] = ginfo['full_g_bin_10'].astype(str).str.zfill(10)\n",
    "ginfo['full_g_bin_13'] = ginfo['full_g_bin_13'].astype(str).str.zfill(13)\n",
    "ginfo['full_g_bin_10_exp'] = ginfo['full_g_bin_10_exp'].astype(str).str.zfill(10)\n",
    "\n",
    "bigt = pd.merge(bigt,ginfo,how='left',on='Well')\n",
    "\n",
    "# Add in blanks column\n",
    "blankslist = pd.read_csv(\"20201104_blankslist.csv\")\n",
    "bigt = pd.merge(bigt,blankslist,on=\"Well\",how='left')\n",
    "\n",
    "# Map on fitness info for each barcode (calc across tech reps)\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        imptab = pd.read_csv('CRISPR_10xmer_BFA_data/20210117_6_selection_coefficients/selection_fa_'+ploidies[p]+'_'+envts[e]+'.txt',\n",
    "                             header=0,sep='\\t',names=['lineage','s_'+ploidies[p]+'-'+envts[e],'stderr(s)_'+ploidies[p]+'-'+envts[e],'f0_array_'+ploidies[p]+'-'+envts[e],'?_'+ploidies[p]+'-'+envts[e]])\n",
    "        imptab.drop(imptab.tail(8).index,inplace=True)\n",
    "        imptab = imptab.rename(columns={'lineage':'BC'})\n",
    "        bigt = pd.merge(bigt,imptab,on='BC',how='left')\n",
    "\n",
    "# For some reason, all values imported as text. Convert to float\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        bigt['s_'+ploidies[p]+'-'+envts[e]] = bigt['s_'+ploidies[p]+'-'+envts[e]].astype(float)\n",
    "        bigt['stderr(s)_'+ploidies[p]+'-'+envts[e]] = bigt['stderr(s)_'+ploidies[p]+'-'+envts[e]].astype(float)\n",
    "        bigt['f0_array_'+ploidies[p]+'-'+envts[e]] = bigt['f0_array_'+ploidies[p]+'-'+envts[e]].astype(float)\n",
    "        bigt['?_'+ploidies[p]+'-'+envts[e]] = bigt['?_'+ploidies[p]+'-'+envts[e]].astype(float)\n",
    "\n",
    "# normalize s values to WT (all psWT) fitnesses and remove things beyond an outlier threshold\n",
    "# don't think I need to do anything to standard errors!\n",
    "\n",
    "mywt = 'AGTCCTGGTAATTGTT' #corresponds to one of the two all psWT barcodes\n",
    "\n",
    "s_outlier_threshold = 1\n",
    "numoutliers2 = []\n",
    "for ct in np.arange(len(ploidies)):\n",
    "    for env in np.arange(len(envts)):\n",
    "        numoutlier = 0\n",
    "        col = 's_'+ploidies[ct]+'-'+envts[env]\n",
    "        col2 = 'stderr(s)_'+ploidies[ct]+'-'+envts[env]\n",
    "        # Skip the sWT normalization!! (new as of 2021.04.17)\n",
    "        #sWT = bigt.loc[(bigt['BC'] == mywt),'s_'+ploidies[ct]+'-'+envts[env]].values[0]\n",
    "        #for i in np.arange(len(bigt)):\n",
    "        #    if pd.isnull(bigt.loc[i,col]):\n",
    "        #        bigt.at[i,col] = np.nan\n",
    "        #    else:\n",
    "        #        bigt.at[i,col] = bigt.loc[i,col] - sWT\n",
    "        #s_var = np.var(bigt[col])\n",
    "        #print('s_var = '+str(s_var))\n",
    "        #print('s range = '+str(bigt[col].max() - bigt[col].min()))\n",
    "        #plt.scatter(np.arange(len(bigt)),bigt[col].sort_values(),s=5)\n",
    "        #plt.title('pre: '+col)\n",
    "        #plt.show()\n",
    "        #plt.scatter(bigt[col].sort_values(),bigt.loc[bigt[col].sort_values().index,'stderr(s)_'+ploidies[ct]+'-'+envts[env]])\n",
    "        #plt.title('pre: '+col+' vs '+'stderr(s)_'+ploidies[ct]+'-'+envts[env])\n",
    "        #plt.show()\n",
    "        #print('median error is '+str(bigt.loc[bigt[col].sort_values().index,'stderr(s)_'+ploidies[ct]+'-'+envts[env]].median()))\n",
    "\n",
    "\n",
    "        for i in np.arange(len(bigt)):\n",
    "            if bigt.loc[i,col2] > s_outlier_threshold:\n",
    "                bigt.at[i,col] = np.nan\n",
    "                bigt.at[i,col2] = np.nan\n",
    "                numoutlier = numoutlier + 1\n",
    "        numoutliers2 = numoutliers2 + [numoutlier]\n",
    "        \n",
    "\n",
    "# Now do the same import and normalization process for different tech reps of the same barcode (split out)\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for r in np.arange(1,3):\n",
    "            imptab = pd.read_csv('CRISPR_10xmer_BFA_data/20210117_6_selection_coefficients/selection_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(r)+'.txt',\n",
    "                                 header=0,sep='\\t',names=['lineage','s_'+ploidies[p]+'-'+envts[e]+'-'+str(r),'stderr(s)_'+ploidies[p]+'-'+envts[e]+'-'+str(r),'f0_array_'+ploidies[p]+'-'+envts[e]+'-'+str(r)])\n",
    "            imptab.drop(imptab.tail(8).index,inplace=True)\n",
    "            imptab = imptab.rename(columns={'lineage':'BC'})\n",
    "            bigt = pd.merge(bigt,imptab,on='BC',how='left')\n",
    "            \n",
    "# For some reason, all values imported as text. Convert to float\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for r in np.arange(1,3):\n",
    "            bigt['s_'+ploidies[p]+'-'+envts[e]+'-'+str(r)] = bigt['s_'+ploidies[p]+'-'+envts[e]+'-'+str(r)].astype(float)\n",
    "            bigt['stderr(s)_'+ploidies[p]+'-'+envts[e]+'-'+str(r)] = bigt['stderr(s)_'+ploidies[p]+'-'+envts[e]+'-'+str(r)].astype(float)\n",
    "            bigt['f0_array_'+ploidies[p]+'-'+envts[e]+'-'+str(r)] = bigt['f0_array_'+ploidies[p]+'-'+envts[e]+'-'+str(r)].astype(float)\n",
    "\n",
    "# normalize s values to WT (all psWT) fitnesses and remove things beyond an outlier threshold\n",
    "# don't think I need to do anything to standard errors!\n",
    "\n",
    "mywt = 'AGTCCTGGTAATTGTT' #corresponds to one of the two all psWT barcodes\n",
    "\n",
    "\n",
    "numoutliers2 = []\n",
    "for ct in np.arange(len(ploidies)):\n",
    "    for env in np.arange(len(envts)):\n",
    "        for r in np.arange(1,3):\n",
    "            numoutlier = 0\n",
    "            col = 's_'+ploidies[ct]+'-'+envts[env]+'-'+str(r)\n",
    "            col2 = 'stderr(s)_'+ploidies[ct]+'-'+envts[env]+'-'+str(r)\n",
    "           \n",
    "            for i in np.arange(len(bigt)):\n",
    "                if bigt.loc[i,col2] > s_outlier_threshold:\n",
    "                    bigt.at[i,col] = np.nan\n",
    "                    bigt.at[i,col2] = np.nan\n",
    "                    numoutlier = numoutlier + 1\n",
    "            numoutliers2 = numoutliers2 + [numoutlier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now get stats on what genotypes we do and do not have after excluding some\n",
    "HEADS UP TAKES A WHILE TO RUN DEPENDING ON HOW DEEP YOU GO!\n",
    "# get the unique list of genotypes rep in our dataset\n",
    "uniqueg = bigt.copy(deep=True)\n",
    "uniqueg = uniqueg.drop_duplicates(subset='full_g_bin_10').reset_index(drop=True)\n",
    "len(uniqueg)\n",
    "\n",
    "# do everything for the first-order first\n",
    "golist = []\n",
    "for l in np.arange(len(floci)):\n",
    "    golist = golist + [[l]]\n",
    "\n",
    "# convert to list of loci\n",
    "for item in np.arange(len(golist)):\n",
    "    newitem = []\n",
    "    for subitem in np.arange(len(golist[item])):\n",
    "        newitem = newitem + [floci[golist[item][subitem]]]\n",
    "    golist[item] = newitem\n",
    "\n",
    "# check how many genotypes with this combo of mutations\n",
    "# create list of genotypes for the locus combinations\n",
    "i = -1\n",
    "deccombolist = np.arange(2**(i+2))\n",
    "bincombolist = []\n",
    "for num in np.arange(len(deccombolist)):\n",
    "    bincombolist = bincombolist + [str(bin(deccombolist[num]))[2:].zfill(i+2)]\n",
    "\n",
    "binsummary = pd.DataFrame()\n",
    "for l in np.arange(i+2):\n",
    "    binsummary['locus'+str(l)] = [item[l] for item in golist]\n",
    "for g in np.arange(len(bincombolist)):\n",
    "    binsummary[bincombolist[g]] = \"\"\n",
    "\n",
    "# for each gentoype, figure out how many of that locus combo are present in the dataset\n",
    "for b in np.arange(len(bincombolist)):\n",
    "    for row in np.arange(len(binsummary)):\n",
    "        temp = uniqueg.copy(deep=True)\n",
    "        for l in np.arange(i+2):\n",
    "            temp = temp.loc[(temp[binsummary.loc[row,'locus'+str(l)]+'_g-update_bin'] == int(bincombolist[b][l]))]\n",
    "        binsummary.at[row,bincombolist[b]] = len(temp)\n",
    "\n",
    "# figure out what % of represented genotypes are in each bucket\n",
    "for b in np.arange(len(bincombolist)):\n",
    "    binsummary[bincombolist[b]+'%'] = binsummary[bincombolist[b]]/binsummary[bincombolist].sum(axis=1)\n",
    "\n",
    "# convert back to list of ints (easier to sort)\n",
    "for item in np.arange(len(golist)):\n",
    "    newitem = []\n",
    "    for subitem in np.arange(len(golist[item])):\n",
    "        newitem = newitem + [floci.index(golist[item][subitem])]\n",
    "    golist[item] = newitem\n",
    "\n",
    "export_csv = binsummary.to_csv(r'20210127_binsummary_'+str(i+2)+'.csv',index=True,header=True)\n",
    "print(i)\n",
    "\n",
    "# now do for pairwise and higher order combinations\n",
    "longlist = longlist + golist\n",
    "mod = []\n",
    "#for i in np.arange(0,1):\n",
    "for i in np.arange(len(floci)-4-1):\n",
    "    for l in np.arange(len(floci)):\n",
    "        for j in np.arange(len(golist)):\n",
    "            newguy = [golist[j],[l]]\n",
    "            newguy = [item for sublist in newguy for item in sublist]\n",
    "            mod = mod + [sorted(newguy)]\n",
    "    mod2 = []\n",
    "    for item in np.arange(len(mod)):\n",
    "        nonduplist = OrderedDict.fromkeys(mod[item])\n",
    "        if len(mod[item]) == len(nonduplist):\n",
    "            mod2 = mod2 + [mod[item]]\n",
    "    mod2 = sorted(mod2)\n",
    "    mod2 = list(mod2 for mod2,_ in groupby(mod2))\n",
    "    \n",
    "    # This is our list of a certain order i of locus combos\n",
    "    golist = copy.deepcopy(mod2)\n",
    "    mod = []\n",
    "    mod2 = []\n",
    "    \n",
    "    # convert to list of loci\n",
    "    for item in np.arange(len(golist)):\n",
    "        newitem = []\n",
    "        for subitem in np.arange(len(golist[item])):\n",
    "            newitem = newitem + [floci[golist[item][subitem]]]\n",
    "        golist[item] = newitem\n",
    "    \n",
    "    # check how many genotypes with this combo of mutations\n",
    "    # create list of genotypes for the locus combinations\n",
    "    deccombolist = np.arange(2**(i+2))\n",
    "    bincombolist = []\n",
    "    for num in np.arange(len(deccombolist)):\n",
    "        bincombolist = bincombolist + [str(bin(deccombolist[num]))[2:].zfill(i+2)]\n",
    "    \n",
    "    binsummary = pd.DataFrame()\n",
    "    for l in np.arange(i+2):\n",
    "        binsummary['locus'+str(l)] = [item[l] for item in golist]\n",
    "    for g in np.arange(len(bincombolist)):\n",
    "        binsummary[bincombolist[g]] = \"\"\n",
    "    \n",
    "    # for each gentoype, figure out how many of that locus combo are present in the dataset\n",
    "    for b in np.arange(len(bincombolist)):\n",
    "        for row in np.arange(len(binsummary)):\n",
    "            temp = uniqueg.copy(deep=True)\n",
    "            for l in np.arange(i+2):\n",
    "                temp = temp.loc[(temp[binsummary.loc[row,'locus'+str(l)]+'_g-update_bin'] == int(bincombolist[b][l]))]\n",
    "            binsummary.at[row,bincombolist[b]] = len(temp)\n",
    "    \n",
    "    # figure out what % of represented genotypes are in each bucket\n",
    "    for b in np.arange(len(bincombolist)):\n",
    "        binsummary[bincombolist[b]+'%'] = binsummary[bincombolist[b]]/binsummary[bincombolist].sum(axis=1)\n",
    "    \n",
    "    # convert back to list of ints (easier to sort)\n",
    "    for item in np.arange(len(golist)):\n",
    "        newitem = []\n",
    "        for subitem in np.arange(len(golist[item])):\n",
    "            newitem = newitem + [floci.index(golist[item][subitem])]\n",
    "        golist[item] = newitem\n",
    "    export_csv = binsummary.to_csv(r'20210127_binsummary_'+str(i+2)+'.csv',index=True,header=True)\n",
    "    print(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the FRS1, HSL7, and SPT7 mutations appear to affect fitness in different environments\n",
    "# Start with FRS1 since we should have the most data for this.\n",
    "# We want to isolate guys with the same genotype except for FRS1.\n",
    "\n",
    "bigtnona = bigt.loc[(~bigt['full_g_bin_13'].isnull())].reset_index(drop=True)\n",
    "\n",
    "bigtnona = bigtnona[~bigtnona['full_g_bin_13'].str.contains(\"2\")].reset_index(drop=True)\n",
    "\n",
    "bcperwell = pd.DataFrame(columns=['bc/well'])\n",
    "bcperwell['Well'] = list(OrderedDict.fromkeys(bigtnona['Well']))\n",
    "for i in np.arange(len(bcperwell)):\n",
    "    bcperwell.at[i,'bc/well'] = len(bigtnona.loc[(bigtnona['Well'] == bcperwell.loc[i,'Well'])])\n",
    "\n",
    "bigtnona = pd.merge(bigtnona,bcperwell.loc[(bcperwell['bc/well'] == 1)],how='inner',on='Well')\n",
    "\n",
    "#orphanloci = ['HSL7','SPT7','FRS1']\n",
    "orphanloci = ['FRS1']\n",
    "#orphanlocinum = [10,11,12]\n",
    "orphanlocinum = [12]\n",
    "\n",
    "for l in np.arange(len(orphanloci)):\n",
    "    bigtnona['full_g_bin_no'+orphanloci[l]] = bigtnona['full_g_bin_13'].str[:orphanlocinum[l]]+bigtnona['full_g_bin_13'].str[orphanlocinum[l]+1:]\n",
    "\n",
    "    bigtnonawt = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'WT')].reset_index(drop=True)\n",
    "    bigtnonamut = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'Mut')].reset_index(drop=True)\n",
    "\n",
    "    bigtjoin = pd.merge(bigtnonawt,bigtnonamut,how='inner',on='full_g_bin_no'+orphanloci[l])\n",
    "    print(orphanloci[l]+': length of join table is '+ str(len(bigtjoin)))\n",
    "    #print(bigtjoin['Well_x'],bigtjoin['Well_y'])\n",
    "\n",
    "    for p in np.arange(2):\n",
    "        for e in np.arange(len(envts)):\n",
    "            devtab = bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_x'] - bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_y']\n",
    "            print(orphanloci[l]+': avg diff = '+str(np.nanmean(devtab)))\n",
    "            \n",
    "            plt.errorbar(bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_x'],bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_y'],\n",
    "                         xerr=bigtjoin['stderr(s)_'+ploidies[p]+'-'+envts[e]+'_x'],yerr=bigtjoin['stderr(s)_'+ploidies[p]+'-'+envts[e]+'_y'],\n",
    "                         fmt='o')\n",
    "            plt.xlim(np.min(bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_x'])-.02,np.max(bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_x'])+.02)\n",
    "            plt.ylim(np.min(bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_y'])-0.02, np.max(bigtjoin['s_'+ploidies[p]+'-'+envts[e]+'_y'])+0.02)\n",
    "            plt.xlabel(orphanloci[l]+'WT')\n",
    "            plt.ylabel(orphanloci[l]+'Mut')\n",
    "            plt.title(ploidies[p]+'-'+envts[e])\n",
    "            plt.plot(np.linspace(-1,1),np.linspace(-1,1),color='k')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the histogram approach now\n",
    "orphanloci = ['HSL7','SPT7','FRS1','WHI2','AKL1','FAS1','SCH9']\n",
    "\n",
    "bigtnona = bigt.loc[(~bigt['full_g_bin_13'].isnull())].reset_index(drop=True)\n",
    "\n",
    "bigtnona = bigtnona.loc[(bigtnona['full_g_match?'] == True)].reset_index(drop=True)\n",
    "\n",
    "#bigtnona = bigtnona[~bigtnona['full_g_bin_13'].str.contains(\"2\")].reset_index(drop=True)\n",
    "\n",
    "for l in np.arange(len(orphanloci)):\n",
    "    if orphanloci[l] == 'HSL7':\n",
    "        wtt = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'WT')&(bigtnona['AKL1_g-update'] == 'WT')&(bigtnona['RPI1_g-update'] == 'WT')]\n",
    "        mutt = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'Mut')&(bigtnona['AKL1_g-update'] == 'WT')&(bigtnona['RPI1_g-update'] == 'WT')]\n",
    "    elif orphanloci[l] == 'SPT7':\n",
    "        wtt = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'WT')&(bigtnona['AKL1_g-update'] == 'WT')&(bigtnona['RPI1_g-update'] == 'Mut')]\n",
    "        mutt = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'Mut')&(bigtnona['AKL1_g-update'] == 'WT')&(bigtnona['RPI1_g-update'] == 'Mut')]\n",
    "    else:\n",
    "        wtt = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'WT')]\n",
    "        mutt = bigtnona.loc[(bigtnona[orphanloci[l]+'_g-update'] == 'Mut')]\n",
    "    for p in np.arange(2):\n",
    "        for e in np.arange(len(envts)):\n",
    "            binwidth=0.01\n",
    "            overdat = bigtnona['s_'+ploidies[p]+'-'+envts[e]]\n",
    "            mydat = wtt['s_'+ploidies[p]+'-'+envts[e]]\n",
    "            lmydatwt = len(mydat)\n",
    "            plt.hist(mydat,weights=np.ones(len(mydat)) / len(mydat),\n",
    "                     bins=np.arange(min(overdat), max(overdat) + binwidth, binwidth),\n",
    "                     color='xkcd:cerulean',alpha=0.4)\n",
    "            mydat = mutt['s_'+ploidies[p]+'-'+envts[e]]\n",
    "            lmydatmut = len(mydat)\n",
    "            plt.hist(mydat,weights=np.ones(len(mydat)) / len(mydat),\n",
    "                     bins=np.arange(min(overdat), max(overdat) + binwidth, binwidth),\n",
    "                     color='xkcd:orange',alpha=0.4)\n",
    "            plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "            plt.xlabel('s: WT = blue (n='+str(lmydatwt)+'), Mut = Orange (n='+str(lmydatmut)+')')\n",
    "            plt.title(orphanloci[l]+' in '+envts[e]+' for '+ploidies[p])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual code for making the techreps figure\n",
    "#Takes a while to run, because have to get all the neighbor distances (maybe half an hour)\n",
    "\n",
    "# Try doing this my own way, since not satisfied with colors yet\n",
    "# Five nearest neighbors, mean distance, scale with MinMaxScaler().\n",
    "fig_p = 1\n",
    "fig_e = 3\n",
    "\n",
    "x = bigt['s_'+ploidies[fig_p]+'-'+envts[fig_e]+'-1']\n",
    "y = bigt['s_'+ploidies[fig_p]+'-'+envts[fig_e]+'-2']\n",
    "\n",
    "mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "x = x[mask].reset_index(drop=True)\n",
    "myxerr = bigt['stderr(s)_'+ploidies[fig_p]+\"-\"+envts[fig_e]+'-1'][mask].reset_index(drop=True)\n",
    "y = y[mask].reset_index(drop=True)\n",
    "myyerr = bigt['stderr(s)_'+ploidies[fig_p]+\"-\"+envts[fig_e]+'-2'][mask].reset_index(drop=True)\n",
    "\n",
    "# Find distance to all other points\n",
    "nei = pd.DataFrame()\n",
    "nei['x'] = x\n",
    "nei['y'] = y\n",
    "nei['xerr'] = myxerr\n",
    "nei['yerr'] = myyerr\n",
    "\n",
    "#for i in np.arange(0,1):\n",
    "for i in np.arange(len(nei)):\n",
    "    focal_x = nei.loc[i,'x']\n",
    "    focal_y = nei.loc[i,'y']\n",
    "    temp = pd.DataFrame()\n",
    "    temp['dist'] = np.sqrt((focal_x-nei['x'])**2+(focal_y-nei['y'])**2)\n",
    "    temp.at[i,'dist'] = np.nan\n",
    "    # Get average distance to 5 nearest neighbors\n",
    "    nei.at[i,'dist'] = temp.sort_values(by='dist',ascending=True).reset_index(drop=True)[:5].mean().values[0]\n",
    "\n",
    "nei = nei.sort_values(by='dist',ascending=False).reset_index(drop=True)\n",
    "\n",
    "minima=min(nei['dist'])\n",
    "maxima=max(nei['dist'])\n",
    "mynorm = mcolors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "mapper = cm.ScalarMappable(norm=mynorm, cmap='viridis_r')\n",
    "\n",
    "x = nei['x']\n",
    "y = nei['y']\n",
    "myxerr = nei['xerr']\n",
    "myyerr = nei['yerr']\n",
    "\n",
    "normvals = MinMaxScaler().fit_transform(np.array(nei['dist']).reshape(-1,1)).tolist()\n",
    "nv = []\n",
    "for i in np.arange(len(normvals)):\n",
    "    nv = nv + [normvals[i][0]]\n",
    "\n",
    "mycs = []\n",
    "for v in nv:\n",
    "    mycs = mycs + [mapper.to_rgba(v)]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(1.4*0.75,1.125*0.75))    \n",
    "\n",
    "for i in np.arange(len(x)):\n",
    "    mym,myc,mybars = ax.errorbar(x[i],y[i], c=mycs[i],\n",
    "                        xerr=myxerr[i], yerr=myyerr[i],\n",
    "                        ecolor=mycs[i],elinewidth=0.5,fmt='o',ms=1,alpha=0.7)\n",
    "    [bar.set_alpha(0.2) for bar in mybars]\n",
    "\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm = mynorm), ax=ax)\n",
    "cbar.ax.set_ylabel('Density')\n",
    "cbar.set_ticks([])\n",
    "ax.plot(np.linspace(-1,1),np.linspace(-1,1),color='k',lw=0.5)\n",
    "#ax.set_xlim(np.min(x)-.02,np.max(x)+.02)\n",
    "#ax.set_ylim(np.min(y)-0.02, np.max(y)+0.02)\n",
    "ax.set_xlim(-0.7,0.03)\n",
    "ax.set_ylim(-0.7,0.03)\n",
    "ax.set_xticks([-0.5,0])\n",
    "ax.set_yticks([-0.5,0])\n",
    "ax.set_xlabel('Fitness, tech rep 1')\n",
    "ax.set_ylabel('Fitness, tech rep 2')\n",
    "#fig.savefig('msfigs/Fig1/techrepexample_'+ploidies[fig_p]+'-'+envts[fig_e]+'_density_witherr_personalmethod_condensed.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()\n",
    "#export_csv = nei.to_csv(r'20210329_techrepdistancestable.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proper regressions of these, based on York et al 2004\n",
    "\n",
    "myiter = 100\n",
    "\n",
    "techreplr = pd.DataFrame()\n",
    "row = 0\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):\n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):        \n",
    "        \n",
    "        techreplr.at[row,'ploidy'] = ploidies[p]\n",
    "        techreplr.at[row,'envt'] = envts[e]\n",
    "        \n",
    "        # get the standard linear regression slope\n",
    "        mask = ~np.isnan(bigt['s_'+ploidies[p]+'-'+envts[e]+'-1']) & ~np.isnan(bigt['s_'+ploidies[p]+'-'+envts[e]+'-2'])\n",
    "        \n",
    "        slope, intercept, r_value, p_value, std_err = linregress(bigt['s_'+ploidies[p]+'-'+envts[e]+'-1'][mask],\n",
    "                                                                 bigt['s_'+ploidies[p]+'-'+envts[e]+'-2'][mask])\n",
    "        print('slope = '+str(slope))\n",
    "        print('intercept = '+str(intercept))\n",
    "        print('r_value = '+str(r_value))\n",
    "        \n",
    "        techreplr.at[row,'dumb_slope'] = slope\n",
    "        techreplr.at[row,'dumb_intercept'] = intercept\n",
    "        techreplr.at[row,'r_value'] = r_value\n",
    "        techreplr.at[row,'r^2'] = r_value**2\n",
    "        \n",
    "        # As our first step, set b = slope\n",
    "        b = slope\n",
    "        \n",
    "        # Now prepare a table to store estimates\n",
    "        est_table = pd.DataFrame(columns=['Xi','Yi','Xerror','Yerror','wXi','wYi','alpha',\n",
    "                                         'Wi','Ui','Vi','Bi','xi','yi','ui','vi','ri'])\n",
    "        est_table['Xi'] = bigt['s_'+ploidies[p]+'-'+envts[e]+'-1']\n",
    "        est_table['Yi'] = bigt['s_'+ploidies[p]+'-'+envts[e]+'-2']\n",
    "        est_table['Xerror'] = bigt['stderr(s)_'+ploidies[p]+\"-\"+envts[e]+'-1']\n",
    "        est_table['Yerror'] = bigt['stderr(s)_'+ploidies[p]+\"-\"+envts[e]+'-2']\n",
    "        # assume correlation between x and y points is zero\n",
    "        est_table['ri'] = 0\n",
    "        \n",
    "        # We want to make sure we're using consistent data - that things that are NaN in y are NaN in x too, and vice versa.\n",
    "        # So do that:\n",
    "        est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Yi'] = np.nan\n",
    "        est_table.at[est_table.loc[(est_table['Yi'].isnull())].index,'Xi'] = np.nan\n",
    "\n",
    "        # Now determine the weights of all these Xi and Yi\n",
    "        est_table['wXi'] = 1/(est_table['Xerror']**2)\n",
    "        est_table['wYi'] = 1/(est_table['Yerror']**2)\n",
    "        \n",
    "        # Calculate alpha (sqrt of the product of wXi and wYi)\n",
    "        est_table['alpha'] = np.sqrt(est_table['wXi']*est_table['wYi'])\n",
    "        \n",
    "        mybs = [b]\n",
    "        \n",
    "        # time to iterate b calculation until get to \"convergence\"\n",
    "        for q in np.arange(myiter):\n",
    "\n",
    "            # Calculate this Wi term, which is quite elaborate. Here right now, not including any ri term, though prob ought to.\n",
    "            est_table['Wi'] = (est_table['wXi']*est_table['wYi'])/(est_table['wXi']+(b**2)*est_table['wYi']-2*b*est_table['ri']*est_table['alpha'])\n",
    "            \n",
    "            # Want to make sure we aren't using Wi values for points that won't exist on the plot\n",
    "            est_table.at[est_table.loc[(est_table['Xi'].isnull())].index,'Wi'] = np.nan\n",
    "\n",
    "            # Calculate X_bar and Y_bar from Xi, Yi, and Wi\n",
    "            X_bar = np.nansum(est_table['Wi']*est_table['Xi'])/np.nansum(est_table['Wi'])\n",
    "            Y_bar = np.nansum(est_table['Wi']*est_table['Yi'])/np.nansum(est_table['Wi'])\n",
    "\n",
    "            #print(X_bar)\n",
    "            #print(Y_bar)\n",
    "\n",
    "            # Calculate Ui, Vi, and Bi using X_bar, Y_bar, Xi, and Yi\n",
    "            est_table['Ui'] = est_table['Xi'] - X_bar\n",
    "            est_table['Vi'] = est_table['Yi'] - Y_bar\n",
    "            est_table['Bi'] = est_table['Wi']*((est_table['Ui']/est_table['wYi'])+(b*est_table['Vi']/est_table['wXi'])-((b*est_table['Ui']+est_table['Vi'])*(est_table['ri']/est_table['alpha'])))\n",
    "            \n",
    "            b_array_top = est_table['Wi']*est_table['Bi']*est_table['Vi']\n",
    "            b_array_bottom = est_table['Wi']*est_table['Bi']*est_table['Ui']\n",
    "            \n",
    "            b_new = np.nansum(b_array_top)/np.nansum(b_array_bottom)\n",
    "            #print(b_new)\n",
    "            mybs = np.append(mybs,b_new)\n",
    "            b = b_new\n",
    "\n",
    "        #plt.show()\n",
    "        #plt.plot(mybs)\n",
    "        #plt.show()\n",
    "        \n",
    "        # get the y-intercept now\n",
    "        a = Y_bar - b*X_bar\n",
    "\n",
    "        # now we want to get the variances in a and b\n",
    "        est_table['xi'] = X_bar + est_table['Bi']\n",
    "        est_table['yi'] = Y_bar + b*est_table['Bi']\n",
    "\n",
    "        # calculate x_bar and y_bar\n",
    "        x_bar = np.nansum(est_table['Wi']*est_table['xi'])/np.nansum(est_table['Wi'])\n",
    "        y_bar = np.nansum(est_table['Wi']*est_table['yi'])/np.nansum(est_table['Wi'])\n",
    "        \n",
    "        # calculate ui and vi\n",
    "        est_table['ui'] = est_table['xi'] - x_bar\n",
    "        est_table['vi'] = est_table['yi'] - y_bar\n",
    "\n",
    "        # calculate sigma_b and sigma_a\n",
    "        wu_array = est_table['Wi']*est_table['ui']**2\n",
    "        \n",
    "        sigma_b_2 = 1/np.nansum(wu_array)\n",
    "        sigma_a_2 = 1/np.nansum(est_table['Wi'])+x_bar**2*sigma_b_2\n",
    "        \n",
    "        # now calculate p-value for whether slope is same as 1 using two-sided t-test\n",
    "        S = np.nansum(est_table['Wi']*(est_table['Yi']-b*est_table['Xi']-a)**2)\n",
    "        n = len(est_table)-sum(np.isnan(est_table['Wi']))\n",
    "        stderr_slope = np.sqrt(sigma_b_2)*np.sqrt(S/(n-2))\n",
    "        o_slope = b\n",
    "        e_slope = 1\n",
    "        t_slope = (o_slope - e_slope)/stderr_slope\n",
    "        \n",
    "        #print('S = ' + str(S))\n",
    "        print('proper slope (b) = '+str(b))\n",
    "        print('proper y-int (a) = '+str(a))\n",
    "        #print('sigma_b_2 = '+str(sigma_b_2))\n",
    "        #print('sigma_a_2 = '+str(sigma_a_2))\n",
    "\n",
    "        #print('standard error of slope = ' +str(stderr_slope))\n",
    "        #print('t score = ' +str(t_slope))\n",
    "        \n",
    "        pval_slope = t.sf(np.abs(t_slope), n-1)*2\n",
    "        #pval = 1- chi2.cdf(t_slope,n-2)  # Find the p-value\n",
    "        #print ('p (slope) = ' + str(pval_slope))\n",
    "        \n",
    "        # now calculate p-value for whether y-intercept is same as 0 using two-sided t-test\n",
    "        # Use the same value of S calculated above\n",
    "        stderr_intercept = np.sqrt(sigma_a_2)*np.sqrt(S/(n-2))\n",
    "        o_intercept = a\n",
    "        e_intercept = 0\n",
    "        t_intercept = (o_intercept - e_intercept)/stderr_intercept\n",
    "        \n",
    "        #print('standard error of y-intercept = ' +str(stderr_intercept))\n",
    "        #print('t score = ' +str(t_intercept))\n",
    "        \n",
    "        pval_intercept = t.sf(np.abs(t_intercept), n-1)*2\n",
    "        #pval = 1- chi2.cdf(t_slope,n-2)  # Find the p-value\n",
    "        #print ('p (intercept) = ' + str(pval_intercept))\n",
    "        \n",
    "        # now draw pretty pictures    \n",
    "        plt.errorbar(bigt['s_'+ploidies[p]+'-'+envts[e]+'-1'],bigt['s_'+ploidies[p]+'-'+envts[e]+'-2'],\n",
    "                     xerr=bigt['stderr(s)_'+ploidies[p]+\"-\"+envts[e]+'-1'], yerr=bigt['stderr(s)_'+ploidies[p]+\"-\"+envts[e]+'-2'],\n",
    "                     fmt='o',elinewidth=0.5,color='xkcd:cerulean',alpha=0.3)\n",
    "        plt.title(ploidies[p]+'-'+envts[e])\n",
    "        plt.xlim(np.min(bigt['s_'+ploidies[p]+'-'+envts[e]+'-1'])-0.02,np.max(bigt['s_'+ploidies[p]+'-'+envts[e]+'-1'])+0.02)\n",
    "        plt.ylim(np.min(bigt['s_'+ploidies[p]+'-'+envts[e]+'-2'])-0.02, np.max(bigt['s_'+ploidies[p]+'-'+envts[e]+'-2'])+0.02)\n",
    "        plt.xlabel('tech rep 1')\n",
    "        plt.ylabel('tech rep 2')\n",
    "        plt.plot(np.linspace(-1,1),np.linspace(-1,1),color='k')\n",
    "        \n",
    "        plt.grid(linestyle = '--')\n",
    "                \n",
    "        x_fit = np.linspace(-1,1,1000)\n",
    "        #y_fit = slope*x_fit + intercept\n",
    "        #plt.plot (x_fit, y_fit,'-r')\n",
    "        y_fit_new = b*x_fit + a\n",
    "        plt.plot (x_fit,y_fit_new,'y')\n",
    "        #plt.annotate('not shown = '+ str(len(ob)), xy=((np.nanpercentile(DREarr['0s'][j],99)-np.nanpercentile(DREarr['0s'][j],1)+.06)/50+np.nanpercentile(DREarr['0s'][j],1)-.03,(np.nanpercentile(DREarr['1s'][j],99)+.03)-(np.nanpercentile(DREarr['1s'][j],99)-np.nanpercentile(DREarr['1s'][j],1)+.06)/20),color='tab:gray')\n",
    "        #plt.savefig(\"fitness_mediated_\"+celltype_envt[x]+\"-\"+mutnames[j]+\".pdf\",bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print('***************************')\n",
    "        \n",
    "        techreplr.at[row,'proper_slope'] = b\n",
    "        techreplr.at[row,'proper_intercept'] = a\n",
    "        row = row +1\n",
    "\n",
    "\n",
    "# Based on bbq paper, (1-R)/(1+R) seems like a suitable way to get a lower bound on variance explained due to measurement error\n",
    "# So add this column.\n",
    "techreplr['(1-R)/(1+R)'] = (1-techreplr['r_value']) / (1+techreplr['r_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ourselves up to compare biological replicates, since this will be best place to see brokenness\n",
    "\n",
    "# Remove wells with multiple barcodes\n",
    "welltwoplusbcs = []\n",
    "for b in np.arange(len(bigt)):\n",
    "    welltwoplusbcs = welltwoplusbcs + [len(bigt[(bigt['Well'] == bigt['Well'][b])])]\n",
    "bigt['Num BCs in well'] = welltwoplusbcs\n",
    "\n",
    "bigt1bc = bigt[(bigt['Num BCs in well'] < 2)].reset_index(drop=True)\n",
    "\n",
    "# Remove blank wells\n",
    "bigt1bc = bigt1bc.loc[(bigt1bc['blank?'] == 'no')].reset_index(drop=True)\n",
    "\n",
    "# Remove genotypes for which there is just one well\n",
    "gjustonewell = []\n",
    "for g in np.arange(len(bigt1bc)):\n",
    "    gjustonewell = gjustonewell + [len(bigt1bc[(bigt1bc['full_g_bin_10_exp'] == bigt1bc['full_g_bin_10_exp'][g])])]\n",
    "\n",
    "bigt1bc['Num wells in genotype'] = gjustonewell\n",
    "\n",
    "bigt1bc2g = bigt1bc[(bigt1bc['Num wells in genotype'] > 1)].reset_index(drop=True)\n",
    "\n",
    "# Sort the dataframe by lineage\n",
    "bigt1bc2g = bigt1bc2g.sort_values(by=['full_g_bin_10_exp']).reset_index(drop=True)\n",
    "\n",
    "# split into two dataframes, one for each barcode set\n",
    "bca = bigt1bc2g.iloc[np.arange(0,len(bigt1bc2g),2)].reset_index(drop=True)\n",
    "bcb = bigt1bc2g.iloc[np.arange(1,len(bigt1bc2g),2)].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bca vs bcb - this is the basic plots, no fancy coloring\n",
    "\n",
    "bioreplr = pd.DataFrame()\n",
    "row = 0\n",
    "\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        plt.errorbar(bca['s_'+ploidies[p]+'-'+envts[e]],bcb['s_'+ploidies[p]+'-'+envts[e]],\n",
    "                     xerr=bca['stderr(s)_'+ploidies[p]+\"-\"+envts[e]], yerr=bcb['stderr(s)_'+ploidies[p]+\"-\"+envts[e]],\n",
    "                     fmt='o',elinewidth=0.5,color='k',alpha=0.5)\n",
    "        plt.title(ploidies[p]+'-'+envts[e])\n",
    "        plt.xlim(np.min(bca['s_'+ploidies[p]+'-'+envts[e]])-.02,np.max(bca['s_'+ploidies[p]+'-'+envts[e]])+.02)\n",
    "        plt.ylim(np.min(bcb['s_'+ploidies[p]+'-'+envts[e]])-0.02, np.max(bcb['s_'+ploidies[p]+'-'+envts[e]])+0.02)\n",
    "        plt.xlabel('bc A')\n",
    "        plt.ylabel('bc B')\n",
    "        plt.plot(np.linspace(-1,1),np.linspace(-1,1),color='k')\n",
    "        \n",
    "        # get the standard linear regression slope\n",
    "        mask = ~np.isnan(bca['s_'+ploidies[p]+'-'+envts[e]]) & ~np.isnan(bcb['s_'+ploidies[p]+'-'+envts[e]])\n",
    "        \n",
    "        slope, intercept, r_value, p_value, std_err = linregress(bca['s_'+ploidies[p]+'-'+envts[e]][mask],\n",
    "                                                                 bcb['s_'+ploidies[p]+'-'+envts[e]][mask])\n",
    "        print('slope = '+str(slope))\n",
    "        print('intercept = '+str(intercept))\n",
    "        print('r_value = '+str(r_value))\n",
    "        \n",
    "        bioreplr.at[row,'ploidy'] = ploidies[p]\n",
    "        bioreplr.at[row,'envt'] = envts[e]\n",
    "        bioreplr.at[row,'dumb_slope'] = slope\n",
    "        bioreplr.at[row,'dumb_intercept'] = intercept\n",
    "        bioreplr.at[row,'r_value'] = r_value\n",
    "        bioreplr.at[row,'r^2'] = r_value**2\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        row = row +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a condensed version of the same bioreps figure\n",
    "# Actual code for making the bioreps figure\n",
    "fig_p = 1\n",
    "fig_e = 3\n",
    "\n",
    "# Try doing this my own way, since not satisfied with colors yet\n",
    "# Five nearest neighbors, mean distance, scale with MinMaxScaler().\n",
    "x = bca['s_'+ploidies[fig_p]+'-'+envts[fig_e]]\n",
    "y = bcb['s_'+ploidies[fig_p]+'-'+envts[fig_e]]\n",
    "\n",
    "mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "x = x[mask].reset_index(drop=True)\n",
    "myxerr = bca['stderr(s)_'+ploidies[fig_p]+\"-\"+envts[fig_e]][mask].reset_index(drop=True)\n",
    "y = y[mask].reset_index(drop=True)\n",
    "myyerr = bcb['stderr(s)_'+ploidies[fig_p]+\"-\"+envts[fig_e]].reset_index(drop=True)\n",
    "\n",
    "# Find distance to all other points\n",
    "nei = pd.DataFrame()\n",
    "nei['x'] = x\n",
    "nei['y'] = y\n",
    "nei['xerr'] = myxerr\n",
    "nei['yerr'] = myyerr\n",
    "\n",
    "#for i in np.arange(0,1):\n",
    "for i in np.arange(len(nei)):\n",
    "    focal_x = nei.loc[i,'x']\n",
    "    focal_y = nei.loc[i,'y']\n",
    "    temp = pd.DataFrame()\n",
    "    temp['dist'] = np.sqrt((focal_x-nei['x'])**2+(focal_y-nei['y'])**2)\n",
    "    temp.at[i,'dist'] = np.nan\n",
    "    # Get average distance to 5 nearest neighbors\n",
    "    nei.at[i,'dist'] = temp.sort_values(by='dist',ascending=True).reset_index(drop=True)[:5].mean().values[0]\n",
    "\n",
    "nei = nei.sort_values(by='dist',ascending=False).reset_index(drop=True)\n",
    "minima=min(nei['dist'])\n",
    "maxima=max(nei['dist'])\n",
    "mynorm = mcolors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "mapper = cm.ScalarMappable(norm=mynorm, cmap='viridis_r')\n",
    "\n",
    "x = nei['x']\n",
    "y = nei['y']\n",
    "myxerr = nei['xerr']\n",
    "myyerr = nei['yerr']\n",
    "\n",
    "normvals = MinMaxScaler().fit_transform(np.array(nei['dist']).reshape(-1,1)).tolist()\n",
    "nv = []\n",
    "for i in np.arange(len(normvals)):\n",
    "    nv = nv + [normvals[i][0]]\n",
    "\n",
    "mycs = []\n",
    "for v in nv:\n",
    "    mycs = mycs + [mapper.to_rgba(v)]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(1.4*0.75,1.125*0.75))    \n",
    "\n",
    "for i in np.arange(len(x)):\n",
    "    mym,myc,mybars = ax.errorbar(x[i],y[i], c=mycs[i],\n",
    "                        xerr=myxerr[i], yerr=myyerr[i],\n",
    "                        ecolor=mycs[i],elinewidth=0.5,fmt='o',ms=1,alpha=0.7)\n",
    "    [bar.set_alpha(0.2) for bar in mybars]\n",
    "\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm = mynorm), ax=ax)\n",
    "cbar.ax.set_ylabel('Density')\n",
    "cbar.set_ticks([])\n",
    "ax.plot(np.linspace(-1,1),np.linspace(-1,1),color='k',lw=0.5)\n",
    "#ax.set_xlim(np.min(x)-.02,np.max(x)+.02)\n",
    "#ax.set_ylim(np.min(y)-0.02, np.max(y)+0.02)\n",
    "ax.set_xlim(-0.66,0.02)\n",
    "ax.set_ylim(-0.66,0.02)\n",
    "ax.set_xticks([-0.6,-0.4,-0.2,0])\n",
    "ax.set_yticks([-0.6,-0.4,-0.2,0])\n",
    "ax.set_xlabel('Fitness, bio rep 1')\n",
    "ax.set_ylabel('Fitness, bio rep 2')\n",
    "#fig.savefig('msfigs/Fig1/biorepexample_'+ploidies[fig_p]+'-'+envts[fig_e]+'_density_witherr_personalmethod_condensed.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot of R2 values for tech, bio reps\n",
    "\n",
    "#fig,ax = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True,figsize=(3,1.5),constrained_layout=True)\n",
    "#for p in np.arange(len(ploidies)):\n",
    "#    temp = techreplr.loc[techreplr['ploidy'] == ploidies[p]]\n",
    "#    ax[p].scatter(temp['r^2'],temp['envt'],s=15)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(1*0.75,1.125*0.75))\n",
    "pcolors = ['xkcd:cerulean','xkcd:orange']\n",
    "for p in np.arange(len(ploidies)):\n",
    "    temp = techreplr.loc[techreplr['ploidy'] == ploidies[p]]\n",
    "    ax.scatter(temp['r^2'],temp['envt'],color=pcolors[p],s=10,alpha=0.8)\n",
    "    ax.set_xlim(0,1.1)\n",
    "fig.gca().invert_yaxis()\n",
    "ax.set_xlabel('Tech rep R^2')\n",
    "ax.set_xticks([0,0.5,1])\n",
    "ax.set_ylabel('Environment')\n",
    "#fig.legend(ploidies,ncol=2, loc='lower left',bbox_to_anchor=(0.6,1.2),handletextpad=-0.1,frameon=False,columnspacing=0.3)\n",
    "fig.savefig('msfigs/Fig1/techrepsumm_v2.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "# Now do bio reps\n",
    "fig,ax = plt.subplots(figsize=(1*0.75,1.125*0.75))\n",
    "pcolors = ['xkcd:cerulean','xkcd:orange']\n",
    "for p in np.arange(len(ploidies)):\n",
    "    temp = bioreplr.loc[bioreplr['ploidy'] == ploidies[p]]\n",
    "    ax.scatter(temp['r^2'],temp['envt'],color=pcolors[p],s=10,alpha=0.8)\n",
    "    ax.set_xlim(0,1.1)\n",
    "fig.gca().invert_yaxis()\n",
    "ax.set_xlabel('Bio rep R^2')\n",
    "ax.set_xticks([0,0.5,1])\n",
    "ax.set_ylabel('Environment')\n",
    "#fig.legend(ploidies,ncol=1, loc='lower left',bbox_to_anchor=(0.6,0.4),handletextpad=-0.1)\n",
    "fig.savefig('msfigs/Fig1/biorepsumm_v2.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the stacked bar varexp plot tipping each plot on its side\n",
    "gowith = ['lasso_v2',11]\n",
    "\n",
    "varstore = pd.DataFrame()\n",
    "myind = 0\n",
    "\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(gowith[1]-1)+'.txt',\n",
    "                                     sep='\\t',names=['todelete','genotype','coeff','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data1 = data1.loc[data1.loc[(data1['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data1 = data1.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "        data1['numMut'] = data1[floci].sum(axis=1)\n",
    "\n",
    "        #for o in np.arange(1,2):\n",
    "        for o in np.arange(1,10+1):\n",
    "            temp = data1.loc[(data1['numMut'] == o)].sort_values('genotype',ascending=False).reset_index(drop=True)\n",
    "            vx = sum(temp['coeff']**2)\n",
    "            varstore.at[myind,'ploidy'] = ploidies[p]\n",
    "            varstore.at[myind,'envt'] = envts[e]\n",
    "            varstore.at[myind,'order'] = o\n",
    "            varstore.at[myind,'var_exp'] = vx\n",
    "            myind = myind + 1\n",
    "        \n",
    "for i in np.arange(len(varstore)):\n",
    "    varstore.at[i,'%var_exp'] = varstore.loc[i,'var_exp'] / varstore.loc[(varstore['ploidy'] == varstore.loc[i,'ploidy'])&(varstore['envt'] == varstore.loc[i,'envt'])]['var_exp'].sum()\n",
    "\n",
    "for i in np.arange(len(varstore)):\n",
    "    if i > 0 and varstore.loc[i-1,'ploidy'] == varstore.loc[i,'ploidy'] and varstore.loc[i-1,'envt'] == varstore.loc[i,'envt']: \n",
    "        varstore.at[i,'%var_exp_cum'] = varstore.loc[i,'%var_exp'] + varstore.loc[i-1,'%var_exp_cum']\n",
    "    else:\n",
    "        varstore.at[i,'%var_exp_cum'] = varstore.loc[i,'%var_exp']\n",
    "\n",
    "goodcolors = ['xkcd:rust','xkcd:orange','xkcd:gold','xkcd:kelly green','xkcd:dark sky blue','xkcd:indigo','xkcd:violet','xkcd:steel','xkcd:salmon','xkcd:dark grey']\n",
    "fig,ax = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,figsize=(2.5,2),constrained_layout=True)\n",
    "for p in np.arange(len(ploidies)):\n",
    "    temp = varstore.loc[(varstore['ploidy'] == ploidies[p])]\n",
    "    for e in np.arange(len(envts)):\n",
    "        tempe = temp.loc[(temp['envt'] == envts[e])].reset_index(drop=True)\n",
    "        my_b = 0\n",
    "        for o in np.arange(1,11):\n",
    "            ax[p].barh(e,tempe.loc[(tempe['order'] == o),'%var_exp'],left=my_b,color=goodcolors[o-1])\n",
    "            my_b = my_b + tempe.loc[(tempe['order'] == o),'%var_exp'].values[0]\n",
    "    ax[p].set_xlim(0,1.01)\n",
    "    ax[p].set_yticks([0,1,2,3,4,5])\n",
    "    ax[p].set_yticklabels(envts)\n",
    "    #ax[p].set_xticks([0,0.2,0.4,0.6,0.8,1.0])\n",
    "    ax[p].set_xticks([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "    #ax[p].set_xticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "ax[1].set_xlabel('Fraction broad sense heritability explained')\n",
    "\n",
    "leg = fig.legend(np.arange(1,11),loc='lower right',handlelength=1,ncol=5,bbox_to_anchor=(0.825,0.655),title='Order',columnspacing=0.6,handletextpad=0.2,framealpha=0.9)\n",
    "leg._legend_box.align = 'left'\n",
    "fig.text(-0.1,0.47,'Environment',rotation='vertical',fontsize=7)\n",
    "fig.text(-0.04,0.77,'hap',rotation='vertical',fontsize=7)\n",
    "fig.text(-0.04,0.34,'hom',rotation='vertical',fontsize=7)\n",
    "fig.gca().invert_yaxis()\n",
    "fig.savefig('msfigs/Fig2/varexp_stackedbarh.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a version of 2E where we flip the axes of the cumulative plot instead\n",
    "fig,ax = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,figsize=(1.5,3),constrained_layout=True)\n",
    "for p in np.arange(len(ploidies)):\n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "        temp = varstoreall.loc[(varstoreall['ploidy'] == ploidies[p])&(varstoreall['envt'] == envts[e])].sort_values(by='%var_exp',ascending=False).reset_index(drop=True)\n",
    "        my_b = 0\n",
    "        for i in np.arange(len(temp)): \n",
    "            if temp.loc[i,'%var_exp'] > 0:\n",
    "                my_b = my_b + temp.loc[i,'%var_exp']\n",
    "                temp.at[i,'cumu'] = my_b\n",
    "            else:\n",
    "                break\n",
    "        temp = temp.loc[temp['cumu'] >= 0]\n",
    "        ax[p].plot(temp['cumu'],np.arange(len(temp)),alpha=0.7)\n",
    "    ax[p].set_xlim(0.5,1.05)\n",
    "    ax[p].set_xticks([0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "    ax[p].set_xticklabels(['50%','60%','70%','80%','90%','100%'],rotation='vertical',fontsize=7)\n",
    "    ax[p].set_ylim(0,75)\n",
    "    #ax[p].set_yticks([0,50,100,150])\n",
    "    #ax[p].set_yticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "leg = fig.legend(envts,loc='lower right',handlelength=0.5,bbox_to_anchor=(0.67,0.32),title='Envt',labelspacing=0.1,borderpad=0.5,handletextpad=0.4)\n",
    "leg._legend_box.align = 'left'\n",
    "fig.text(0,0.47,'# terms (by rank)',rotation='vertical',fontsize=7)\n",
    "#ax[1].set_xlabel('\\n'.join(wrap('% variance explained / total epistatic variance',25)))\n",
    "ax[1].set_xlabel('% total epistatic variance explained')\n",
    "fig.savefig('msfigs/Fig2/varexp_alleporder_cumu_flip.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "# try flipping but with these truncated axes\n",
    "arturscolors=['#D62928','#F57E20','#2DA048','#2077B5','#9268AC','#8C574C']\n",
    "fig,ax = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,figsize=(1.5,2),constrained_layout=True)\n",
    "for p in np.arange(len(ploidies)):\n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "        temp = varstoreall.loc[(varstoreall['ploidy'] == ploidies[p])&(varstoreall['envt'] == envts[e])].sort_values(by='%var_exp',ascending=False).reset_index(drop=True)\n",
    "        my_b = 0\n",
    "        for i in np.arange(len(temp)): \n",
    "            if temp.loc[i,'%var_exp'] > 0:\n",
    "                my_b = my_b + temp.loc[i,'%var_exp']\n",
    "                temp.at[i,'cumu'] = my_b\n",
    "            else:\n",
    "                break\n",
    "        temp = temp.loc[temp['cumu'] >= 0]\n",
    "        ax[p].plot(np.arange(len(temp)),temp['cumu'],alpha=0.7,color=arturscolors[e])\n",
    "    ax[p].set_ylim(0.5,1.05)\n",
    "    ax[p].set_yticks([0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "    ax[p].set_yticklabels(['50%','60%','70%','80%','90%','100%'],fontsize=7)\n",
    "    ax[p].set_xlim(0,75)\n",
    "    ax[p].set_xticks([0,10,20,30,40,50,60,70])\n",
    "    #ax[p].set_yticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "leg = fig.legend(envts,loc='lower right',ncol=2,handlelength=0.25,bbox_to_anchor=(1.14,0.65),title='Environment',labelspacing=0.1,borderpad=0.5,handletextpad=0.4,columnspacing=0.5)\n",
    "leg._legend_box.align = 'left'\n",
    "fig.text(-0.1,0.35,'\\n'.join(wrap('% total epistatic variance explained',25)),rotation='vertical',fontsize=7)\n",
    "ax[1].set_xlabel('# terms (by rank)')\n",
    "fig.savefig('msfigs/Fig2/varexp_alleporder_cumu_trunc.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a hybrid version of 2A with the two above -- observed in top right, modeled in bottom left\n",
    "\n",
    "# Takes some time to run, maybe about 10min\n",
    "\n",
    "# start with observed genotype fitnesses\n",
    "corrtab = pd.DataFrame()\n",
    "corrtab['ploidy'] = \"\"\n",
    "corrtab['envt'] = \"\"\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        corrtab[ploidies[p]+'_'+envts[e]] = \"\"\n",
    "\n",
    "o=10\n",
    "\n",
    "myind = 0\n",
    "for p1 in np.arange(len(ploidies)):\n",
    "    #print(ploidies[p1])\n",
    "    for e1 in np.arange(len(envts)):\n",
    "        #print(envts[e1])\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "        \n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "        \n",
    "        # Since we're just using the obs s, remove the pred columns\n",
    "        data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)])\n",
    "        \n",
    "        # Average genotypes, propagating error\n",
    "        glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "        \n",
    "        data2 = pd.DataFrame()\n",
    "        \n",
    "        for g in np.arange(len(glist)):\n",
    "            tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "            if len(tempg) == 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs']\n",
    "                data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs-err']\n",
    "            elif len(tempg) > 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'].mean()\n",
    "                my_svar = statistics.variance(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'])\n",
    "                my_svar = 0\n",
    "                mymean_stderr = np.mean(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs-err']**2)\n",
    "                data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "        \n",
    "        data1 = data2\n",
    "        \n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "        \n",
    "        corrtab.at[myind,'ploidy'] = ploidies[p1]\n",
    "        corrtab.at[myind,'envt'] = envts[e1]\n",
    "        \n",
    "        for p2 in np.arange(len(ploidies)):\n",
    "            #print(ploidies[p2])\n",
    "            for e2 in np.arange(len(envts)):\n",
    "                #print(envts[e2])\n",
    "                if p1 != p2 or e1 != e2:\n",
    "                    data2 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p2]+'_'+envts[e2]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p2]+'_'+envts[e2]+'_s-pred_'+str(o),ploidies[p2]+'_'+envts[e2]+'_s-obs',ploidies[p2]+'_'+envts[e2]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "                    # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "                    data2 = data2.loc[:data2.loc[(data2['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "                    # Binary style for genotype\n",
    "                    data2['genotype'] = data2['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "                    # Since we're just using the obs s, remove the pred columns\n",
    "                    data2 = data2.drop(columns=[ploidies[p2]+'_'+envts[e2]+'_s-pred_'+str(o)])\n",
    "\n",
    "                    # Average genotypes, propagating error\n",
    "                    glist = list(OrderedDict.fromkeys(data2['genotype']))\n",
    "\n",
    "                    data3 = pd.DataFrame()\n",
    "\n",
    "                    for g in np.arange(len(glist)):\n",
    "                        tempg = data2.loc[(data2['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                        if len(tempg) == 1:\n",
    "                            data3.at[g,'genotype'] = glist[g]\n",
    "                            data3.at[g,'s'] = tempg.loc[0,ploidies[p2]+'_'+envts[e2]+'_s-obs']\n",
    "                            data3.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p2]+'_'+envts[e2]+'_s-obs-err']\n",
    "                        elif len(tempg) > 1:\n",
    "                            data3.at[g,'genotype'] = glist[g]\n",
    "                            data3.at[g,'s'] = tempg[ploidies[p2]+'_'+envts[e2]+'_s-obs'].mean()\n",
    "                            my_svar = statistics.variance(tempg[ploidies[p2]+'_'+envts[e2]+'_s-obs'])\n",
    "                            my_svar = 0\n",
    "                            mymean_stderr = np.mean(tempg[ploidies[p2]+'_'+envts[e2]+'_s-obs-err']**2)\n",
    "                            data3.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                            #data3.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "                    data2 = data3\n",
    "\n",
    "                    # Create a column for each locus, and for the genotype with that locus removed\n",
    "                    for l in np.arange(len(floci)):\n",
    "                        data2[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "                    for l in np.arange(len(floci)):\n",
    "                        data2['without_'+floci[l]] = data2.loc[:,'genotype'].str[:l] + data2.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "                    # Merge data1 and data2\n",
    "                    data = pd.merge(data1,data2,on='genotype',how='inner')\n",
    "        \n",
    "                    res = linregress(data['s_x'],data['s_y'])\n",
    "                    corrtab.at[myind,ploidies[p2]+'_'+envts[e2]] = res.rvalue\n",
    "        myind = myind + 1\n",
    "\n",
    "corrtabhom = corrtab.copy(deep=True)\n",
    "        \n",
    "# Now spike in the modeled values\n",
    "corrtabhap = pd.DataFrame()\n",
    "corrtabhap['ploidy'] = \"\"\n",
    "corrtabhap['envt'] = \"\"\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        corrtabhap[ploidies[p]+'_'+envts[e]] = \"\"\n",
    "\n",
    "myind = 0\n",
    "for p1 in np.arange(len(ploidies)):\n",
    "    for e1 in np.arange(len(envts)):\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "        \n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "        \n",
    "        # Since we're just using the predicted s, remove the obs columns\n",
    "        data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "        \n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "        \n",
    "        corrtabhap.at[myind,'ploidy'] = ploidies[p1]\n",
    "        corrtabhap.at[myind,'envt'] = envts[e1]\n",
    "        \n",
    "        for p2 in np.arange(len(ploidies)):\n",
    "            for e2 in np.arange(len(envts)):\n",
    "                if p1 != p2 or e1 != e2:\n",
    "                    data2 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p2]+'_'+envts[e2]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p2]+'_'+envts[e2]+'_s-pred_'+str(o),ploidies[p2]+'_'+envts[e2]+'_s-obs',ploidies[p2]+'_'+envts[e2]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "                    # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "                    data2 = data2.loc[:data2.loc[(data2['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "                    # Binary style for genotype\n",
    "                    data2['genotype'] = data2['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "                    # Since we're just using the predicted s, remove the obs columns\n",
    "                    data2 = data2.drop(columns=[ploidies[p2]+'_'+envts[e2]+'_s-obs',ploidies[p2]+'_'+envts[e2]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "\n",
    "                    # Create a column for each locus, and for the genotype with that locus removed\n",
    "                    for l in np.arange(len(floci)):\n",
    "                        data2[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "                    for l in np.arange(len(floci)):\n",
    "                        data2['without_'+floci[l]] = data2.loc[:,'genotype'].str[:l] + data2.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "                    # Merge data1 and data2\n",
    "                    data = pd.merge(data1,data2,on='genotype',how='inner')\n",
    "        \n",
    "                    res = linregress(data[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)],data[ploidies[p2]+'_'+envts[e2]+'_s-pred_'+str(o)])\n",
    "                    corrtabhap.at[myind,ploidies[p2]+'_'+envts[e2]] = res.rvalue\n",
    "        myind = myind + 1\n",
    "        \n",
    "for r in np.arange(len(corrtab)):\n",
    "    for c in np.arange(len(corrtab)):\n",
    "        if r > c:\n",
    "            corrtab.at[r,corrtab.columns.tolist()[c+2]] = corrtabhap.loc[r,corrtab.columns.tolist()[c+2]]\n",
    "        \n",
    "# Start by making the one plot version of the heatmap. I think this prob will be better than 3 separate plots,\n",
    "# for both space and concision\n",
    "# put 1 into 1:1 fields\n",
    "corrtab = corrtab.fillna(1)\n",
    "hmdarr = np.asarray(corrtab.iloc[:,2:],dtype='f')\n",
    "\n",
    "mysc = 2\n",
    "mypeg = 5/6\n",
    "fig,ax = plt.subplots(figsize=(mysc,mysc*mypeg))\n",
    "\n",
    "#ax = sns.heatmap(hmdarr,xticklabels=corrtab['ploidy']+'_'+corrtab['envt'],yticklabels=corrtab.columns.to_list()[2:],center=0,cmap=\"vlag\",vmin=-1,vmax=1)\n",
    "ax = sns.heatmap(hmdarr,xticklabels=corrtab['envt'],yticklabels=corrtab['envt'],center=0,cmap=\"vlag\",vmin=-1,vmax=1) #,cbar_kws={\"ticks\":[-1,0,1]}\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "ax.hlines([6], *ax.get_xlim())\n",
    "ax.vlines([6], *ax.get_ylim())\n",
    "\n",
    "fig.text(-0.13,0.67,'hap',rotation='vertical',fontsize=7)\n",
    "fig.text(-0.13,0.3,'hom',rotation='vertical',fontsize=7)\n",
    "fig.text(0.25,-0.17,'hap',rotation='horizontal',fontsize=7)\n",
    "fig.text(0.56,-0.17,'hom',rotation='horizontal',fontsize=7)\n",
    "fig.text(0.95,0.48,'ρ',fontsize=7,style='italic')\n",
    "\n",
    "#fig.savefig('msfigs/Fig2/genotypecorr_acrossploidiesenvts_v2_observedandmodeled.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 2B\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "megas = pd.DataFrame()\n",
    "\n",
    "for e in np.arange(len(envts)):\n",
    "    for p in np.arange(len(ploidies)):\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(gowith[1]-1)+'.txt',\n",
    "                                     sep='\\t',names=['todelete','genotype','s','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data1 = data1.loc[data1.loc[(data1['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data1 = data1.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "        data1['numMut'] = data1[floci].sum(axis=1)\n",
    "        \n",
    "        data1 = data1.loc[(data1['numMut'] == 1)].sort_values('genotype',ascending=False).reset_index(drop=True)\n",
    "        data1['ploidy'] = ploidies[p]\n",
    "        data1['envt'] = envts[e]\n",
    "        data1.insert(0,'locus',floci)\n",
    "        \n",
    "        # do confidence intervals too\n",
    "        cis = pd.read_csv('CRISPR_10xmer_BFA_data/9_CI/CI_'+ploidies[p]+'_'+envts[e]+'.txt',\n",
    "                            sep='\\t',names=['numMut','genotype','lower','upper'])\n",
    "        \n",
    "        cis['genotype'] = cis['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "        \n",
    "        cis = cis.loc[(cis['numMut'] == 1)].sort_values('genotype',ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        cis = cis.drop(columns=['numMut'])\n",
    "        \n",
    "        data1 = pd.merge(data1,cis,on='genotype',how='left')\n",
    "        \n",
    "        megas = megas.append(data1).reset_index(drop=True)\n",
    "\n",
    "# get the errors ready for errobars (diffs, not absolutes)\n",
    "megas['lower_mag'] = megas['s'] - megas['lower']\n",
    "megas['upper_mag'] = megas['upper'] - megas['s']\n",
    "        \n",
    "pez = []\n",
    "for e in np.arange(len(envts)):\n",
    "    for p in np.arange(len(ploidies)):\n",
    "        pez = pez + [ploidies[p]+'_'+envts[e]]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(2.4,1.2))\n",
    "\n",
    "#offset loci\n",
    "offl = []\n",
    "for l in np.arange(len(floci)):\n",
    "    offl = offl + [-0.3+0.06*l]\n",
    "\n",
    "for l in np.arange(len(floci)):\n",
    "    ax.errorbar(np.arange(0,12)+offl[l],megas.loc[(megas['locus'] == floci[l]),'s'].tolist(),\n",
    "                yerr = [megas.loc[(megas['locus'] == floci[l]),'lower_mag'].tolist(),\n",
    "                        megas.loc[(megas['locus'] == floci[l]),'upper_mag'].tolist()],\n",
    "                alpha=0.7,linestyle='None',marker='.',ms=4)\n",
    "    #plt.show()\n",
    "leg = ax.legend(floci,bbox_to_anchor=(-0.05, 1.42), loc='upper left',ncol=5,columnspacing=0.5,handlelength=0.5,handletextpad=0.3,frameon=False)\n",
    "leg._legend_box.align = 'left'\n",
    "for line in leg.get_lines():\n",
    "    line.set_linewidth(1.0)\n",
    "ax.set_xticks(np.arange(0,12))\n",
    "ax.set_xticklabels(ploidies*6,rotation = 90)\n",
    "ax.set_xlabel('Environment',labelpad=16)\n",
    "ax.set_ylabel('Additive effect',labelpad=-2)\n",
    "ax.tick_params(axis='x',pad=1)\n",
    "ax.set_xlim(-0.5,11.5)\n",
    "\n",
    "# alternate shading for envts\n",
    "thisalpha = 0.25\n",
    "ax.autoscale(False)\n",
    "for i in np.arange(0,12,4):\n",
    "    ax.fill([i-0.5,i-0.5,i+1.5,i+1.5],[-1,1,1,-1],color='xkcd:light grey',alpha=thisalpha,edgecolor=None,zorder=0)\n",
    "\n",
    "fig.savefig('msfigs/Fig2/additive-spaghetti_v04.pdf',bbox_inches='tight',dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABC vs aBC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all plots, for si\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "o = 10\n",
    "\n",
    "yorkn = 100\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):    \n",
    "    f0,a0 = plt.subplots(nrows=len(floci), ncols=len(envts), sharex='col', sharey=False,figsize=(14*0.45,20*0.39),constrained_layout=True)\n",
    "    fjunk,ajunk = plt.subplots(nrows=len(floci), ncols=len(envts), sharex='col', sharey=False,figsize=(20,20),constrained_layout=True)\n",
    "    \n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "        \n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "        \n",
    "        # Since we're just using the obs s, remove the pred columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)])\n",
    "        \n",
    "        # Average genotypes, propagating error\n",
    "        glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "        \n",
    "        data2 = pd.DataFrame()\n",
    "        \n",
    "        for g in np.arange(len(glist)):\n",
    "            tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "            if len(tempg) == 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "                data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "            elif len(tempg) > 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "                my_svar = statistics.variance(tempg[ploidies[p]+'_'+envts[e]+'_s-obs'])\n",
    "                my_svar = 0\n",
    "                mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "                data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "            \n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data2[floci[l]] = data2.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data2['without_'+floci[l]] = data2.loc[:,'genotype'].str[:l] + data2.loc[:,'genotype'].str[l+1:]\n",
    "        \n",
    "        #for l in np.arange(9,10):\n",
    "        for l in np.arange(len(floci)):\n",
    "            abc = pd.DataFrame()\n",
    "            \n",
    "            tab0 = data2.loc[(data2[floci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "            tab1 = data2.loc[(data2[floci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "            \n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "            #temp['s_diff'] = temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'] - temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x']\n",
    "            \n",
    "            # get the x and y limits\n",
    "            ajunk[l][e].scatter(temp['s_x'],temp['s_y'])\n",
    "            xs = ajunk[l][e].get_xlim()\n",
    "            ys = ajunk[l][e].get_ylim()\n",
    "            \n",
    "            lower = min(xs[0],ys[0])\n",
    "            upper = max(xs[1],ys[1])\n",
    "            \n",
    "            # plot the data\n",
    "            markers,caps,bars = a0[l][e].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                                  xerr = temp['stderr(s)_x'],yerr = temp['stderr(s)_y'],alpha=0.7,\n",
    "                                                  linestyle='None',elinewidth=0.5,marker='.',ms=0.7)\n",
    "            \n",
    "            [bar.set_alpha(0.5) for bar in bars]\n",
    "            \n",
    "            # plot x = y\n",
    "            xlist = np.linspace(lower,upper)\n",
    "            a0[l][e].plot(xlist,xlist,c='k',lw=1,zorder=0)\n",
    "            \n",
    "            # get the regression line and plot it\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],temp['stderr(s)_x'],temp['stderr(s)_y'],yorkn)\n",
    "            a0[l][e].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:blue',alpha=1,zorder=0,lw=0.7)\n",
    "            \n",
    "            a0[l][e].set_xlim(lower,upper)\n",
    "            a0[l][e].set_ylim(lower,upper)\n",
    "            \n",
    "            # Set up some labels\n",
    "            if e == 0:\n",
    "                a0[l][e].set_ylabel(floci[l], size=8, fontweight='bold')\n",
    "            \n",
    "            if l == len(floci)-1:\n",
    "                a0[l][e].set_xlabel(envts[e], size=8, fontweight='bold')\n",
    "                \n",
    "    f0.align_ylabels()\n",
    "\n",
    "    f0.text(0.47,-0.01,'$\\phi$, S288C allele',size=8,fontweight='bold')\n",
    "\n",
    "    f0.text(-0.03, 0.5, '$\\phi$, alternate allele', va='center', rotation='vertical',size=8, fontweight = 'bold')\n",
    "\n",
    "    f0.savefig('msfigs/big-multipanel_humane_'+ploidies[p]+'.pdf',bbox_inches='tight',dpi=300)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the 2 4-figure plots we'll actually be using in our supplement\n",
    "# First, make the hap 4NQO PMA1 example, which illustrates an \"intuitive\" reversion\n",
    "\n",
    "# common plot parameters\n",
    "mysize = 1\n",
    "myalpha = 0.5\n",
    "elw = 0.5\n",
    "ealpha = 0.15\n",
    "\n",
    "yorkn=100\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "o=10\n",
    "\n",
    "for p1 in np.arange(0,1):\n",
    "#for p1 in np.arange(len(ploidies)):\n",
    "    for e1 in np.arange(1,2):\n",
    "    #for e1 in np.arange(len(envts)):\n",
    "        for l1a in np.arange(4,5):\n",
    "        #for l1a in np.arange(len(floci)):\n",
    "            # single panel\n",
    "            fig,ax = plt.subplots(nrows = 2, ncols = 2, sharex=True, sharey='row', figsize=(2.3,2))\n",
    "            plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "            data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Since we're using the observed s, remove the predicted columns\n",
    "            data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)]).reset_index(drop=True)\n",
    "\n",
    "            # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "            # represented multiple times.\n",
    "            glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "            data2 = pd.DataFrame()\n",
    "\n",
    "            for g in np.arange(len(glist)):\n",
    "                tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                if len(tempg) == 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs']\n",
    "                    data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs-err']\n",
    "                elif len(tempg) > 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'].mean()\n",
    "                    my_svar = statistics.variance(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'])\n",
    "                    my_svar = 0\n",
    "                    mymean_stderr = np.mean(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs-err']**2)\n",
    "                    data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                    #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "            data1 = data2\n",
    "\n",
    "            # Create a column for each locus, and for the genotype with that locus removed\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l1a]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "            tab1 = data1.loc[(data1[floci[l1a]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "\n",
    "            #temp = pd.merge(tab1,tab0,how='inner',on='without_'+floci[l1a])\n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l1a])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "            # DO TOP LEFT\n",
    "            markers0,caps0,bars0 = ax[0][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = temp['stderr(s)_y'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][0].get_xlim()\n",
    "            ys = ax[0][0].get_ylim()\n",
    "\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],\n",
    "                                 temp['stderr(s)_x'],temp['stderr(s)_y'],\n",
    "                                 yorkn)\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][0].set_xlim(xs)\n",
    "            ax[0][0].set_ylim(ys)\n",
    "\n",
    "            # DO TOP RIGHT\n",
    "            markers0,caps0,bars0 = ax[0][1].errorbar(temp['s_y'],temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = temp['stderr(s)_x'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][1].get_xlim()\n",
    "            ys = ax[0][1].get_ylim()\n",
    "\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_y'],temp['s_x'],\n",
    "                                 temp['stderr(s)_y'],temp['stderr(s)_x'],\n",
    "                                 yorkn)\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][1].set_xlim(xs)\n",
    "            ax[0][1].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM LEFT\n",
    "            markers0,caps0,bars0 = ax[1][0].errorbar(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][0].get_xlim()\n",
    "            ys = ax[1][0].get_ylim()\n",
    "\n",
    "            ax[1][0].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s original full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_x'],temp['s_y'] - temp['s_x'])\n",
    "            ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s original full slope = '+str(res.slope))\n",
    "\n",
    "            ax[1][0].set_xlim(xs)\n",
    "            ax[1][0].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM RIGHT\n",
    "            markers0,caps0,bars0 = ax[1][1].errorbar(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][1].get_xlim()\n",
    "            ys = ax[1][1].get_ylim()\n",
    "\n",
    "            ax[1][1].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s reversion full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_y'],temp['s_x'] - temp['s_y'])\n",
    "            ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s reversion full slope = '+str(res.slope))\n",
    "\n",
    "            #ax.set_yticks([-0.4,-0.2,0])\n",
    "            #ax.set_xticks([-0.4,-0.2,0])\n",
    "\n",
    "            ax[1][1].set_xlim(xs)\n",
    "            ax[1][1].set_ylim(ys)\n",
    "\n",
    "            # Set labels, ticks, and limits\n",
    "\n",
    "            ax[1][0].set_xlabel('\\n'.join(wrap('Fitness, PMA1 234S',9)))\n",
    "            ax[1][1].set_xlabel('\\n'.join(wrap('Fitness, PMA1 234C',9)))\n",
    "\n",
    "            ax[1][0].set_ylabel('\\n'.join(wrap('∆$\\phi$, PMA1 S234C',11)))\n",
    "            ax[1][1].set_ylabel('\\n'.join(wrap('∆$\\phi$, PMA1 C234S',11)),labelpad=5)\n",
    "\n",
    "            ax[0][0].set_ylabel('\\n'.join(wrap('Fitness, PMA1 234C',9)))\n",
    "            ax[0][1].set_ylabel('\\n'.join(wrap('Fitness, PMA1 234S',9)),labelpad=5)\n",
    "\n",
    "            for i in np.arange(2):\n",
    "                for j in np.arange(2):\n",
    "                    ax[i][j].set_xlim(-0.6,0.05)\n",
    "            \n",
    "            for i in np.arange(2):\n",
    "                ax[0][i].set_ylim(-0.6,0.05)\n",
    "                ax[1][i].set_ylim(-0.36,0.36)\n",
    "                ax[0][i].set_yticks([-0.5,0])\n",
    "                ax[1][i].set_yticks([-0.2,0,0.2])\n",
    "            \n",
    "            fig.align_ylabels()\n",
    "            \n",
    "            #fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v02.pdf',bbox_inches='tight',dpi=3000)\n",
    "            fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v03_linregressfordelta.pdf',bbox_inches='tight',dpi=3000)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            print(ploidies[p1])\n",
    "            print(envts[e1])\n",
    "            print(floci[l1a])\n",
    "            print('********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same thing except for the \"non-intuitive\" example now\n",
    "\n",
    "# common plot parameters\n",
    "mysize = 1\n",
    "myalpha = 0.5\n",
    "elw = 0.5\n",
    "ealpha = 0.15\n",
    "\n",
    "yorkn=100\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "o=10\n",
    "\n",
    "for p1 in np.arange(1,2):\n",
    "#for p1 in np.arange(len(ploidies)):\n",
    "    for e1 in np.arange(3,4):\n",
    "    #for e1 in np.arange(len(envts)):\n",
    "        for l1a in np.arange(2,3):\n",
    "        #for l1a in np.arange(len(floci)):\n",
    "            # single panel\n",
    "            fig,ax = plt.subplots(nrows = 2, ncols = 2, sharex=True, sharey='row', figsize=(2.3,2))\n",
    "            plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "            data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Since we're using the observed s, remove the predicted columns\n",
    "            data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)]).reset_index(drop=True)\n",
    "\n",
    "            # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "            # represented multiple times.\n",
    "            glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "            data2 = pd.DataFrame()\n",
    "\n",
    "            for g in np.arange(len(glist)):\n",
    "                tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                if len(tempg) == 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs']\n",
    "                    data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs-err']\n",
    "                elif len(tempg) > 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'].mean()\n",
    "                    my_svar = statistics.variance(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'])\n",
    "                    my_svar = 0\n",
    "                    mymean_stderr = np.mean(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs-err']**2)\n",
    "                    data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                    #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "            data1 = data2\n",
    "\n",
    "            # Create a column for each locus, and for the genotype with that locus removed\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l1a]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "            tab1 = data1.loc[(data1[floci[l1a]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "\n",
    "            #temp = pd.merge(tab1,tab0,how='inner',on='without_'+floci[l1a])\n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l1a])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "            # DO TOP LEFT\n",
    "            markers0,caps0,bars0 = ax[0][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = temp['stderr(s)_y'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][0].get_xlim()\n",
    "            ys = ax[0][0].get_ylim()\n",
    "\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],\n",
    "                                 temp['stderr(s)_x'],temp['stderr(s)_y'],\n",
    "                                 yorkn)\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][0].set_xlim(xs)\n",
    "            ax[0][0].set_ylim(ys)\n",
    "\n",
    "            # DO TOP RIGHT\n",
    "            markers0,caps0,bars0 = ax[0][1].errorbar(temp['s_y'],temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = temp['stderr(s)_x'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][1].get_xlim()\n",
    "            ys = ax[0][1].get_ylim()\n",
    "\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_y'],temp['s_x'],\n",
    "                                 temp['stderr(s)_y'],temp['stderr(s)_x'],\n",
    "                                 yorkn)\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][1].set_xlim(xs)\n",
    "            ax[0][1].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM LEFT\n",
    "            markers0,caps0,bars0 = ax[1][0].errorbar(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][0].get_xlim()\n",
    "            ys = ax[1][0].get_ylim()\n",
    "\n",
    "            ax[1][0].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s original full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_x'],temp['s_y'] - temp['s_x'])\n",
    "            ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s original full slope = '+str(res.slope))\n",
    "\n",
    "            ax[1][0].set_xlim(xs)\n",
    "            ax[1][0].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM RIGHT\n",
    "            markers0,caps0,bars0 = ax[1][1].errorbar(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][1].get_xlim()\n",
    "            ys = ax[1][1].get_ylim()\n",
    "\n",
    "            ax[1][1].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s reversion full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_y'],temp['s_x'] - temp['s_y'])\n",
    "            ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s reversion full slope = '+str(res.slope))\n",
    "\n",
    "            #ax.set_yticks([-0.4,-0.2,0])\n",
    "            #ax.set_xticks([-0.4,-0.2,0])\n",
    "\n",
    "            ax[1][1].set_xlim(xs)\n",
    "            ax[1][1].set_ylim(ys)\n",
    "\n",
    "            ax[1][1].set_xlim(xs)\n",
    "            ax[1][1].set_ylim(ys)\n",
    "\n",
    "            # Set labels, ticks, and limits\n",
    "\n",
    "            ax[1][0].set_xlabel('\\n'.join(wrap('Fitness, MKT1 30D',9)))\n",
    "            ax[1][1].set_xlabel('\\n'.join(wrap('Fitness, MKT1 30G',9)))\n",
    "\n",
    "            ax[1][0].set_ylabel('\\n'.join(wrap('∆$\\phi$, MKT1 D30G',11)))\n",
    "            ax[1][1].set_ylabel('\\n'.join(wrap('∆$\\phi$, MKT1 G30D',11)),labelpad=5)\n",
    "\n",
    "            ax[0][0].set_ylabel('\\n'.join(wrap('Fitness, MKT1 30G',9)))\n",
    "            ax[0][1].set_ylabel('\\n'.join(wrap('Fitness, MKT1 30D',9)),labelpad=5)\n",
    "\n",
    "            for i in np.arange(2):\n",
    "                for j in np.arange(2):\n",
    "                    ax[i][j].set_xlim(-0.7,0.02)\n",
    "            \n",
    "            for i in np.arange(2):\n",
    "                ax[0][i].set_ylim(-0.7,0.02)\n",
    "                ax[1][i].set_ylim(-0.4,0.4)\n",
    "                ax[0][i].set_yticks([-0.5,0])\n",
    "                ax[1][i].set_yticks([-0.3,0,0.3])\n",
    "            \n",
    "            fig.align_ylabels()\n",
    "            \n",
    "            #fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v02.pdf',bbox_inches='tight',dpi=3000)\n",
    "            fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v03_linregressfordelta.pdf',bbox_inches='tight',dpi=3000)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            print(ploidies[p1])\n",
    "            print(envts[e1])\n",
    "            print(floci[l1a])\n",
    "            print('********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the least squares linear regression, do hap 37 whi2\n",
    "# common plot parameters\n",
    "mysize = 1\n",
    "myalpha = 0.5\n",
    "elw = 0.5\n",
    "ealpha = 0.15\n",
    "\n",
    "yorkn=100\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "o=10\n",
    "\n",
    "for p1 in np.arange(0,1):\n",
    "#for p1 in np.arange(len(ploidies)):\n",
    "    for e1 in np.arange(0,1):\n",
    "    #for e1 in np.arange(len(envts)):\n",
    "        for l1a in np.arange(7,8):\n",
    "        #for l1a in np.arange(len(floci)):\n",
    "            # single panel\n",
    "            fig,ax = plt.subplots(nrows = 2, ncols = 2, sharex=True, sharey='row', figsize=(2.3,2))\n",
    "            plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "            data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Since we're using the observed s, remove the predicted columns\n",
    "            data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)]).reset_index(drop=True)\n",
    "\n",
    "            # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "            # represented multiple times.\n",
    "            glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "            data2 = pd.DataFrame()\n",
    "\n",
    "            for g in np.arange(len(glist)):\n",
    "                tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                if len(tempg) == 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs']\n",
    "                    data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs-err']\n",
    "                elif len(tempg) > 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'].mean()\n",
    "                    my_svar = statistics.variance(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'])\n",
    "                    my_svar = 0\n",
    "                    mymean_stderr = np.mean(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs-err']**2)\n",
    "                    data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                    #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "            data1 = data2\n",
    "\n",
    "            # Create a column for each locus, and for the genotype with that locus removed\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l1a]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "            tab1 = data1.loc[(data1[floci[l1a]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "\n",
    "            temp = pd.merge(tab1,tab0,how='inner',on='without_'+floci[l1a])\n",
    "            #temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l1a])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "            # DO TOP LEFT\n",
    "            markers0,caps0,bars0 = ax[0][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = temp['stderr(s)_y'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][0].get_xlim()\n",
    "            ys = ax[0][0].get_ylim()\n",
    "\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],\n",
    "                                 temp['stderr(s)_x'],temp['stderr(s)_y'],\n",
    "                                 yorkn)\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][0].set_xlim(xs)\n",
    "            ax[0][0].set_ylim(ys)\n",
    "\n",
    "            # DO TOP RIGHT\n",
    "            markers0,caps0,bars0 = ax[0][1].errorbar(temp['s_y'],temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = temp['stderr(s)_x'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][1].get_xlim()\n",
    "            ys = ax[0][1].get_ylim()\n",
    "\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_y'],temp['s_x'],\n",
    "                                 temp['stderr(s)_y'],temp['stderr(s)_x'],\n",
    "                                 yorkn)\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][1].set_xlim(xs)\n",
    "            ax[0][1].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM LEFT\n",
    "            markers0,caps0,bars0 = ax[1][0].errorbar(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][0].get_xlim()\n",
    "            ys = ax[1][0].get_ylim()\n",
    "\n",
    "            ax[1][0].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s original full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_x'],temp['s_y'] - temp['s_x'])\n",
    "            ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s original full slope = '+str(res.slope))\n",
    "\n",
    "            ax[1][0].set_xlim(xs)\n",
    "            ax[1][0].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM RIGHT\n",
    "            markers0,caps0,bars0 = ax[1][1].errorbar(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][1].get_xlim()\n",
    "            ys = ax[1][1].get_ylim()\n",
    "\n",
    "            ax[1][1].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s reversion full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_y'],temp['s_x'] - temp['s_y'])\n",
    "            ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s reversion full slope = '+str(res.slope))\n",
    "\n",
    "            #ax.set_yticks([-0.4,-0.2,0])\n",
    "            #ax.set_xticks([-0.4,-0.2,0])\n",
    "\n",
    "            ax[1][1].set_xlim(xs)\n",
    "            ax[1][1].set_ylim(ys)\n",
    "\n",
    "            ax[1][1].set_xlim(xs)\n",
    "            ax[1][1].set_ylim(ys)\n",
    "\n",
    "            # Set labels, ticks, and limits\n",
    "\n",
    "            ax[1][0].set_xlabel('\\n'.join(wrap('Fitness, WHI2 262S',9)))\n",
    "            ax[1][1].set_xlabel('\\n'.join(wrap('Fitness, WHI2 262L',9)))\n",
    "\n",
    "            ax[1][0].set_ylabel('\\n'.join(wrap('∆$\\phi$, WHI2 S262L',11)))\n",
    "            ax[1][1].set_ylabel('\\n'.join(wrap('∆$\\phi$, WHI2 L262S',11)),labelpad=5)\n",
    "\n",
    "            ax[0][0].set_ylabel('\\n'.join(wrap('Fitness, WHI2 262L',9)))\n",
    "            ax[0][1].set_ylabel('\\n'.join(wrap('Fitness, WHI2 262S',9)),labelpad=5)\n",
    "\n",
    "            for i in np.arange(2):\n",
    "                for j in np.arange(2):\n",
    "                    ax[i][j].set_xlim(-0.18,0.03)\n",
    "            \n",
    "            for i in np.arange(2):\n",
    "                ax[0][i].set_ylim(-0.18,0.03)\n",
    "                ax[1][i].set_ylim(-0.12,0.12)\n",
    "                ax[0][i].set_yticks([-0.1,0])\n",
    "                ax[1][i].set_yticks([-0.1,0,0.1])\n",
    "            \n",
    "            fig.align_ylabels()\n",
    "            \n",
    "            #fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v02.pdf',bbox_inches='tight',dpi=3000)\n",
    "            fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v03_linregressfordeltarev.pdf',bbox_inches='tight',dpi=3000)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            print(ploidies[p1])\n",
    "            print(envts[e1])\n",
    "            print(floci[l1a])\n",
    "            print('********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same thing except for the flat and declining example now\n",
    "# Make the 2 4-figure plots we'll actually be using in our supplement\n",
    "# First, make the hap 4NQO MKT1 example, which illustrates an \"intuitive\" reversion\n",
    "# Make some example plots for the supplement, to show how we can get diverse ∆s behaviors (up and down, down and down)\n",
    "\n",
    "# common plot parameters\n",
    "mysize = 1\n",
    "myalpha = 0.5\n",
    "elw = 0.5\n",
    "ealpha = 0.15\n",
    "\n",
    "yorkn=100\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "o=10\n",
    "\n",
    "for p1 in np.arange(0,1):\n",
    "#for p1 in np.arange(len(ploidies)):\n",
    "    for e1 in np.arange(4,5):\n",
    "    #for e1 in np.arange(len(envts)):\n",
    "        for l1a in np.arange(0,1):\n",
    "        #for l1a in np.arange(len(floci)):\n",
    "            # single panel\n",
    "            fig,ax = plt.subplots(nrows = 2, ncols = 2, sharex=True, sharey='row', figsize=(2.3,2))\n",
    "            plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "            data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Since we're using the observed s, remove the predicted columns\n",
    "            data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)]).reset_index(drop=True)\n",
    "\n",
    "            # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "            # represented multiple times.\n",
    "            glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "            data2 = pd.DataFrame()\n",
    "\n",
    "            for g in np.arange(len(glist)):\n",
    "                tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                if len(tempg) == 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs']\n",
    "                    data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs-err']\n",
    "                elif len(tempg) > 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'].mean()\n",
    "                    my_svar = statistics.variance(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'])\n",
    "                    my_svar = 0\n",
    "                    mymean_stderr = np.mean(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs-err']**2)\n",
    "                    data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                    #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "            data1 = data2\n",
    "\n",
    "            # Create a column for each locus, and for the genotype with that locus removed\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l1a]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "            tab1 = data1.loc[(data1[floci[l1a]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "\n",
    "            #temp = pd.merge(tab1,tab0,how='inner',on='without_'+floci[l1a])\n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l1a])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "            # DO TOP LEFT\n",
    "            markers0,caps0,bars0 = ax[0][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = temp['stderr(s)_y'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][0].get_xlim()\n",
    "            ys = ax[0][0].get_ylim()\n",
    "\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],\n",
    "                                 temp['stderr(s)_x'],temp['stderr(s)_y'],\n",
    "                                 yorkn)\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][0].set_xlim(xs)\n",
    "            ax[0][0].set_ylim(ys)\n",
    "\n",
    "            # DO TOP RIGHT\n",
    "            markers0,caps0,bars0 = ax[0][1].errorbar(temp['s_y'],temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = temp['stderr(s)_x'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][1].get_xlim()\n",
    "            ys = ax[0][1].get_ylim()\n",
    "\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_y'],temp['s_x'],\n",
    "                                 temp['stderr(s)_y'],temp['stderr(s)_x'],\n",
    "                                 yorkn)\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][1].set_xlim(xs)\n",
    "            ax[0][1].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM LEFT\n",
    "            markers0,caps0,bars0 = ax[1][0].errorbar(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][0].get_xlim()\n",
    "            ys = ax[1][0].get_ylim()\n",
    "\n",
    "            ax[1][0].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s original full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_x'],temp['s_y'] - temp['s_x'])\n",
    "            ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s original full slope = '+str(res.slope))\n",
    "\n",
    "            ax[1][0].set_xlim(xs)\n",
    "            ax[1][0].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM RIGHT\n",
    "            markers0,caps0,bars0 = ax[1][1].errorbar(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][1].get_xlim()\n",
    "            ys = ax[1][1].get_ylim()\n",
    "\n",
    "            ax[1][1].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s reversion full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_y'],temp['s_x'] - temp['s_y'])\n",
    "            ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s reversion full slope = '+str(res.slope))\n",
    "\n",
    "            #ax.set_yticks([-0.4,-0.2,0])\n",
    "            #ax.set_xticks([-0.4,-0.2,0])\n",
    "\n",
    "            ax[1][1].set_xlim(xs)\n",
    "            ax[1][1].set_ylim(ys)\n",
    "\n",
    "            # Set labels, ticks, and limits\n",
    "\n",
    "            ax[1][0].set_xlabel('\\n'.join(wrap('Fitness, BUL2 883L',9)))\n",
    "            ax[1][1].set_xlabel('\\n'.join(wrap('Fitness, BUL2 883F',9)))\n",
    "\n",
    "            ax[1][0].set_ylabel('\\n'.join(wrap('∆$\\phi$, BUL2 L883F',11)))\n",
    "            ax[1][1].set_ylabel('\\n'.join(wrap('∆$\\phi$, BUL2 F883L',11)),labelpad=5)\n",
    "\n",
    "            ax[0][0].set_ylabel('\\n'.join(wrap('Fitness, BUL2 883F',9)))\n",
    "            ax[0][1].set_ylabel('\\n'.join(wrap('Fitness, BUL2 883L',9)),labelpad=5)\n",
    "\n",
    "            for i in np.arange(2):\n",
    "                for j in np.arange(2):\n",
    "                    ax[i][j].set_xlim(-0.13,0.02)\n",
    "            \n",
    "            for i in np.arange(2):\n",
    "                ax[0][i].set_ylim(-0.13,0.02)\n",
    "                ax[1][i].set_ylim(-0.07,0.07)\n",
    "                #ax[0][i].set_yticks([-0.5,0])\n",
    "                #ax[1][i].set_yticks([-0.3,0,0.3])\n",
    "            \n",
    "            fig.align_ylabels()\n",
    "            \n",
    "            #fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v02.pdf',bbox_inches='tight',dpi=3000)\n",
    "            fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v03_linregressfordelta.pdf',bbox_inches='tight',dpi=3000)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            print(ploidies[p1])\n",
    "            print(envts[e1])\n",
    "            print(floci[l1a])\n",
    "            print('********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a better version of the flat-down with the least squares linear regression now\n",
    "# Try hap ypda pma1\n",
    "\n",
    "# common plot parameters\n",
    "mysize = 1\n",
    "myalpha = 0.5\n",
    "elw = 0.5\n",
    "ealpha = 0.15\n",
    "\n",
    "yorkn=100\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "o=10\n",
    "\n",
    "for p1 in np.arange(0,1):\n",
    "#for p1 in np.arange(len(ploidies)):\n",
    "    for e1 in np.arange(5,6):\n",
    "    #for e1 in np.arange(len(envts)):\n",
    "        for l1a in np.arange(4,5):\n",
    "        #for l1a in np.arange(len(floci)):\n",
    "            # single panel\n",
    "            fig,ax = plt.subplots(nrows = 2, ncols = 2, sharex=True, sharey='row', figsize=(2.3,2))\n",
    "            plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "            data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Since we're using the observed s, remove the predicted columns\n",
    "            data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)]).reset_index(drop=True)\n",
    "\n",
    "            # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "            # represented multiple times.\n",
    "            glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "            data2 = pd.DataFrame()\n",
    "\n",
    "            for g in np.arange(len(glist)):\n",
    "                tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                if len(tempg) == 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs']\n",
    "                    data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs-err']\n",
    "                elif len(tempg) > 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'].mean()\n",
    "                    my_svar = statistics.variance(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'])\n",
    "                    my_svar = 0\n",
    "                    mymean_stderr = np.mean(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs-err']**2)\n",
    "                    data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                    #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "            data1 = data2\n",
    "\n",
    "            # Create a column for each locus, and for the genotype with that locus removed\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l1a]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "            tab1 = data1.loc[(data1[floci[l1a]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "\n",
    "            temp = pd.merge(tab1,tab0,how='inner',on='without_'+floci[l1a])\n",
    "            #temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l1a])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "            # DO TOP LEFT\n",
    "            markers0,caps0,bars0 = ax[0][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = temp['stderr(s)_y'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][0].get_xlim()\n",
    "            ys = ax[0][0].get_ylim()\n",
    "\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],\n",
    "                                 temp['stderr(s)_x'],temp['stderr(s)_y'],\n",
    "                                 yorkn)\n",
    "            ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][0].set_xlim(xs)\n",
    "            ax[0][0].set_ylim(ys)\n",
    "\n",
    "            # DO TOP RIGHT\n",
    "            markers0,caps0,bars0 = ax[0][1].errorbar(temp['s_y'],temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = temp['stderr(s)_x'],\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[0][1].get_xlim()\n",
    "            ys = ax[0][1].get_ylim()\n",
    "\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "\n",
    "            # Full regression slope\n",
    "            res = yorkreg_nocorr(temp['s_y'],temp['s_x'],\n",
    "                                 temp['stderr(s)_y'],temp['stderr(s)_x'],\n",
    "                                 yorkn)\n",
    "            ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('full slope = '+str(res[0]))\n",
    "\n",
    "            ax[0][1].set_xlim(xs)\n",
    "            ax[0][1].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM LEFT\n",
    "            markers0,caps0,bars0 = ax[1][0].errorbar(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "                        xerr = temp['stderr(s)_x'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][0].get_xlim()\n",
    "            ys = ax[1][0].get_ylim()\n",
    "\n",
    "            ax[1][0].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_x'],temp['s_y'] - temp['s_x'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s original full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_x'],temp['s_y'] - temp['s_x'])\n",
    "            ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s original full slope = '+str(res.slope))\n",
    "\n",
    "            ax[1][0].set_xlim(xs)\n",
    "            ax[1][0].set_ylim(ys)\n",
    "\n",
    "            # DO BOTTOM RIGHT\n",
    "            markers0,caps0,bars0 = ax[1][1].errorbar(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "                        xerr = temp['stderr(s)_y'],\n",
    "                        yerr = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "                        alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "            [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "            xs = ax[1][1].get_xlim()\n",
    "            ys = ax[1][1].get_ylim()\n",
    "\n",
    "            ax[1][1].axhline(y=0,color='xkcd:grey',lw=0.5)\n",
    "\n",
    "            # Full regression slope\n",
    "            # for v02\n",
    "            #res = yorkreg_nocorr(temp['s_y'],temp['s_x'] - temp['s_y'],\n",
    "            #                     temp['stderr(s)_x'],np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2),\n",
    "            #                     yorkn)\n",
    "            #ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            #print('∆s reversion full slope = '+str(res[0]))\n",
    "            \n",
    "            #for v03\n",
    "            res = linregress(temp['s_y'],temp['s_x'] - temp['s_y'])\n",
    "            ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res.slope+res.intercept,color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "            print('∆s reversion full slope = '+str(res.slope))\n",
    "\n",
    "            #ax.set_yticks([-0.4,-0.2,0])\n",
    "            #ax.set_xticks([-0.4,-0.2,0])\n",
    "\n",
    "            ax[1][1].set_xlim(xs)\n",
    "            ax[1][1].set_ylim(ys)\n",
    "\n",
    "            # Set labels, ticks, and limits\n",
    "\n",
    "            ax[1][0].set_xlabel('\\n'.join(wrap('Fitness, PMA1 234C',9)))\n",
    "            ax[1][1].set_xlabel('\\n'.join(wrap('Fitness, PMA1 234S',9)))\n",
    "\n",
    "            ax[1][0].set_ylabel('\\n'.join(wrap('∆$\\phi$, PMA1 C234S',11)))\n",
    "            ax[1][1].set_ylabel('\\n'.join(wrap('∆$\\phi$, PMA1 S234C',11)),labelpad=5)\n",
    "\n",
    "            ax[0][0].set_ylabel('\\n'.join(wrap('Fitness, PMA1 234S',9)))\n",
    "            ax[0][1].set_ylabel('\\n'.join(wrap('Fitness, PMA1 234C',9)),labelpad=5)\n",
    "\n",
    "            for i in np.arange(2):\n",
    "                for j in np.arange(2):\n",
    "                    ax[i][j].set_xlim(-0.65,0.1)\n",
    "            \n",
    "            for i in np.arange(2):\n",
    "                ax[0][i].set_ylim(-0.65,0.1)\n",
    "                ax[1][i].set_ylim(-0.6,0.6)\n",
    "                #ax[0][i].set_yticks([-0.5,0])\n",
    "                #ax[1][i].set_yticks([-0.3,0,0.3])\n",
    "            \n",
    "            fig.align_ylabels()\n",
    "            \n",
    "            #fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v02.pdf',bbox_inches='tight',dpi=3000)\n",
    "            fig.savefig('msfigs/SIfigs/'+ploidies[p1]+envts[e1]+floci[l1a]+'_v03_linregressfordeltarev.pdf',bbox_inches='tight',dpi=3000)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            print(ploidies[p1])\n",
    "            print(envts[e1])\n",
    "            print(floci[l1a])\n",
    "            print('********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 3C\n",
    "\n",
    "# Define our ploidies and loci for the TOP ROW\n",
    "p1 = 0\n",
    "e1 = 0\n",
    "l1a = 5\n",
    "l2a = 7\n",
    "l3a = 9\n",
    "\n",
    "\n",
    "# common plot parameters\n",
    "mysize = 1\n",
    "myalpha = 0.5\n",
    "elw = 0.5\n",
    "ealpha = 0.3\n",
    "\n",
    "yorkn=100\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "o=10\n",
    "\n",
    "fig,ax = plt.subplots(nrows = 1, ncols = 3, sharex=True, sharey=True, figsize=(2.52,0.8))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "\n",
    "# 3Ci\n",
    "data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p1]+'_'+envts[e1]+'_'+str(o)+'.txt',\n",
    "                     sep='\\t',names=['genotype',ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o),ploidies[p1]+'_'+envts[e1]+'_s-obs',ploidies[p1]+'_'+envts[e1]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "# Since we're using the observed s, remove the predicted columns\n",
    "data1 = data1.drop(columns=[ploidies[p1]+'_'+envts[e1]+'_s-pred_'+str(o)]).reset_index(drop=True)\n",
    "\n",
    "# Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "# represented multiple times.\n",
    "glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "data2 = pd.DataFrame()\n",
    "\n",
    "for g in np.arange(len(glist)):\n",
    "    tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "    if len(tempg) == 1:\n",
    "        data2.at[g,'genotype'] = glist[g]\n",
    "        data2.at[g,'s'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs']\n",
    "        data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p1]+'_'+envts[e1]+'_s-obs-err']\n",
    "    elif len(tempg) > 1:\n",
    "        data2.at[g,'genotype'] = glist[g]\n",
    "        data2.at[g,'s'] = tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'].mean()\n",
    "        my_svar = statistics.variance(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs'])\n",
    "        my_svar = 0\n",
    "        mymean_stderr = np.mean(tempg[ploidies[p1]+'_'+envts[e1]+'_s-obs-err']**2)\n",
    "        data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "        #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "data1 = data2\n",
    "\n",
    "# Create a column for each locus, and for the genotype with that locus removed\n",
    "for l in np.arange(len(floci)):\n",
    "    data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "for l in np.arange(len(floci)):\n",
    "    data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "\n",
    "tab0 = data1.loc[(data1[floci[l1a]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "tab1 = data1.loc[(data1[floci[l1a]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l1a]]]\n",
    "\n",
    "#temp = pd.merge(tab1,tab0,how='inner',on='without_'+floci[l])\n",
    "temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l1a])\n",
    "#temp['s_diff'] = temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'] - temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x']\n",
    "\n",
    "#print('x min = '+str(temp['s_x'].min()),' , x max = '+str(temp['s_x'].max()))\n",
    "#print('y min = '+str(temp['s_y'].min()),' , y max = '+str(temp['s_y'].max()))\n",
    "\n",
    "for i in np.arange(len(floci)):\n",
    "    temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "#export_csv = temp.to_csv(r'20210330_tempcheck_fig3c.csv',index=True,header=True)    \n",
    "\n",
    "# work in the second locus\n",
    "templ20 = temp.loc[(temp[floci[l2a]] == 0)].copy(deep=True).reset_index(drop=True)\n",
    "templ21 = temp.loc[(temp[floci[l2a]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "# Create a version where we have a third locus\n",
    "templ200 = templ20.loc[(templ20[floci[l3a]] == 0)].copy(deep=True).reset_index(drop=True)\n",
    "templ201 = templ20.loc[(templ20[floci[l3a]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "templ210 = templ21.loc[(templ21[floci[l3a]] == 0)].copy(deep=True).reset_index(drop=True)\n",
    "templ211 = templ21.loc[(templ21[floci[l3a]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "markers0,caps0,bars0 = ax[0].errorbar(temp['s_x'],temp['s_y'],\n",
    "            xerr = temp['stderr(s)_x'],\n",
    "            yerr = temp['stderr(s)_y'],\n",
    "            alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:dark grey',elinewidth=elw)\n",
    "\n",
    "markers1,caps1,bars1 = ax[1].errorbar(templ20['s_x'],templ20['s_y'],\n",
    "            xerr = templ20['stderr(s)_x'],\n",
    "            yerr = templ20['stderr(s)_y'],\n",
    "            alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:cerulean',elinewidth=elw)\n",
    "markers2,caps2,bars2 = ax[1].errorbar(templ21['s_x'],templ21['s_y'],\n",
    "            xerr = templ21['stderr(s)_x'],\n",
    "            yerr = templ21['stderr(s)_y'],\n",
    "            alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:orange',elinewidth=elw)\n",
    "\n",
    "markers3,caps3,bars3 = ax[2].errorbar(templ200['s_x'],templ200['s_y'],\n",
    "                                     xerr = templ200['stderr(s)_x'],\n",
    "                                     yerr = templ200['stderr(s)_y'],\n",
    "                                     alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:cerulean',elinewidth=elw)\n",
    "markers4,caps4,bars4 = ax[2].errorbar(templ201['s_x'],templ201['s_y'],\n",
    "                                     xerr = templ201['stderr(s)_x'],\n",
    "                                     yerr = templ201['stderr(s)_y'],\n",
    "                                     alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:topaz',elinewidth=elw)\n",
    "markers5,caps5,bars5 = ax[2].errorbar(templ210['s_x'],templ210['s_y'],\n",
    "                                     xerr = templ210['stderr(s)_x'],\n",
    "                                     yerr = templ210['stderr(s)_y'],\n",
    "                                     alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:orange',elinewidth=elw)\n",
    "markers6,caps6,bars6 = ax[2].errorbar(templ211['s_x'],templ211['s_y'],\n",
    "                                     xerr = templ211['stderr(s)_x'],\n",
    "                                     yerr = templ211['stderr(s)_y'],\n",
    "                                     alpha=myalpha,linestyle='None',marker='.',ms=mysize,color='xkcd:wine red',elinewidth=elw)\n",
    "\n",
    "[bar.set_alpha(ealpha) for bar in bars0]\n",
    "[bar.set_alpha(ealpha) for bar in bars1]\n",
    "[bar.set_alpha(ealpha) for bar in bars2]\n",
    "[bar.set_alpha(ealpha) for bar in bars3]\n",
    "[bar.set_alpha(ealpha) for bar in bars4]\n",
    "[bar.set_alpha(ealpha) for bar in bars5]\n",
    "[bar.set_alpha(ealpha) for bar in bars6]\n",
    "\n",
    "\n",
    "#ax[1].set_xlabel('MKT1 WT')\n",
    "#ax[0].set_ylabel(floci[l1a]+' Mut')\n",
    "#fig.text(0.44,-0.1,floci[l]+' WT',fontsize=7)\n",
    "#plt.title(ploidies[p]+'-'+envts[e]+', orange = '+floci[l2]+' = 1')\n",
    "\n",
    "xs = ax[1].get_xlim()\n",
    "ys = ax[1].get_ylim()\n",
    "\n",
    "# for hap/hom 37 MKT1, RHO5\n",
    "#xs = (-0.14,0.025)\n",
    "#ys = (-0.14,0.025)\n",
    "\n",
    "#fig.legend(['WT','Mut'],title=floci[l2],loc='lower right',handlelength=1,bbox_to_anchor=(0.98,0.28))\n",
    "\n",
    "ax[0].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "ax[1].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "ax[2].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',zorder=0)\n",
    "ax[0].set_xlim(xs)\n",
    "ax[0].set_ylim(ys)\n",
    "ax[1].set_xlim(xs)\n",
    "ax[1].set_ylim(ys)\n",
    "ax[2].set_xlim(xs)\n",
    "ax[2].set_ylim(ys)\n",
    "\n",
    "# Full regression slope\n",
    "res = yorkreg_nocorr(temp['s_x'],temp['s_y'],\n",
    "                     #[0]*len(temp),[0]*len(temp),\n",
    "                     temp['stderr(s)_x'],temp['stderr(s)_y'],\n",
    "                     yorkn)\n",
    "ax[0].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=1,zorder=0,lw=0.6)\n",
    "print('full slope = '+str(res[0]))\n",
    "\n",
    "res = yorkreg_nocorr(templ20['s_x'],templ20['s_y'],\n",
    "                     #[0]*len(templ20),[0]*len(templ20),\n",
    "                     templ20['stderr(s)_x'],templ20['stderr(s)_y'],\n",
    "                     yorkn)\n",
    "ax[1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:cerulean',alpha=1,zorder=0,lw=0.6)\n",
    "print('without'+floci[l2a]+' slope slope = '+str(res[0]))\n",
    "\n",
    "res = yorkreg_nocorr(templ21['s_x'],templ21['s_y'],\n",
    "                     #[0]*len(templ21),[0]*len(templ21),\n",
    "                     templ21['stderr(s)_x'],templ21['stderr(s)_y'],\n",
    "                     yorkn)\n",
    "ax[1].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:orange',alpha=1,zorder=0,lw=0.6)\n",
    "print('with'+floci[l2a]+' slope slope = '+str(res[0]))\n",
    "\n",
    "res = yorkreg_nocorr(templ200['s_x'],templ200['s_y'],\n",
    "                     #[0]*len(templ200),[0]*len(templ200),\n",
    "                     templ200['stderr(s)_x'],templ200['stderr(s)_y'],\n",
    "                     yorkn)\n",
    "ax[2].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:cerulean',alpha=1,zorder=0,lw=0.6)\n",
    "print('no'+floci[l2a]+'no'+floci[l3a]+' slope = '+str(res[0]))\n",
    "\n",
    "res = yorkreg_nocorr(templ201['s_x'],templ201['s_y'],\n",
    "                     #[0]*len(templ201),[0]*len(templ201),\n",
    "                     templ201['stderr(s)_x'],templ201['stderr(s)_y'],\n",
    "                     yorkn)\n",
    "ax[2].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:topaz',alpha=1,zorder=0,lw=0.6)\n",
    "print('no'+floci[l2a]+'yes'+floci[l3a]+' slope = '+str(res[0]))\n",
    "\n",
    "res = yorkreg_nocorr(templ210['s_x'],templ210['s_y'],\n",
    "                     #[0]*len(templ210),[0]*len(templ210),\n",
    "                     templ210['stderr(s)_x'],templ210['stderr(s)_y'],\n",
    "                     yorkn)\n",
    "ax[2].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:orange',alpha=1,zorder=0,lw=0.6)\n",
    "print('yes'+floci[l2a]+'no'+floci[l3a]+' slope = '+str(res[0]))\n",
    "\n",
    "res = yorkreg_nocorr(templ211['s_x'],templ211['s_y'],\n",
    "                     #[0]*len(templ211),[0]*len(templ211),\n",
    "                     templ211['stderr(s)_x'],templ211['stderr(s)_y'],\n",
    "                     yorkn)\n",
    "ax[2].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:wine red',alpha=1,zorder=0,lw=0.6)\n",
    "print('yes'+floci[l2a]+'yes'+floci[l3a]+' slope = '+str(res[0]))\n",
    "\n",
    "for i in np.arange(3):\n",
    "    ax[i].set_xlim(-0.17,0.02)\n",
    "    ax[i].set_ylim(-0.17,0.02)\n",
    "\n",
    "ax[0].set_yticks([-0.1,0])\n",
    "\n",
    "ax[0].set_ylabel('\\n'.join(wrap('Fitness, RHO5 10S',10)))\n",
    "\n",
    "fig.align_ylabels()\n",
    "\n",
    "fig.text(0.35,-0.25,'Fitness, RHO5 10G',fontsize=7)\n",
    "\n",
    "\n",
    "mylegmark = 3\n",
    "mycs = 0.1\n",
    "myhtp = -0.4\n",
    "myls = 0.1\n",
    "dot1 = Line2D([], [], color='xkcd:cerulean', marker='.', linestyle='None',\n",
    "                          markersize=mylegmark, label='L')\n",
    "dot2 = Line2D([], [], color='xkcd:orange', marker='.', linestyle='None',\n",
    "                          markersize=mylegmark, label='S')\n",
    "\n",
    "leg2a = fig.legend(handles=[dot1,dot2],ncol=2,handletextpad=myhtp,loc='lower right',bbox_to_anchor=(0.69,1.22),title='WHI2',\n",
    "                  borderpad=0.2,columnspacing=mycs,labelspacing=myls,frameon=False)\n",
    "leg2a._legend_box.align = \"left\"\n",
    "leg2a.get_title().set_position((5, 0))\n",
    "\n",
    "dot1 = Line2D([], [], color='xkcd:cerulean', marker='.', linestyle='None',\n",
    "                          markersize=mylegmark, label='L, S')\n",
    "dot2 = Line2D([], [], color='xkcd:topaz', marker='.', linestyle='None',\n",
    "                          markersize=mylegmark, label='L, P')\n",
    "dot3 = Line2D([], [], color='xkcd:orange', marker='.', linestyle='None',\n",
    "                          markersize=mylegmark, label='S, S')\n",
    "dot4 = Line2D([], [], color='xkcd:wine red', marker='.', linestyle='None',\n",
    "                          markersize=mylegmark, label='S, P')\n",
    "\n",
    "leg3a = fig.legend(handles=[dot1,dot2,dot3,dot4],ncol=2,handletextpad=myhtp,loc='lower right',bbox_to_anchor=(1.05,1.22),title='WHI2, AKL1',\n",
    "                  borderpad=0.2,columnspacing=mycs,labelspacing=myls,frameon=False)\n",
    "leg3a._legend_box.align = \"left\"\n",
    "leg3a.get_title().set_position((5, 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig('msfigs/Fig3/3Ci_v01.pdf',bbox_inches='tight',dpi=3000)\n",
    "#fig.savefig('msfigs/Fig3/3Ci_v01.jpg',bbox_inches='tight',dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 3D\n",
    "# Redo the plot immediately above but flip to be a 4x2 instead of a 2x4\n",
    "\n",
    "locinow = ['AKL1','MKT1','RHO5','WHI2',1,2,4,5]\n",
    "#locinow = ['MKT1','WHI2','AKL1','PMA1',1,2,4,5]\n",
    "#locinow = ['WHI2','MKT1','AKL1','PMA1',0,1,4,5]\n",
    "#locinow = ['RHO5','WHI2','MKT1','BUL2',1,2,1,2]\n",
    "\n",
    "yorkn = 100\n",
    "\n",
    "o=10\n",
    "\n",
    "for p in np.arange(locinow[4],locinow[5]):\n",
    "    for e in np.arange(locinow[6],locinow[7]):\n",
    "        # Do this same thing with observed values\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                                 sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_10',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        # Since we're just using the observed s, remove the pred columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-pred_10']).reset_index(drop=True)\n",
    "        \n",
    "        # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "        # represented multiple times.\n",
    "        glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "        \n",
    "        data2 = pd.DataFrame()\n",
    "        \n",
    "        for g in np.arange(len(glist)):\n",
    "            tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "            if len(tempg) == 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "                data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "            elif len(tempg) > 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "                my_svar = statistics.variance(tempg[ploidies[p]+'_'+envts[e]+'_s-obs'])\n",
    "                my_svar = 0\n",
    "                mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "                data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "        \n",
    "        data1 = data2\n",
    "        \n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        mysize = 1\n",
    "        myalpha = 0.7\n",
    "        elw = 0.5\n",
    "        ealpha = 0.15\n",
    "        lws = 0.1\n",
    "        \n",
    "        colorsnow = ['xkcd:cerulean','xkcd:orange','xkcd:bright green','xkcd:indigo','xkcd:pink','xkcd:rust','xkcd:gold','xkcd:bright aqua']\n",
    "\n",
    "        tab0 = data1.loc[(data1[locinow[0]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "        tab0 = tab0[['genotype','s','stderr(s)','without_'+locinow[0]]]\n",
    "        tab1 = data1.loc[(data1[locinow[0]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "        tab1 = tab1[['genotype','s','stderr(s)','without_'+locinow[0]]]\n",
    "\n",
    "        temp = pd.merge(tab0,tab1,how='inner',on='without_'+locinow[0])\n",
    "        \n",
    "\n",
    "        for i in np.arange(len(floci)):\n",
    "            temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "            \n",
    "\n",
    "        myalpha=0.7\n",
    "        mysize = 1\n",
    "        \n",
    "        mytoplot0 = temp.loc[(temp[locinow[1]] == 0)]\n",
    "        mytoplot1 = temp.loc[(temp[locinow[1]] == 1)]\n",
    "        \n",
    "        mytoplot00 = mytoplot0.loc[(mytoplot0[locinow[2]] == 0)]\n",
    "        mytoplot01 = mytoplot0.loc[(mytoplot0[locinow[2]] == 1)]\n",
    "        \n",
    "        mytoplot10 = mytoplot1.loc[(mytoplot1[locinow[2]] == 0)]\n",
    "        mytoplot11 = mytoplot1.loc[(mytoplot1[locinow[2]] == 1)]\n",
    "        \n",
    "        mytoplot000 = mytoplot00.loc[(mytoplot00[locinow[3]] == 0)]\n",
    "        mytoplot001 = mytoplot00.loc[(mytoplot00[locinow[3]] == 1)]\n",
    "        \n",
    "        mytoplot010 = mytoplot01.loc[(mytoplot01[locinow[3]] == 0)]\n",
    "        mytoplot011 = mytoplot01.loc[(mytoplot01[locinow[3]] == 1)]\n",
    "        \n",
    "        mytoplot100 = mytoplot10.loc[(mytoplot10[locinow[3]] == 0)]\n",
    "        mytoplot101 = mytoplot10.loc[(mytoplot10[locinow[3]] == 1)]\n",
    "        \n",
    "        mytoplot110 = mytoplot11.loc[(mytoplot11[locinow[3]] == 0)]\n",
    "        mytoplot111 = mytoplot11.loc[(mytoplot11[locinow[3]] == 1)]\n",
    "\n",
    "        mymec = 'xkcd:dark grey'\n",
    "        mysize = 1.5\n",
    "        \n",
    "        # FIGURE DEFINITION\n",
    "        fig,ax = plt.subplots(nrows = 4, ncols = 2, sharex=True, sharey=True, figsize=(1.36,2.84))\n",
    "        fig.subplots_adjust(wspace=0.2,hspace=0.2)\n",
    "        \n",
    "        markers1,caps1,bars1 = ax[0][0].errorbar(mytoplot000['s_x'],mytoplot000['s_y'],\n",
    "                                xerr = mytoplot000['stderr(s)_x'],\n",
    "                                yerr = mytoplot000['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[0],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        markers2,caps2,bars2 = ax[1][0].errorbar(mytoplot001['s_x'],mytoplot001['s_y'],\n",
    "                                xerr = mytoplot001['stderr(s)_x'],\n",
    "                                yerr = mytoplot001['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[1],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        markers3,caps3,bars3 = ax[2][0].errorbar(mytoplot010['s_x'],mytoplot010['s_y'],\n",
    "                                xerr = mytoplot010['stderr(s)_x'],\n",
    "                                yerr = mytoplot010['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[2],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        markers4,caps4,bars4 = ax[3][0].errorbar(mytoplot011['s_x'],mytoplot011['s_y'],\n",
    "                                xerr = mytoplot011['stderr(s)_x'],\n",
    "                                yerr = mytoplot011['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[3],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        markers5,caps5,bars5 = ax[0][1].errorbar(mytoplot100['s_x'],mytoplot100['s_y'],\n",
    "                                xerr = mytoplot100['stderr(s)_x'],\n",
    "                                yerr = mytoplot100['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[4],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        markers6,caps6,bars6 = ax[1][1].errorbar(mytoplot101['s_x'],mytoplot101['s_y'],\n",
    "                                xerr = mytoplot101['stderr(s)_x'],\n",
    "                                yerr = mytoplot101['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[5],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        markers7,caps7,bars7 = ax[2][1].errorbar(mytoplot110['s_x'],mytoplot110['s_y'],\n",
    "                                xerr = mytoplot110['stderr(s)_x'],\n",
    "                                yerr = mytoplot110['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[6],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        markers8,caps8,bars8 = ax[3][1].errorbar(mytoplot111['s_x'],mytoplot111['s_y'],\n",
    "                                xerr = mytoplot111['stderr(s)_x'],\n",
    "                                yerr = mytoplot111['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize*3+lws,mew=lws,mec=mymec,color=colorsnow[7],elinewidth=elw,zorder=4)\n",
    "        \n",
    "        [bar.set_alpha(ealpha) for bar in bars1]\n",
    "        [bar.set_alpha(ealpha) for bar in bars2]\n",
    "        [bar.set_alpha(ealpha) for bar in bars3]\n",
    "        [bar.set_alpha(ealpha) for bar in bars4]\n",
    "        [bar.set_alpha(ealpha) for bar in bars5]\n",
    "        [bar.set_alpha(ealpha) for bar in bars6]\n",
    "        [bar.set_alpha(ealpha) for bar in bars7]\n",
    "        [bar.set_alpha(ealpha) for bar in bars8]\n",
    "        \n",
    "        #leave out for now until can fix\n",
    "        #fig.legend(['000','001','010','011','100','101','110','111'],bbox_to_anchor=(0.9, 1), loc='upper left')\n",
    "        \n",
    "        xs = ax[0][1].get_xlim()\n",
    "        ys = ax[0][1].get_ylim()\n",
    "        xs = (-0.12,0.01)\n",
    "        ys = (-0.12,0.01)\n",
    "        \n",
    "        # subslopes\n",
    "        res000 = yorkreg_nocorr(mytoplot000['s_x'],mytoplot000['s_y'],mytoplot000['stderr(s)_x'],mytoplot000['stderr(s)_y'],yorkn)\n",
    "        res001 = yorkreg_nocorr(mytoplot001['s_x'],mytoplot001['s_y'],mytoplot001['stderr(s)_x'],mytoplot001['stderr(s)_y'],yorkn)\n",
    "        res010 = yorkreg_nocorr(mytoplot010['s_x'],mytoplot010['s_y'],mytoplot010['stderr(s)_x'],mytoplot010['stderr(s)_y'],yorkn)\n",
    "        res011 = yorkreg_nocorr(mytoplot011['s_x'],mytoplot011['s_y'],mytoplot011['stderr(s)_x'],mytoplot011['stderr(s)_y'],yorkn)\n",
    "        res100 = yorkreg_nocorr(mytoplot100['s_x'],mytoplot100['s_y'],mytoplot100['stderr(s)_x'],mytoplot100['stderr(s)_y'],yorkn)\n",
    "        res101 = yorkreg_nocorr(mytoplot101['s_x'],mytoplot101['s_y'],mytoplot101['stderr(s)_x'],mytoplot101['stderr(s)_y'],yorkn)\n",
    "        res110 = yorkreg_nocorr(mytoplot110['s_x'],mytoplot110['s_y'],mytoplot110['stderr(s)_x'],mytoplot110['stderr(s)_y'],yorkn)\n",
    "        res111 = yorkreg_nocorr(mytoplot111['s_x'],mytoplot111['s_y'],mytoplot111['stderr(s)_x'],mytoplot111['stderr(s)_y'],yorkn)\n",
    "        \n",
    "        ax[0][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res000[0]+res000[1],color=colorsnow[0],alpha=0.7,zorder=3,lw=0.6)\n",
    "        ax[1][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res001[0]+res001[1],color=colorsnow[1],alpha=0.7,zorder=3,lw=0.6)\n",
    "        ax[2][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res010[0]+res010[1],color=colorsnow[2],alpha=0.7,zorder=3,lw=0.6)\n",
    "        ax[3][0].plot(np.linspace(-1,1),np.linspace(-1,1)*res011[0]+res011[1],color=colorsnow[3],alpha=0.7,zorder=3,lw=0.6)\n",
    "        ax[0][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res100[0]+res100[1],color=colorsnow[4],alpha=0.7,zorder=3,lw=0.6)\n",
    "        ax[1][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res101[0]+res101[1],color=colorsnow[5],alpha=0.7,zorder=3,lw=0.6)\n",
    "        ax[2][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res110[0]+res110[1],color=colorsnow[6],alpha=0.7,zorder=3,lw=0.6)\n",
    "        ax[3][1].plot(np.linspace(-1,1),np.linspace(-1,1)*res111[0]+res111[1],color=colorsnow[7],alpha=0.7,zorder=3,lw=0.6)\n",
    "        \n",
    "        bgcolor = '#C0C7BD'\n",
    "        \n",
    "        markers1b,caps1b,bars1b = ax[0][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "        \n",
    "        markers2b,caps2b,bars2b = ax[1][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "        \n",
    "        markers3b,caps3b,bars3b = ax[2][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "        \n",
    "        markers4b,caps4b,bars4b = ax[3][0].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "        \n",
    "        markers5b,caps5b,bars5b = ax[0][1].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "        \n",
    "        markers6b,caps6b,bars6b = ax[1][1].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "        \n",
    "        markers7b,caps7b,bars7b = ax[2][1].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "        \n",
    "        markers8b,caps8b,bars8b = ax[3][1].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                xerr = temp['stderr(s)_x'],\n",
    "                                yerr = temp['stderr(s)_y'],\n",
    "                                alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=bgcolor,elinewidth=elw,zorder=2)\n",
    "\n",
    "        \n",
    "        [bar.set_alpha(ealpha) for bar in bars1b]\n",
    "        [bar.set_alpha(ealpha) for bar in bars2b]\n",
    "        [bar.set_alpha(ealpha) for bar in bars3b]\n",
    "        [bar.set_alpha(ealpha) for bar in bars4b]\n",
    "        [bar.set_alpha(ealpha) for bar in bars5b]\n",
    "        [bar.set_alpha(ealpha) for bar in bars6b]\n",
    "        [bar.set_alpha(ealpha) for bar in bars7b]\n",
    "        [bar.set_alpha(ealpha) for bar in bars8b]\n",
    "        \n",
    "\n",
    "        for i in np.arange(2):\n",
    "            for j in np.arange(4):\n",
    "                ax[j][i].plot(np.linspace(-1,1),np.linspace(-1,1),color='xkcd:black',zorder=0,lw=0.8)\n",
    "        \n",
    "        fig.text(-0.14,0.37,'Fitness, AKL1 176P',fontsize=7,rotation='vertical')\n",
    "        fig.text(0.2,0.02,'Fitness, AKL1 176S',fontsize=7)\n",
    "\n",
    "        yorkn=100\n",
    "        # Full regression slope\n",
    "        res = yorkreg_nocorr(temp['s_x'],temp['s_y'],temp['stderr(s)_x'],temp['stderr(s)_y'],yorkn)\n",
    "        \n",
    "        for i in np.arange(2):\n",
    "            for j in np.arange(4):\n",
    "                ax[j][i].plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='xkcd:dark grey',alpha=0.7,zorder=1,lw=0.6)\n",
    "        #print('full slope = '+str(res[0][0]))\n",
    "\n",
    "        for i in np.arange(2):\n",
    "            for j in np.arange(4):\n",
    "                ax[j][i].set_xlim(xs)\n",
    "                ax[j][i].set_ylim(ys)\n",
    "        \n",
    "        for i in np.arange(4):\n",
    "            ax[i][0].set_yticks([-0.1,0])\n",
    "            ax[i][0].tick_params(axis='y', which='major', pad=0.7)\n",
    "        \n",
    "        fig.savefig('msfigs/Fig3/homsulocAKL1_8panel_RAW_newerr_2cols.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3B\n",
    "# Will do a bunch of hierarchical linear regressions now.\n",
    "# First, do a linear regression on the whole batch of points.\n",
    "# Then separate into two batches based on an independent mutation (10 pairs total) and perform regressions on each.\n",
    "# Then divide further into 4 batches based on pairwise combinations of mutations in the background.\n",
    "# And finally go to 8 batches, max we'll do, based on 3rd-order combinations of mutations in the background.\n",
    "\n",
    "gowith = ['lasso_v2',11]\n",
    "\n",
    "# Get the data\n",
    "\n",
    "o1tab = pd.DataFrame()\n",
    "o2tab = pd.DataFrame()\n",
    "o3tab = pd.DataFrame()\n",
    "\n",
    "myindo1 = 0\n",
    "myindo2 = 0\n",
    "myindo3 = 0\n",
    "\n",
    "o = 10\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):    \n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        # Since we're just using the predicted s, remove the obs columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "\n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        # So we can plot in different ways, import the background-averaged fitness effects of each mutation\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(gowith[1]-1)+'.txt',\n",
    "                                     sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(gowith[1]-1),'na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "\n",
    "        data0 = data0.loc[(data0['numMut'] == 1)].sort_values(by='genotype',ascending=False).reset_index(drop=True)\n",
    "\n",
    "        mysize = 15\n",
    "        myalpha = 0.5\n",
    "        \n",
    "\n",
    "        # Divvy up the data by focal locus\n",
    "        #for l in np.arange(2,3):\n",
    "        for l in np.arange(len(floci)):\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),'without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),'without_'+floci[l]]]\n",
    "\n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "            \n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "            \n",
    "            # Perform a linear regression, capturing slope, intercept, and R^2\n",
    "            res = linregress(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "            res\n",
    "            o1tab.at[myindo1,'ploidy'] = ploidies[p]\n",
    "            o1tab.at[myindo1,'envt'] = envts[e]\n",
    "            o1tab.at[myindo1,'locus'] = floci[l]\n",
    "            o1tab.at[myindo1,'slope-o1'] = res.slope\n",
    "            o1tab.at[myindo1,'intercept-o1'] = res.intercept\n",
    "            o1tab.at[myindo1,'r^2-o1'] = res.rvalue**2\n",
    "            \n",
    "            myindo1 = myindo1 + 1\n",
    "            \n",
    "            for l2 in np.arange(len(floci)):\n",
    "                if l != l2:\n",
    "                    templ20 = temp.loc[(temp[floci[l2]] == 0)].copy(deep=True).reset_index(drop=True)\n",
    "                    templ21 = temp.loc[(temp[floci[l2]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                    \n",
    "                    resl20 = linregress(templ20[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],templ20[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "                    \n",
    "                    resl21 = linregress(templ21[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],templ21[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "                    \n",
    "                    o2tab.at[myindo2,'ploidy'] = ploidies[p]\n",
    "                    o2tab.at[myindo2,'envt'] = envts[e]\n",
    "                    o2tab.at[myindo2,'locus-1'] = floci[l]\n",
    "                    o2tab.at[myindo2,'locus-2'] = floci[l2]\n",
    "                    o2tab.at[myindo2,'slope-o2, 0'] = resl20.slope\n",
    "                    o2tab.at[myindo2,'intercept-o2, 0'] = resl20.intercept\n",
    "                    o2tab.at[myindo2,'r^2-o2, 0'] = resl20.rvalue**2\n",
    "                    o2tab.at[myindo2,'slope-o2, 1'] = resl21.slope\n",
    "                    o2tab.at[myindo2,'intercept-o2, 1'] = resl21.intercept\n",
    "                    o2tab.at[myindo2,'r^2-o2, 1'] = resl21.rvalue**2\n",
    "                    \n",
    "                    myindo2 = myindo2 + 1\n",
    "                    \n",
    "                    for l3 in np.arange(len(floci)):\n",
    "                        if l2 != l3 and l != l3:\n",
    "                            templ2030 = templ20.loc[(templ20[floci[l3]] == 0)].copy(deep=True).reset_index(drop=True)\n",
    "                            templ2031 = templ20.loc[(templ20[floci[l3]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                            templ2130 = templ21.loc[(templ21[floci[l3]] == 0)].copy(deep=True).reset_index(drop=True)\n",
    "                            templ2131 = templ21.loc[(templ21[floci[l3]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                            \n",
    "                            resl2030 = linregress(templ2030[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],templ2030[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "                            resl2031 = linregress(templ2031[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],templ2031[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "                            resl2130 = linregress(templ2130[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],templ2130[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "                            resl2131 = linregress(templ2131[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],templ2131[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "                            \n",
    "                            o3tab.at[myindo3,'ploidy'] = ploidies[p]\n",
    "                            o3tab.at[myindo3,'envt'] = envts[e]\n",
    "                            o3tab.at[myindo3,'locus-1'] = floci[l]\n",
    "                            o3tab.at[myindo3,'locus-2'] = floci[l2]\n",
    "                            o3tab.at[myindo3,'locus-3'] = floci[l3]\n",
    "                            o3tab.at[myindo3,'slope-o3, 00'] = resl2030.slope\n",
    "                            o3tab.at[myindo3,'intercept-o3, 00'] = resl2030.intercept\n",
    "                            o3tab.at[myindo3,'r^2-o3, 00'] = resl2030.rvalue**2\n",
    "                            o3tab.at[myindo3,'slope-o3, 01'] = resl2031.slope\n",
    "                            o3tab.at[myindo3,'intercept-o3, 01'] = resl2031.intercept\n",
    "                            o3tab.at[myindo3,'r^2-o3, 01'] = resl2031.rvalue**2\n",
    "                            o3tab.at[myindo3,'slope-o3, 10'] = resl2130.slope\n",
    "                            o3tab.at[myindo3,'intercept-o3, 10'] = resl2130.intercept\n",
    "                            o3tab.at[myindo3,'r^2-o3, 10'] = resl2130.rvalue**2\n",
    "                            o3tab.at[myindo3,'slope-o3, 11'] = resl2131.slope\n",
    "                            o3tab.at[myindo3,'intercept-o3, 11'] = resl2131.intercept\n",
    "                            o3tab.at[myindo3,'r^2-o3, 11'] = resl2131.rvalue**2\n",
    "                            \n",
    "                            myindo3 = myindo3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(o1tab.loc[o1tab[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start out, just to see, plot all the first-order slopes and R^2 values\n",
    "plt.scatter(o1tab['slope-o1'],o1tab['r^2-o1'],linestyle='None',alpha=0.5)\n",
    "plt.axvline(x=1,c='xkcd:grey',lw=0.5,zorder=0)\n",
    "plt.xlabel('slope')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('first order')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(o1tab['slope-o1'],o1tab['intercept-o1'],linestyle='None',alpha=0.5)\n",
    "plt.axhline(y=0,c='xkcd:grey',lw=0.5,zorder=0)\n",
    "plt.axvline(x=1,c='xkcd:grey',lw=0.5,zorder=0)\n",
    "plt.xlabel('slope')\n",
    "plt.ylabel('intercept')\n",
    "plt.title('first order')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(o1tab['slope-o1'],bins=30)\n",
    "plt.xlabel('slope')\n",
    "plt.show()\n",
    "print('mean slope = '+str(o1tab['slope-o1'].mean()))\n",
    "print('median slope = '+str(o1tab['slope-o1'].median()))\n",
    "\n",
    "plt.hist(o1tab['r^2-o1'],bins=30)\n",
    "plt.xlabel('R^2')\n",
    "plt.show()\n",
    "print('mean R^2 = '+str(o1tab['r^2-o1'].mean()))\n",
    "print('median R^2 = '+str(o1tab['r^2-o1'].median()))\n",
    "\n",
    "myperc = len(o1tab.loc[(abs(o1tab['slope-o1'] - 1) > .1)])/len(o1tab)\n",
    "print(str(myperc)+' of slopes deviate from 1 by > 0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same as above but with york regression, plot histogram of slopes\n",
    "gowith = ['lasso_v2',11]\n",
    "#THIS TAKES A WHILE TO RUN TOO\n",
    "# Get the data\n",
    "\n",
    "o1tab = pd.DataFrame()\n",
    "\n",
    "myindo1 = 0\n",
    "\n",
    "o = 10\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):    \n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        # Since we're just using the predicted s, remove the obs columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "\n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        # So we can plot in different ways, import the background-averaged fitness effects of each mutation\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(gowith[1]-1)+'.txt',\n",
    "                                     sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(gowith[1]-1),'na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "\n",
    "        #data0 = data0.loc[(data0['numMut'] == 1)].sort_values(by='genotype',ascending=False).reset_index(drop=True)\n",
    "\n",
    "        mysize = 15\n",
    "        myalpha = 0.5\n",
    "        \n",
    "\n",
    "        # Divvy up the data by focal locus\n",
    "        #for l in np.arange(2,3):\n",
    "        for l in np.arange(len(floci)):\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),'without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),'without_'+floci[l]]]\n",
    "\n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "            \n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "            \n",
    "            temp['∆s(y-x)'] = temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'] - temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x']\n",
    "\n",
    "            \n",
    "            # Perform a linear regression, capturing slope and intercept\n",
    "            yorkn = 100\n",
    "            #res = linregress(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "            res = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            o1tab.at[myindo1,'ploidy'] = ploidies[p]\n",
    "            o1tab.at[myindo1,'envt'] = envts[e]\n",
    "            o1tab.at[myindo1,'locus'] = floci[l]\n",
    "            o1tab.at[myindo1,'slope'] = res[0]\n",
    "            o1tab.at[myindo1,'intercept'] = res[1]\n",
    "            o1tab.at[myindo1,'stderr(slope)'] = np.sqrt(res[2])\n",
    "            o1tab.at[myindo1,'additive_s'] = data0.loc[(data0[floci[l]] == 1)&(data0['numMut'] == 1),ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)].values[0]\n",
    "            o1tab.at[myindo1,'epmagsum'] = abs(data0.loc[(data0[floci[l]] == 1)&(data0['numMut'] > 1),ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)]).sum()\n",
    "            \n",
    "            resdo = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp['∆s(y-x)'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            resdr = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'],-1*temp['∆s(y-x)'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            o1tab.at[myindo1,'delta_s-slope_original']  = resdo[0]\n",
    "            o1tab.at[myindo1,'delta_s-slope_reversion']  = resdr[0]\n",
    "            \n",
    "            myindo1 = myindo1 + 1\n",
    "\n",
    "for i in np.arange(len(o1tab)):\n",
    "    if abs(o1tab.loc[i,'slope']) > 1:\n",
    "        o1tab.at[i,'abs_slope'] = 1/o1tab.loc[i,'slope']\n",
    "        o1tab.at[i,'abs_stderr(slope)'] = abs(o1tab.loc[i,'stderr(slope)']/(o1tab.loc[i,'slope']**2))\n",
    "    else:\n",
    "        o1tab.at[i,'abs_slope'] = o1tab.loc[i,'slope']\n",
    "        o1tab.at[i,'abs_stderr(slope)'] = o1tab.loc[i,'stderr(slope)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Figure 3B I think 2021.03.18\n",
    "plt.scatter(abs(o1tab['additive_s']),o1tab['abs_slope'],alpha=0.5)\n",
    "plt.ylim(-0.25,1.02)\n",
    "#plt.ylim(1/1.2,1.2)\n",
    "plt.axhline(y=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.axhline(y=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.xlabel('|s|')\n",
    "plt.ylabel('< 1 version of slope')\n",
    "plt.show()\n",
    "\n",
    "# updated version of the above\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(2,2), gridspec_kw={'width_ratios': [2,1]},constrained_layout=True,sharey=True)\n",
    "axes[0].scatter(abs(o1tab['additive_s']),o1tab['abs_slope'],alpha=0.5,s=5)\n",
    "axes[0].set_ylim(-0.25,1.04)\n",
    "axes[0].axhline(y=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].axhline(y=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].set_xlabel('|additive effect|')\n",
    "axes[0].set_ylabel('regression slope')\n",
    "\n",
    "axes[1].hist(o1tab['abs_slope'],orientation='horizontal',bins=50)\n",
    "axes[1].set_xlabel('count')\n",
    "right_side = axes[1].spines[\"right\"]\n",
    "top_side = axes[1].spines[\"top\"]\n",
    "bottom_side = axes[1].spines[\"bottom\"]\n",
    "right_side.set_visible(False)\n",
    "top_side.set_visible(False)\n",
    "bottom_side.set_visible(False)\n",
    "\n",
    "fig.savefig('msfigs/Fig3/slopesAdds.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(o1tab['slope'],bins=100)\n",
    "plt.xlabel('slope')\n",
    "plt.show()\n",
    "print('mean slope = '+str(o1tab['slope'].mean()))\n",
    "print('median slope = '+str(o1tab['slope'].median()))\n",
    "o1tab.sort_values(by='slope')\n",
    "plt.hist(o1tab.loc[(o1tab['slope'] > -2.5)&(o1tab['slope'] < 2.5)]['slope'],bins=30)\n",
    "plt.xlabel('slope')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(o1tab['additive_s'],o1tab['slope'])\n",
    "plt.ylim(0,2.5)\n",
    "plt.axhline(y=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('slope')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(abs(o1tab['additive_s']),o1tab['slope'])\n",
    "plt.ylim(0,2.5)\n",
    "#plt.ylim(1/1.2,1.2)\n",
    "plt.axhline(y=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.xlabel('|s|')\n",
    "plt.ylabel('slope')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "myu = 1.05\n",
    "myl = 1/myu\n",
    "myumoreprop = len(o1tab.loc[(o1tab['slope']>myu)])/len(o1tab)\n",
    "myllessprop = len(o1tab.loc[(o1tab['slope']<myl)])/len(o1tab)\n",
    "print('proportion of slopes above '+str(myu)+' = '+str(myumoreprop))\n",
    "print('proportion of slopes below '+str(myl)+' = '+str(myllessprop))\n",
    "print('sum of these is '+str(myumoreprop+myllessprop))\n",
    "\n",
    "len(o1tab.loc[(abs(o1tab['additive_s'])>0.005)&(o1tab['slope']<=myu)&(o1tab['slope']>=myl)])/len(o1tab)\n",
    "\n",
    "\n",
    "plt.scatter(o1tab['epmagsum'],o1tab['abs_slope'],alpha=0.6)\n",
    "#plt.ylim(0,2.5)\n",
    "plt.xlabel('sum of magnitude of epistatic terms')\n",
    "plt.ylabel('> 1 version of slope')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(o1tab['epmagsum'],o1tab['delta_s-slope_original'],alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(o1tab['epmagsum'],o1tab['delta_s-slope_reversion'],alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(o1tab['delta_s-slope_original'],o1tab['delta_s-slope_reversion'],alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(o1tab['slope'],o1tab['delta_s-slope_original'],alpha=0.6)\n",
    "plt.xlabel('abc slope')\n",
    "plt.ylabel('∆s_orig slope')\n",
    "plt.axhline(y=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.xlim(-0.5,2.5)\n",
    "plt.xlim(0.7,1.3)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(o1tab['slope'],o1tab['delta_s-slope_reversion'],alpha=0.6)\n",
    "plt.xlabel('abc slope')\n",
    "plt.ylabel('∆s_rev slope')\n",
    "plt.axhline(y=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "plt.xlim(-0.5,2.5)\n",
    "plt.xlim(0.7,1.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead, want 3B to be with observed data, slopes (but still additive effects)\n",
    "# So go through and create a version of o1tab with these values in them\n",
    " \n",
    "gowith = ['lasso_v2',11]\n",
    "#THIS TAKES A WHILE TO RUN TOO\n",
    "# Get the data\n",
    "\n",
    "obstab = pd.DataFrame()\n",
    "\n",
    "myindo1 = 0\n",
    "\n",
    "o = 10\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):    \n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),'s','stderr(s)'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        # Since we're just using the observed s, remove the pred columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)]).reset_index(drop=True)\n",
    "        \n",
    "        # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "        # represented multiple times.\n",
    "        glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "        data2 = pd.DataFrame()\n",
    "\n",
    "        for g in np.arange(len(glist)):\n",
    "            tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "            if len(tempg) == 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg.loc[0,'s']\n",
    "                data2.at[g,'stderr(s)'] = tempg.loc[0,'stderr(s)']\n",
    "            elif len(tempg) > 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg['s'].mean()\n",
    "                #my_svar = statistics.variance(tempg['s'])\n",
    "                my_svar = 0\n",
    "                mymean_stderr = np.mean(tempg['stderr(s)']**2)\n",
    "                data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "        data1 = data2\n",
    "\n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        # So we can plot in different ways, import the background-averaged fitness effects of each mutation\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(gowith[1]-1)+'.txt',\n",
    "                                     sep='\\t',names=['todelete','genotype','coeff','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "\n",
    "        # Just additive effects!\n",
    "        data0 = data0.loc[(data0['numMut'] == 1)].sort_values(by='genotype',ascending=False).reset_index(drop=True)\n",
    "\n",
    "        mysize = 15\n",
    "        myalpha = 0.5\n",
    "        \n",
    "        # Divvy up the data by focal locus\n",
    "        #for l in np.arange(5,6):\n",
    "        for l in np.arange(len(floci)):\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "\n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "            \n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "            \n",
    "            # Perform a linear regression, capturing slope and intercept\n",
    "            yorkn = 100\n",
    "            #res = linregress(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],temp['stderr(s)_x'],temp['stderr(s)_y'],yorkn)\n",
    "            obstab.at[myindo1,'ploidy'] = ploidies[p]\n",
    "            obstab.at[myindo1,'envt'] = envts[e]\n",
    "            obstab.at[myindo1,'locus'] = floci[l]\n",
    "            obstab.at[myindo1,'slope'] = res[0]\n",
    "            obstab.at[myindo1,'intercept'] = res[1]\n",
    "            obstab.at[myindo1,'stderr(slope)'] = np.sqrt(res[2])\n",
    "            obstab.at[myindo1,'additive_s'] = data0.loc[(data0[floci[l]] == 1)&(data0['numMut'] == 1),'coeff'].values[0]\n",
    "            \n",
    "            #resdo = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp['∆s(y-x)'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            #resdr = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'],-1*temp['∆s(y-x)'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            #obstab.at[myindo1,'delta_s-slope_original']  = resdo[0]\n",
    "            #obstab.at[myindo1,'delta_s-slope_reversion']  = resdr[0]\n",
    "            \n",
    "            myindo1 = myindo1 + 1\n",
    "\n",
    "for i in np.arange(len(obstab)):\n",
    "    if abs(obstab.loc[i,'slope']) > 1:\n",
    "        obstab.at[i,'abs_slope'] = 1/obstab.loc[i,'slope']\n",
    "        obstab.at[i,'abs_stderr(slope)'] = abs(obstab.loc[i,'stderr(slope)']/(obstab.loc[i,'slope']**2))\n",
    "    else:\n",
    "        obstab.at[i,'abs_slope'] = obstab.loc[i,'slope']\n",
    "        obstab.at[i,'abs_stderr(slope)'] = obstab.loc[i,'stderr(slope)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip the x axis\n",
    "# Same as immediately above except rotated 90°\n",
    "# Do updated version of fig 3B with OBSERVED data\n",
    "\n",
    "# remake this with the standard errors of the slopes as yerror\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(1.38,.9), gridspec_kw={'height_ratios': [1.2,2]},sharex=True)\n",
    "fig.subplots_adjust(hspace=0.25)\n",
    "axes[1].errorbar(obstab['abs_slope'],abs(obstab['additive_s']),\n",
    "                 xerr = obstab['abs_stderr(slope)'], yerr = None,\n",
    "                 alpha=0.5,elinewidth=0.5,linestyle='None',marker='.',ms=3)\n",
    "#axes[1].scatter(abs(obstab['additive_s']),obstab['abs_slope'],alpha=0.5,s=5)\n",
    "axes[1].set_xlim(1.04,-0.25)\n",
    "axes[1].set_ylim(-0.005,0.11)\n",
    "axes[1].axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[1].axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[1].axvline(x=0.9,color='xkcd:orange',zorder=0,lw=0.5,linestyle = 'dashed')\n",
    "axes[1].axhline(y=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[1].set_ylabel('\\n'.join(wrap('|additive effect|',9)))\n",
    "axes[1].set_xlabel('regression slope, $b$',labelpad=3)\n",
    "axes[1].set_yticks([0,0.1])\n",
    "axes[1].tick_params(axis='x',pad=1.5)\n",
    "\n",
    "\n",
    "axes[0].hist(obstab['abs_slope'],bins=50)\n",
    "axes[0].set_ylabel('count')\n",
    "axes[0].axvline(x=0.9,color='xkcd:orange',zorder=0,lw=0.5,linestyle = 'dashed')\n",
    "right_side = axes[0].spines[\"right\"]\n",
    "top_side = axes[0].spines[\"top\"]\n",
    "left_side = axes[0].spines[\"left\"]\n",
    "right_side.set_visible(False)\n",
    "top_side.set_visible(False)\n",
    "left_side.set_visible(False)\n",
    "\n",
    "fig.align_ylabels()\n",
    "\n",
    "fig.savefig('msfigs/Fig3/slopesAdds_errorbar_v2_flip_flip.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to do ABC vs aBC plots now where, instead of subtracting terms by rank,\n",
    "# we add them by rank.\n",
    "\n",
    "gonna take ages (about 3.5h)\n",
    "#time the code\n",
    "mystart = time.perf_counter()\n",
    "\n",
    "megaadder = pd.DataFrame()\n",
    "o=10\n",
    "ncyc = 30\n",
    "ncycthresh = 2\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "diffthresh = 0.01\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):\n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "        \n",
    "        print(ploidies[p]+'_'+envts[e])\n",
    "        \n",
    "        # First, import a list of genotypes to predict\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        # Since we're just using the predicted s, remove the obs columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "\n",
    "        # Create a column for each locus\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "        \n",
    "        # Now import a set of coefficients\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                                             sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_term','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "        \n",
    "        # Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "        for i in np.arange(len(data0)):\n",
    "            if data0.loc[i,'numMut'] % 2 == 1:\n",
    "                data0.at[i,'baseline'] = -1\n",
    "            else:\n",
    "                data0.at[i,'baseline'] = 1\n",
    "        \n",
    "        \n",
    "        #for l in np.arange(2,3):\n",
    "        for l in np.arange(len(floci)):\n",
    "        \n",
    "            # Want to start with \"no-epistasis\" predictions in data1\n",
    "            # builder is where we build up coefficients for fitness predictions\n",
    "            # Start by setting all epistatic terms INVOLVING THE FOCAL LOCUS to 0\n",
    "\n",
    "            builder = data0.copy(deep=True)\n",
    "            builder.at[(builder['numMut'] > 1)&(builder[floci[l]] == 1),ploidies[p]+'_'+envts[e]+'_term'] = 0\n",
    "\n",
    "            for g in np.arange(len(data1)):\n",
    "                temp = builder.copy(deep=True)\n",
    "                # to make life faster, remove all zero values\n",
    "                temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                for t in np.arange(len(temp)):\n",
    "                    temprowlist = []\n",
    "                    for locus in np.arange(len(floci)):\n",
    "                        temprowlist = temprowlist + [temp.loc[t,floci[locus]]*data1.loc[g,floci[locus]]]\n",
    "                    #remove zeros\n",
    "                    temprowlist = [value for value in temprowlist if value != 0]\n",
    "                    # find product\n",
    "                    firstprod = np.prod(temprowlist)\n",
    "                    # subtract out baseline\n",
    "                    firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                    # multiply by term's value\n",
    "                    mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                    temp.at[t,'tosum'] = mytermval\n",
    "                totalsum = temp['tosum'].sum()\n",
    "                myintercept = data1.loc[(data1['genotype'] == '0000000000'),ploidies[p]+'_'+envts[e]+'_'+'Alex prediction'].values[0]\n",
    "                data1.at[g,'noEp_pred'] = totalsum + myintercept\n",
    "            \n",
    "            locusadder = pd.DataFrame()\n",
    "            \n",
    "            tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','noEp_pred','without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','noEp_pred','without_'+floci[l]]]\n",
    "\n",
    "            templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "                \n",
    "            # Get the 1:1 R^2 value\n",
    "            my_x = templ['noEp_pred_x']\n",
    "            my_y = templ['noEp_pred_y']\n",
    "            #my_y_mean = np.mean(my_y)\n",
    "            #my_x_mean = np.mean(my_x)\n",
    "            #TSS = np.sum((my_y - my_y_mean)**2)\n",
    "            #RSS = np.sum((my_y - my_y_mean - my_x + my_x_mean)**2)\n",
    "            \n",
    "            locusadder.at[0,'num term add'] = 0\n",
    "            locusadder.at[0,'term added'] = np.nan\n",
    "            locusadder.at[0,'term order'] = np.nan\n",
    "            locusadder.at[0,'coefficient'] = np.nan\n",
    "            #locusadder.at[0,'1:1 R2'] = 1-RSS/TSS\n",
    "            \n",
    "            # Perform standard least-squares linear regression\n",
    "            #myreg = linregress(my_x,my_y)\n",
    "            myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "            locusadder.at[0,'abc-original_slope'] = myreg[0]\n",
    "            locusadder.at[0,'abc-original_intercept'] = myreg[1]\n",
    "            myreg = yorkreg_nocorr(my_x,my_y-my_x,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "            locusadder.at[0,'deltas-original_slope'] = myreg[0]\n",
    "            locusadder.at[0,'deltas-original_intercept'] = myreg[1]\n",
    "            myreg = yorkreg_nocorr(my_y,my_x-my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "            locusadder.at[0,'deltas-reversion_slope'] = myreg[0]\n",
    "            locusadder.at[0,'deltas-reversion_intercept'] = myreg[1]\n",
    "            #locusadder.at[0,'linreg r2'] = myreg.rvalue**2\n",
    "            #locusadder.at[0,'stderr_slope'] = myreg.stderr\n",
    "            #locusadder.at[0,'stderr_intercept'] = myreg.intercept_stderr\n",
    "            # Compare this slope to 1\n",
    "            #myt = (myreg.slope - 1) / (myreg.stderr - 0)\n",
    "            #locusadder.at[0,'t_stat'] = myt\n",
    "            #mydf = len(templ) - 2\n",
    "            # two-sided t test p value\n",
    "            #myp = stats.t.sf(np.abs(myt), mydf)*2\n",
    "            #locusadder.at[0,'p_val'] = myp\n",
    "            \n",
    "            #plt.scatter(my_x,my_y)\n",
    "            #plt.show()\n",
    "            \n",
    "            # Begin cycles\n",
    "            data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[floci[l]] == 1)]\n",
    "            data0adder['abs coefficient'] = abs(data0adder[ploidies[p]+'_'+envts[e]+'_term'])\n",
    "            data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "            \n",
    "            for n in np.arange(ncyc):\n",
    "                # add the strongest epistatic coefficient that involves locus l\n",
    "                topind = data0adder.index[0]\n",
    "                builder.at[topind,ploidies[p]+'_'+envts[e]+'_term'] = data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                \n",
    "                # log coefficient info in locusadder\n",
    "                locusadder.at[n+1,'num term add'] = n+1\n",
    "                locusadder.at[n+1,'term added'] = data0adder.loc[topind,'genotype'] \n",
    "                locusadder.at[n+1,'term order'] = data0adder.loc[topind,'numMut']\n",
    "                locusadder.at[n+1,'coefficient'] = data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                \n",
    "                # Estimate the new predicted value for each genotype\n",
    "                for g in np.arange(len(data1)):\n",
    "                    temp = builder.copy(deep=True)\n",
    "                    # to make life faster, remove all zero values\n",
    "                    temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                    for t in np.arange(len(temp)):\n",
    "                        temprowlist = []\n",
    "                        for locus in np.arange(len(floci)):\n",
    "                            temprowlist = temprowlist + [temp.loc[t,floci[locus]]*data1.loc[g,floci[locus]]]\n",
    "                        #remove zeros\n",
    "                        temprowlist = [value for value in temprowlist if value != 0]\n",
    "                        # find product\n",
    "                        firstprod = np.prod(temprowlist)\n",
    "                        # subtract out baseline\n",
    "                        firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                        # multiply by term's value\n",
    "                        mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                        temp.at[t,'tosum'] = mytermval\n",
    "                    totalsum = temp['tosum'].sum()\n",
    "                    myintercept = data1.loc[(data1['genotype'] == '0000000000'),ploidies[p]+'_'+envts[e]+'_'+'Alex prediction'].values[0]\n",
    "                    data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "                    \n",
    "                # For the new predictions, get the values I want\n",
    "                tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab0 = tab0[['genotype','new_pred','without_'+floci[l]]]\n",
    "                tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab1 = tab1[['genotype','new_pred','without_'+floci[l]]]\n",
    "\n",
    "                templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "                for i in np.arange(len(floci)):\n",
    "                    templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "                # Get the 1:1 R^2 value\n",
    "                my_x = templ['new_pred_x']\n",
    "                my_y = templ['new_pred_y']\n",
    "                #my_y_mean = np.mean(my_y)\n",
    "                #my_x_mean = np.mean(my_x)\n",
    "                #TSS = np.sum((my_y - my_y_mean)**2)\n",
    "                #RSS = np.sum((my_y - my_y_mean - my_x + my_x_mean)**2)\n",
    "                \n",
    "                #locusadder.at[n+1,'1:1 R2'] = 1-RSS/TSS\n",
    "\n",
    "                # Perform standard least-squares linear regression\n",
    "                #myreg = linregress(my_x,my_y)\n",
    "                myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "                locusadder.at[n+1,'abc-original_slope'] = myreg[0]\n",
    "                locusadder.at[n+1,'abc-original_intercept'] = myreg[1]\n",
    "                myreg = yorkreg_nocorr(my_x,my_y-my_x,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "                locusadder.at[n+1,'deltas-original_slope'] = myreg[0]\n",
    "                locusadder.at[n+1,'deltas-original_intercept'] = myreg[1]\n",
    "                myreg = yorkreg_nocorr(my_y,my_x-my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "                locusadder.at[n+1,'deltas-reversion_slope'] = myreg[0]\n",
    "                locusadder.at[n+1,'deltas-reversion_intercept'] = myreg[1]\n",
    "                #locusadder.at[n+1,'slope'] = myreg.slope\n",
    "                #locusadder.at[n+1,'intercept'] = myreg.intercept\n",
    "                #locusadder.at[n+1,'linreg r2'] = myreg.rvalue**2\n",
    "                #locusadder.at[n+1,'stderr_slope'] = myreg.stderr\n",
    "                #locusadder.at[n+1,'stderr_intercept'] = myreg.intercept_stderr\n",
    "                # Compare this slope to 1\n",
    "                #myt = (myreg.slope - 1) / (myreg.stderr - 0)\n",
    "                #locusadder.at[n+1,'t_stat'] = myt\n",
    "                #mydf = len(templ) - 2\n",
    "                # two-sided t test p value\n",
    "                #myp = stats.t.sf(np.abs(myt), mydf)*2\n",
    "                #locusadder.at[n+1,'p_val'] = myp\n",
    "                \n",
    "                # Remove strongest epistatic coefficient from data0adder\n",
    "                data0adder = data0adder[1:]\n",
    "                \n",
    "                #plt.scatter(my_x,my_y)\n",
    "                #plt.plot(np.linspace(-0.4,0.4),np.linspace(-0.4,0.4))\n",
    "                #plt.plot(np.linspace(-0.4,0.4),np.linspace(-0.4,0.4)*myreg[0]+myreg[1])\n",
    "                #plt.show()\n",
    "                \n",
    "                \n",
    "                if n >= ncycthresh:\n",
    "                    diff3 = abs(locusadder.loc[n+1,'abc-original_slope'] - locusadder.loc[n-2,'abc-original_slope'])\n",
    "                    diff2 = abs(locusadder.loc[n+1,'abc-original_slope'] - locusadder.loc[n-1,'abc-original_slope'])\n",
    "                    diff1 = abs(locusadder.loc[n+1,'abc-original_slope'] - locusadder.loc[n,'abc-original_slope'])\n",
    "                    \n",
    "                    if diff3 <= diffthresh and diff2 <= diffthresh and diff1 <= diffthresh:\n",
    "                        diff3do = abs(locusadder.loc[n+1,'deltas-original_slope'] - locusadder.loc[n-2,'deltas-original_slope'])\n",
    "                        diff2do = abs(locusadder.loc[n+1,'deltas-original_slope'] - locusadder.loc[n-1,'deltas-original_slope'])\n",
    "                        diff1do = abs(locusadder.loc[n+1,'deltas-original_slope'] - locusadder.loc[n,'deltas-original_slope'])\n",
    "                        \n",
    "                        if diff3do <= diffthresh and diff2do <= diffthresh and diff1do <= diffthresh:\n",
    "                            diff3dr = abs(locusadder.loc[n+1,'deltas-reversion_slope'] - locusadder.loc[n-2,'deltas-reversion_slope'])\n",
    "                            diff2dr = abs(locusadder.loc[n+1,'deltas-reversion_slope'] - locusadder.loc[n-1,'deltas-reversion_slope'])\n",
    "                            diff1dr = abs(locusadder.loc[n+1,'deltas-reversion_slope'] - locusadder.loc[n,'deltas-reversion_slope'])\n",
    "                            \n",
    "                            if diff3dr <= diffthresh and diff2dr <= diffthresh and diff1dr <= diffthresh:\n",
    "                                break\n",
    " \n",
    "            locusadder.at[1:,'term added'] = locusadder.loc[1:,'term added'].astype(int).astype(str).str.zfill(10)\n",
    "            locusadder.insert(0,'main locus', floci[l])\n",
    "            locusadder.insert(0,'envt',envts[e])\n",
    "            locusadder.insert(0,'ploidy',ploidies[p])\n",
    "            \n",
    "            megaadder = megaadder.append(locusadder)\n",
    "            print(floci[l])\n",
    "\n",
    "mystop = time.perf_counter()\n",
    "elapsed = mystop-mystart\n",
    "print(str(elapsed)+' seconds to run')\n",
    "                \n",
    "#export_csv = megaadder.to_csv(r'20210324_megaadder_haphom_withdeltas.csv',index=True,header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the full-data prediction slopes\n",
    "gowith = ['lasso_v2',11]\n",
    "# Get the data\n",
    "o1tab = pd.DataFrame()\n",
    "\n",
    "myindo1 = 0\n",
    "\n",
    "o = 10\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):    \n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        # Since we're just using the predicted s, remove the obs columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "\n",
    "        # Create a column for each locus, and for the genotype with that locus removed\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l]\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        # So we can plot in different ways, import the background-averaged fitness effects of each mutation\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(gowith[1]-1)+'.txt',\n",
    "                                     sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(gowith[1]-1),'na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "\n",
    "        #data0 = data0.loc[(data0['numMut'] == 1)].sort_values(by='genotype',ascending=False).reset_index(drop=True)\n",
    "\n",
    "        mysize = 15\n",
    "        myalpha = 0.5\n",
    "\n",
    "        # Divvy up the data by focal locus\n",
    "        #for l in np.arange(2,3):\n",
    "        for l in np.arange(len(floci)):\n",
    "\n",
    "            tab0 = data1.loc[(data1[floci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),'without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype',ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o),'without_'+floci[l]]]\n",
    "\n",
    "            temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "            \n",
    "            for i in np.arange(len(floci)):\n",
    "                temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "            \n",
    "            temp['∆s(y-x)'] = temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'] - temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x']\n",
    "\n",
    "            # Perform a linear regression, capturing slope and intercept\n",
    "            yorkn = 100\n",
    "            #res = linregress(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'])\n",
    "            res = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            o1tab.at[myindo1,'ploidy'] = ploidies[p]\n",
    "            o1tab.at[myindo1,'envt'] = envts[e]\n",
    "            o1tab.at[myindo1,'main locus'] = floci[l]\n",
    "            o1tab.at[myindo1,'fullmodel_slope'] = res[0]\n",
    "            #o1tab.at[myindo1,'intercept'] = res[1]\n",
    "            #o1tab.at[myindo1,'additive_s'] = data0.loc[(data0[floci[l]] == 1)&(data0['numMut'] == 1),ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)].values[0]\n",
    "            #o1tab.at[myindo1,'epmagsum'] = abs(data0.loc[(data0[floci[l]] == 1)&(data0['numMut'] > 1),ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)]).sum()\n",
    "            \n",
    "            #resdo = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x'],temp['∆s(y-x)'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            #resdr = yorkreg_nocorr(temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'],-1*temp['∆s(y-x)'],[0]*len(temp),[0]*len(temp),yorkn)\n",
    "            #o1tab.at[myindo1,'delta_s-slope_original']  = resdo[0]\n",
    "            #o1tab.at[myindo1,'delta_s-slope_reversion']  = resdr[0]\n",
    "            \n",
    "            if abs(res[0]) > 1:\n",
    "                o1tab.at[myindo1,'fullmodel_slope_-1to1'] = 1/res[0]\n",
    "            else:\n",
    "                o1tab.at[myindo1,'fullmodel_slope_-1to1'] = res[0]\n",
    "            \n",
    "            \n",
    "            myindo1 = myindo1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fullmodel_slope_-1to1\n",
    "# Do the same as above but use the fullmodel_slope_-1to1 instead of the convergence slope\n",
    "\n",
    "# Analyze the megaadder\n",
    "ma = pd.read_csv('20210324_megaadder_haphom_withdeltas.csv')\n",
    "ma = ma.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "#ma = megaadder.copy(deep=True).reset_index(drop=True)\n",
    "diffthresh = 0.01\n",
    "yorkn = 100\n",
    "\n",
    "# Create a modified slope column that takes the reciprocal if abs(abc slope) > 1\n",
    "for i in np.arange(len(ma)):\n",
    "    if abs(ma.loc[i,'abc-original_slope']) > 1:\n",
    "        ma.at[i,'abc-original_slope_-1to1'] = 1/ma.loc[i,'abc-original_slope']\n",
    "    else:\n",
    "        ma.at[i,'abc-original_slope_-1to1'] = ma.loc[i,'abc-original_slope']\n",
    "\n",
    "# Now create a column that says whether a given locus is a turning point.\n",
    "# Create a second version of dataframe, ma2b, which doesn't contain any rows beyond the turning point.\n",
    "ma2 = pd.DataFrame()\n",
    "ma2b = pd.DataFrame()\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for l in np.arange(len(floci)):\n",
    "            temp = ma.loc[(ma['ploidy'] == ploidies[p])&(ma['envt'] == envts[e])&(ma['main locus'] == floci[l])].copy(deep=True).reset_index(drop=True)\n",
    "            for i in np.arange(len(temp)):\n",
    "                if i > 2:\n",
    "                    diff1 = abs(temp.loc[i,'abc-original_slope_-1to1'] - temp.loc[i-1,'abc-original_slope_-1to1'])\n",
    "                    diff2 = abs(temp.loc[i,'abc-original_slope_-1to1'] - temp.loc[i-2,'abc-original_slope_-1to1'])\n",
    "                    diff3 = abs(temp.loc[i,'abc-original_slope_-1to1'] - temp.loc[i-3,'abc-original_slope_-1to1'])\n",
    "                    if diff1 <= diffthresh and diff2 <= diffthresh and diff3 <= diffthresh:\n",
    "                        temp.at[i-3,'d(ds/ds) < diffthresh'] = 'y'\n",
    "                        temp.at[i-3,'numterms to d(ds/ds) < diffthresh'] = i-3\n",
    "                        temp.at[i-3,'sumtermscoeff to d(ds/ds) < diffthresh'] = temp.loc[0:i-3,'coefficient'].sum()\n",
    "                        break\n",
    "                        \n",
    "            ma2 = ma2.append(temp,sort=False)\n",
    "            ma2b=ma2b.append(temp.loc[0:i-3])\n",
    "\n",
    "# Condense ma2 to the most relevant rows\n",
    "ma3 = ma2.loc[ma2['d(ds/ds) < diffthresh'] == 'y'].reset_index(drop=True)\n",
    "ma3 = pd.merge(ma3,o1tab,how='left',on=['ploidy','envt','main locus'])\n",
    "\n",
    "# create a column comparing the d(ds/ds) < diffthresh slope to the full model slope\n",
    "ma3['abs(diff): abc-original_full vs final slope'] = abs(ma3['fullmodel_slope_-1to1'] - ma3['abc-original_slope_-1to1'])\n",
    "\n",
    "#look at the differences\n",
    "#ma3.sort_values(by='abs(diff): full vs final slope',ascending=False)\n",
    "\n",
    "# remove extraneous columns\n",
    "ma3 = ma3.drop(columns=['num term add','term added','term order','coefficient','abc-original_intercept'])\n",
    "\n",
    "sthresh = 0.9\n",
    "mybins = list(np.arange(0,13))\n",
    "\n",
    "# Plot histogram for slopes <= sthresh\n",
    "ma3t = ma3.loc[ma3['fullmodel_slope_-1to1'] <= sthresh].copy(deep=True).reset_index(drop=True)\n",
    "ma3supert = ma3.loc[ma3['fullmodel_slope_-1to1'] > sthresh].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "# DEFINE FIGURE\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(1.27,1.15*1.2), gridspec_kw={'width_ratios': [2.9,1]},sharey=True)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "for i in np.arange(len(ma3)):\n",
    "    if ma3.loc[i,'fullmodel_slope_-1to1'] <= sthresh:\n",
    "        axes[0].scatter(ma3.loc[i,'fullmodel_slope_-1to1'],ma3.loc[i,'numterms to d(ds/ds) < diffthresh'],alpha=0.5,s=3,color='xkcd:orange')\n",
    "    else:\n",
    "        axes[0].scatter(ma3.loc[i,'fullmodel_slope_-1to1'],ma3.loc[i,'numterms to d(ds/ds) < diffthresh'],alpha=0.5,s=3,color='xkcd:cerulean')\n",
    "#axes[0].scatter(ma3['abc-original_slope_-1to1'],ma3['numterms to d(ds/ds) < diffthresh'],alpha=0.5,s=5)\n",
    "#axes[0].set_ylim(-0.25,1.04)\n",
    "axes[0].axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].set_xlabel('\\n'.join(wrap('full model regression slope, $b$',11)))\n",
    "#axes[0].set_ylabel('\\n'.join(wrap('number of terms necessary',15)))\n",
    "axes[0].set_ylabel('\\n'.join(wrap('# terms sufficient to reach global $b$',19)),labelpad=0.2)\n",
    "axes[0].set_yticks([0,2,4,6,8,10,12])\n",
    "\n",
    "axes[1].hist(ma3t['numterms to d(ds/ds) < diffthresh'],bins=mybins,orientation='horizontal',color='xkcd:orange',alpha=0.7)\n",
    "axes[1].hist(ma3supert['numterms to d(ds/ds) < diffthresh'],bins=mybins,orientation='horizontal',color='xkcd:cerulean',zorder=0,alpha=0.7)\n",
    "axes[1].set_xlabel('count')\n",
    "axes[1].axhline(y=ma3t['numterms to d(ds/ds) < diffthresh'].mean(),color='xkcd:dark orange',lw=1)\n",
    "axes[1].axhline(y=ma3supert['numterms to d(ds/ds) < diffthresh'].mean(),color='xkcd:blue',lw=1)\n",
    "right_side = axes[1].spines[\"right\"]\n",
    "top_side = axes[1].spines[\"top\"]\n",
    "bottom_side = axes[1].spines[\"bottom\"]\n",
    "right_side.set_visible(False)\n",
    "top_side.set_visible(False)\n",
    "bottom_side.set_visible(False)\n",
    "\n",
    "dot1 = Line2D([], [], color='xkcd:orange', marker='.', linestyle='None',\n",
    "                          markersize=5, label='$b$ ≤ 0.9')\n",
    "dot2 = Line2D([], [], color='xkcd:cerulean', marker='.', linestyle='None',\n",
    "                          markersize=5, label='0.9 < $b$ ≤ 1')\n",
    "\n",
    "fig.legend(handles=[dot1,dot2],handletextpad=-0.4,borderpad=0.2,labelspacing=0.1,loc='upper left',bbox_to_anchor=(0.33,1.44),frameon=False)\n",
    "\n",
    "fig.savefig('msfigs/Fig3/slopesNumterms_moreinhistbutnottoomuch_v4.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to look at the % of total non-zero terms interacting with the focal locus that are added\n",
    "# Also want to look at what % of interaction-term variance that represents\n",
    "\n",
    "# Analyze the megaadder\n",
    "ma = pd.read_csv('20210324_megaadder_haphom_withdeltas.csv')\n",
    "ma = ma.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "# add variance explained\n",
    "ma['varexp'] = ma['coefficient']**2\n",
    "\n",
    "# add cumulative variance\n",
    "ma1 = pd.DataFrame()\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for l in np.arange(len(floci)):\n",
    "            temp = ma.loc[(ma['ploidy'] == ploidies[p])&(ma['envt'] == envts[e])&(ma['main locus'] == floci[l])].reset_index(drop=True)\n",
    "            my_b = 0\n",
    "            temp.at[0,'varexp_cum'] = my_b\n",
    "            for i in np.arange(1,len(temp)):\n",
    "                my_b = my_b + temp.loc[i,'varexp']\n",
    "                temp.at[i,'varexp_cum'] = my_b\n",
    "            ma1 = ma1.append(temp)\n",
    "ma = ma1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#ma = megaadder.copy(deep=True).reset_index(drop=True)\n",
    "diffthresh = 0.01\n",
    "yorkn = 100\n",
    "\n",
    "# Create a modified slope column that takes the reciprocal if abs(abc slope) > 1\n",
    "for i in np.arange(len(ma)):\n",
    "    if abs(ma.loc[i,'abc-original_slope']) > 1:\n",
    "        ma.at[i,'abc-original_slope_-1to1'] = 1/ma.loc[i,'abc-original_slope']\n",
    "    else:\n",
    "        ma.at[i,'abc-original_slope_-1to1'] = ma.loc[i,'abc-original_slope']\n",
    "\n",
    "# Now create a column that says whether a given locus is a turning point.\n",
    "# Create a second version of dataframe, ma2b, which doesn't contain any rows beyond the turning point.\n",
    "ma2 = pd.DataFrame()\n",
    "ma2b = pd.DataFrame()\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for l in np.arange(len(floci)):\n",
    "            temp = ma.loc[(ma['ploidy'] == ploidies[p])&(ma['envt'] == envts[e])&(ma['main locus'] == floci[l])].copy(deep=True).reset_index(drop=True)\n",
    "            for i in np.arange(len(temp)):\n",
    "                if i > 2:\n",
    "                    diff1 = abs(temp.loc[i,'abc-original_slope_-1to1'] - temp.loc[i-1,'abc-original_slope_-1to1'])\n",
    "                    diff2 = abs(temp.loc[i,'abc-original_slope_-1to1'] - temp.loc[i-2,'abc-original_slope_-1to1'])\n",
    "                    diff3 = abs(temp.loc[i,'abc-original_slope_-1to1'] - temp.loc[i-3,'abc-original_slope_-1to1'])\n",
    "                    if diff1 <= diffthresh and diff2 <= diffthresh and diff3 <= diffthresh:\n",
    "                        temp.at[i-3,'d(ds/ds) < diffthresh'] = 'y'\n",
    "                        temp.at[i-3,'numterms to d(ds/ds) < diffthresh'] = i-3\n",
    "                        temp.at[i-3,'sumtermscoeff to d(ds/ds) < diffthresh'] = temp.loc[0:i-3,'coefficient'].sum()\n",
    "                        break\n",
    "                        \n",
    "            ma2 = ma2.append(temp,sort=False)\n",
    "            ma2b=ma2b.append(temp.loc[0:i-3])\n",
    "\n",
    "# Condense ma2 to the most relevant rows\n",
    "ma3 = ma2.loc[ma2['d(ds/ds) < diffthresh'] == 'y'].reset_index(drop=True)\n",
    "ma3 = pd.merge(ma3,o1tab,how='left',on=['ploidy','envt','main locus'])\n",
    "\n",
    "# create a column comparing the d(ds/ds) < diffthresh slope to the full model slope\n",
    "ma3['abs(diff): abc-original_full vs final slope'] = abs(ma3['fullmodel_slope_-1to1'] - ma3['abc-original_slope_-1to1'])\n",
    "\n",
    "# For each row in ma3, add in the # of non-zero terms that COULD have been added, as well as the total variance that COULD have been added\n",
    "for p in np.arange(len(ploidies)):    \n",
    "    for e in np.arange(len(envts)):\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/'+gowith[0]+'_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(gowith[1]-1)+'.txt',\n",
    "                                     sep='\\t',names=['todelete','genotype','coeff','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "        \n",
    "        for l in np.arange(len(floci)):\n",
    "            reltab = data0.loc[(data0['numMut'] > 1)&(data0[floci[l]] == 1)]\n",
    "            totalvarexp = (reltab['coeff']**2).sum()\n",
    "            totalnonzeroterms = len(reltab.loc[reltab['coeff'] != 0])\n",
    "            ma3.at[(ma3['ploidy'] == ploidies[p])&(ma3['envt'] == envts[e])&(ma3['main locus'] == floci[l]),'total potentially added nonzero terms'] = totalnonzeroterms\n",
    "            ma3.at[(ma3['ploidy'] == ploidies[p])&(ma3['envt'] == envts[e])&(ma3['main locus'] == floci[l]),'total potentially added variance'] = totalvarexp\n",
    "\n",
    "# add % potential variance explained of the added terms\n",
    "ma3['% potential terms'] = ma3['numterms to d(ds/ds) < diffthresh']/ma3['total potentially added nonzero terms']\n",
    "ma3['% potential varexp'] = ma3['varexp_cum']/ma3['total potentially added variance']\n",
    "ma3['convergence - full model -1 to 1 slopes'] = ma3['abc-original_slope_-1to1'] - ma3['fullmodel_slope_-1to1']\n",
    "\n",
    "sthresh = 0.9\n",
    "\n",
    "mybins = np.arange(0,0.51,0.02)\n",
    "\n",
    "ma3t = ma3.loc[ma3['abc-original_slope_-1to1'] <= sthresh].copy(deep=True).reset_index(drop=True)\n",
    "ma3supert = ma3.loc[ma3['abc-original_slope_-1to1'] > sthresh].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "# DEFINE % TERMS FIGURE\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(1.2,1.15*1.2), gridspec_kw={'width_ratios': [2.9,1]},sharey=True)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "for i in np.arange(len(ma3)):\n",
    "    if ma3.loc[i,'abc-original_slope_-1to1'] <= sthresh:\n",
    "        axes[0].scatter(ma3.loc[i,'abc-original_slope_-1to1'],ma3.loc[i,'% potential terms'],alpha=0.5,s=3,color='xkcd:orange')\n",
    "    else:\n",
    "        axes[0].scatter(ma3.loc[i,'abc-original_slope_-1to1'],ma3.loc[i,'% potential terms'],alpha=0.5,s=3,color='xkcd:cerulean')\n",
    "#axes[0].scatter(ma3['abc-original_slope_-1to1'],ma3['numterms to d(ds/ds) < diffthresh'],alpha=0.5,s=5)\n",
    "#axes[0].set_ylim(-0.25,1.04)\n",
    "axes[0].axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].set_xlabel('\\n'.join(wrap('full model regression slope, b',11)))\n",
    "#axes[0].set_ylabel('\\n'.join(wrap('number of terms necessary',15)))\n",
    "axes[0].set_ylabel('Fraction terms necessary')\n",
    "axes[0].set_yticks(np.arange(0,0.51,0.1))\n",
    "\n",
    "axes[1].hist(ma3t['% potential terms'],orientation='horizontal',color='xkcd:orange',alpha=0.7,bins=mybins)\n",
    "axes[1].hist(ma3supert['% potential terms'],orientation='horizontal',color='xkcd:cerulean',zorder=0,alpha=0.7,bins=mybins)\n",
    "axes[1].set_xlabel('count')\n",
    "axes[1].axhline(y=ma3t['% potential terms'].mean(),color='xkcd:dark orange',lw=1)\n",
    "axes[1].axhline(y=ma3supert['% potential terms'].mean(),color='xkcd:blue',lw=1)\n",
    "right_side = axes[1].spines[\"right\"]\n",
    "top_side = axes[1].spines[\"top\"]\n",
    "bottom_side = axes[1].spines[\"bottom\"]\n",
    "right_side.set_visible(False)\n",
    "top_side.set_visible(False)\n",
    "bottom_side.set_visible(False)\n",
    "\n",
    "dot1 = Line2D([], [], color='xkcd:orange', marker='.', linestyle='None',\n",
    "                          markersize=5, label='b ≤ 0.9')\n",
    "dot2 = Line2D([], [], color='xkcd:cerulean', marker='.', linestyle='None',\n",
    "                          markersize=5, label='0.9 < b ≤ 1')\n",
    "\n",
    "fig.legend(handles=[dot1,dot2],handletextpad=-0.4,borderpad=0.2,labelspacing=0.1,loc='upper left',bbox_to_anchor=(0.25,1.41),frameon=False)\n",
    "\n",
    "fig.savefig('msfigs/Fig3/slopesNumterms_moreinhistbutnottoomuch_v3_percentterms.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "mybins = np.arange(0,1.01,0.02)\n",
    "\n",
    "# DEFINE FIGURE, NOW FOR % VARIANCE\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(1.2,1.15*1.2), gridspec_kw={'width_ratios': [2.9,1]},sharey=True)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "for i in np.arange(len(ma3)):\n",
    "    if ma3.loc[i,'abc-original_slope_-1to1'] <= sthresh:\n",
    "        axes[0].scatter(ma3.loc[i,'abc-original_slope_-1to1'],ma3.loc[i,'% potential varexp'],alpha=0.5,s=3,color='xkcd:orange')\n",
    "    else:\n",
    "        axes[0].scatter(ma3.loc[i,'abc-original_slope_-1to1'],ma3.loc[i,'% potential varexp'],alpha=0.5,s=3,color='xkcd:cerulean')\n",
    "#axes[0].scatter(ma3['abc-original_slope_-1to1'],ma3['numterms to d(ds/ds) < diffthresh'],alpha=0.5,s=5)\n",
    "#axes[0].set_ylim(-0.25,1.04)\n",
    "axes[0].axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].set_xlabel('\\n'.join(wrap('full model regression slope, b',11)))\n",
    "#axes[0].set_ylabel('\\n'.join(wrap('number of terms necessary',15)))\n",
    "axes[0].set_ylabel('Fraction variance necessary')\n",
    "axes[0].set_yticks(np.arange(0,1.01,0.2))\n",
    "\n",
    "axes[1].hist(ma3t['% potential varexp'],orientation='horizontal',color='xkcd:orange',alpha=0.7,bins=mybins)\n",
    "axes[1].hist(ma3supert['% potential varexp'],orientation='horizontal',color='xkcd:cerulean',zorder=0,alpha=0.7,bins=mybins)\n",
    "axes[1].set_xlabel('count')\n",
    "axes[1].axhline(y=ma3t['% potential varexp'].mean(),color='xkcd:dark orange',lw=1)\n",
    "axes[1].axhline(y=ma3supert['% potential varexp'].mean(),color='xkcd:blue',lw=1)\n",
    "right_side = axes[1].spines[\"right\"]\n",
    "top_side = axes[1].spines[\"top\"]\n",
    "bottom_side = axes[1].spines[\"bottom\"]\n",
    "right_side.set_visible(False)\n",
    "top_side.set_visible(False)\n",
    "bottom_side.set_visible(False)\n",
    "\n",
    "dot1 = Line2D([], [], color='xkcd:orange', marker='.', linestyle='None',\n",
    "                          markersize=5, label='b ≤ 0.9')\n",
    "dot2 = Line2D([], [], color='xkcd:cerulean', marker='.', linestyle='None',\n",
    "                          markersize=5, label='0.9 < b ≤ 1')\n",
    "\n",
    "fig.legend(handles=[dot1,dot2],handletextpad=-0.4,borderpad=0.2,labelspacing=0.1,loc='upper left',bbox_to_anchor=(0.25,1.41),frameon=False)\n",
    "\n",
    "fig.savefig('msfigs/Fig3/slopesNumterms_moreinhistbutnottoomuch_v3_percentvarexp.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the above figures but in style identical to what appears in 3G\n",
    "\n",
    "# DEFINE FIGURE, % terms\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(1.27,1.15*1.2), gridspec_kw={'width_ratios': [2.9,1]},sharey=True)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "for i in np.arange(len(ma3)):\n",
    "    if ma3.loc[i,'fullmodel_slope_-1to1'] <= sthresh:\n",
    "        axes[0].scatter(ma3.loc[i,'fullmodel_slope_-1to1'],ma3.loc[i,'% potential terms'],alpha=0.5,s=3,color='xkcd:orange')\n",
    "    else:\n",
    "        axes[0].scatter(ma3.loc[i,'fullmodel_slope_-1to1'],ma3.loc[i,'% potential terms'],alpha=0.5,s=3,color='xkcd:cerulean')\n",
    "axes[0].axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].set_xlabel('\\n'.join(wrap('full model regression slope, $b$',11)))\n",
    "axes[0].set_ylabel('\\n'.join(wrap('% potential terms sufficient to reach global $b$',30)),labelpad=0.2)\n",
    "axes[0].set_yticks(np.arange(0,0.51,0.1))\n",
    "axes[0].set_yticklabels([0,10,20,30,40,50])\n",
    "axes[0].set_ylim(-0.05,0.55)\n",
    "\n",
    "axes[1].hist(ma3t['% potential terms'],bins=mybins,orientation='horizontal',color='xkcd:orange',alpha=0.7)\n",
    "axes[1].hist(ma3supert['% potential terms'],bins=mybins,orientation='horizontal',color='xkcd:cerulean',zorder=0,alpha=0.7)\n",
    "axes[1].set_xlabel('count')\n",
    "axes[1].axhline(y=ma3t['% potential terms'].mean(),color='xkcd:dark orange',lw=1)\n",
    "axes[1].axhline(y=ma3supert['% potential terms'].mean(),color='xkcd:blue',lw=1)\n",
    "right_side = axes[1].spines[\"right\"]\n",
    "top_side = axes[1].spines[\"top\"]\n",
    "bottom_side = axes[1].spines[\"bottom\"]\n",
    "right_side.set_visible(False)\n",
    "top_side.set_visible(False)\n",
    "bottom_side.set_visible(False)\n",
    "\n",
    "dot1 = Line2D([], [], color='xkcd:orange', marker='.', linestyle='None',\n",
    "                          markersize=5, label='$b$ ≤ 0.9')\n",
    "dot2 = Line2D([], [], color='xkcd:cerulean', marker='.', linestyle='None',\n",
    "                          markersize=5, label='0.9 < $b$ ≤ 1')\n",
    "\n",
    "fig.legend(handles=[dot1,dot2],handletextpad=-0.4,borderpad=0.2,labelspacing=0.1,loc='upper left',bbox_to_anchor=(0.33,1.44),frameon=False)\n",
    "\n",
    "fig.savefig('msfigs/Fig3/slopesNumterms_moreinhistbutnottoomuch_v4_percentterms.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# DEFINE FIGURE, % variance explained\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(1.27,1.15*1.2), gridspec_kw={'width_ratios': [2.9,1]},sharey=True)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "for i in np.arange(len(ma3)):\n",
    "    if ma3.loc[i,'fullmodel_slope_-1to1'] <= sthresh:\n",
    "        axes[0].scatter(ma3.loc[i,'fullmodel_slope_-1to1'],ma3.loc[i,'% potential varexp'],alpha=0.5,s=3,color='xkcd:orange')\n",
    "    else:\n",
    "        axes[0].scatter(ma3.loc[i,'fullmodel_slope_-1to1'],ma3.loc[i,'% potential varexp'],alpha=0.5,s=3,color='xkcd:cerulean')\n",
    "axes[0].axvline(x=0,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].axvline(x=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "axes[0].set_xlabel('\\n'.join(wrap('full model regression slope, $b$',11)))\n",
    "axes[0].set_ylabel('\\n'.join(wrap('% potential epistatic variance sufficient to reach global $b$',30)),labelpad=0.2)\n",
    "axes[0].set_yticks(np.arange(0,1.01,0.2))\n",
    "axes[0].set_yticklabels([0,20,40,60,80,100])\n",
    "axes[0].set_ylim(-0.05,1.05)\n",
    "\n",
    "axes[1].hist(ma3t['% potential varexp'],bins=mybins,orientation='horizontal',color='xkcd:orange',alpha=0.7)\n",
    "axes[1].hist(ma3supert['% potential varexp'],bins=mybins,orientation='horizontal',color='xkcd:cerulean',zorder=0,alpha=0.7)\n",
    "axes[1].set_xlabel('count')\n",
    "axes[1].axhline(y=ma3t['% potential varexp'].mean(),color='xkcd:dark orange',lw=1)\n",
    "axes[1].axhline(y=ma3supert['% potential varexp'].mean(),color='xkcd:blue',lw=1)\n",
    "right_side = axes[1].spines[\"right\"]\n",
    "top_side = axes[1].spines[\"top\"]\n",
    "bottom_side = axes[1].spines[\"bottom\"]\n",
    "right_side.set_visible(False)\n",
    "top_side.set_visible(False)\n",
    "bottom_side.set_visible(False)\n",
    "\n",
    "dot1 = Line2D([], [], color='xkcd:orange', marker='.', linestyle='None',\n",
    "                          markersize=5, label='$b$ ≤ 0.9')\n",
    "dot2 = Line2D([], [], color='xkcd:cerulean', marker='.', linestyle='None',\n",
    "                          markersize=5, label='0.9 < $b$ ≤ 1')\n",
    "\n",
    "fig.legend(handles=[dot1,dot2],handletextpad=-0.4,borderpad=0.2,labelspacing=0.1,loc='upper left',bbox_to_anchor=(0.33,1.44),frameon=False)\n",
    "\n",
    "fig.savefig('msfigs/Fig3/slopesNumterms_moreinhistbutnottoomuch_v4_percentvarexp.pdf',bbox_inches='tight',dpi=1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the proper removal analysis (2021.07.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this analysis with JUST predicted values\n",
    "# Want to do ABC vs aBC plots now where we subtract terms by rank from predicted values\n",
    "\n",
    "#time the code\n",
    "mystart = time.perf_counter()\n",
    "\n",
    "megaremoverv2 = pd.DataFrame()\n",
    "o=10\n",
    "ncyc = 30\n",
    "ncycthresh = 10\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "#diffthresh = 0.01\n",
    "ethresh = 0.50\n",
    "\n",
    "for p in np.arange(0,1):\n",
    "#for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(3,4):\n",
    "    #for e in np.arange(len(envts)):\n",
    "        \n",
    "        print(ploidies[p]+'_'+envts[e])\n",
    "        \n",
    "        # First, import a list of genotypes to predict\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype','s',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "        \n",
    "        # Remove observed data and duplicates\n",
    "        data1 = data1[['genotype','s']].copy(deep=True).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "        \n",
    "        # Create a column for each locus\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "        \n",
    "        # Now import a set of coefficients\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                                             sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_term','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "        \n",
    "        # Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "        for i in np.arange(len(data0)):\n",
    "            if data0.loc[i,'numMut'] % 2 == 1:\n",
    "                data0.at[i,'baseline'] = -1\n",
    "            else:\n",
    "                data0.at[i,'baseline'] = 1\n",
    "        \n",
    "        for l in np.arange(4,5):\n",
    "        #for l in np.arange(len(floci)):\n",
    "        \n",
    "            # Want to start with \"all-epistasis\" observed fitnesses in data1\n",
    "            # Get b_obs, TLS_b_obs, TLS_1 (and intercept values)\n",
    "            tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','without_'+floci[l]]]\n",
    "\n",
    "            templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "            \n",
    "            # York regression to get sum of total least squares deviations, S\n",
    "            myreg = yorkreg_nocorr(templ['s_x'],templ['s_y'],len(templ)*[0],len(templ)*[0],yorkn)\n",
    "            myreg1 = york_slope1(templ['s_x'],templ['s_y'],len(templ)*[0],len(templ)*[0],yorkn)\n",
    "            \n",
    "            plt.scatter(templ['s_x'],templ['s_y'],alpha=0.7,linestyle='None')\n",
    "            xs = plt.gca().get_xlim()\n",
    "            ys = plt.gca().get_ylim()\n",
    "            mainx = np.linspace(-1,1)\n",
    "            plt.plot(mainx,mainx*myreg[0]+myreg[1],color='xkcd:cerulean',zorder=0)\n",
    "            plt.plot(mainx,mainx*1+myreg1[0],color='xkcd:orange',zorder=0)\n",
    "            plt.plot(mainx,mainx,color='k',zorder=0)\n",
    "            plt.xlim(xs)\n",
    "            plt.ylim(ys)\n",
    "            plt.show()\n",
    "            \n",
    "            # Create a subtable - we'll append these together into megaremoverv2\n",
    "            locusremover = pd.DataFrame()\n",
    "            \n",
    "            # wrap up the below, haven't edited yet...\n",
    "            locusremover.at[0,'num term add'] = 0\n",
    "            locusremover.at[0,'term added'] = 'na'\n",
    "            locusremover.at[0,'term order'] = 'na'\n",
    "            locusremover.at[0,'coefficient'] = 'na'\n",
    "            locusremover.at[0,'inferred_b'] = myreg[0]\n",
    "            locusremover.at[0,'inferred_a'] = myreg[1]\n",
    "            locusremover.at[0,'inferred_S'] = myreg[3]\n",
    "            locusremover.at[0,'main_b'] = myreg[0]\n",
    "            locusremover.at[0,'main_a'] = myreg[1]\n",
    "            locusremover.at[0,'main_S'] = myreg[3]\n",
    "            locusremover.at[0,'N'] = len(templ)\n",
    "            locusremover.at[0,'1_a'] = myreg1[0]\n",
    "            locusremover.at[0,'1_S'] = myreg1[1]\n",
    "            \n",
    "            # OPTION TO CONSIDER: ADD IN ∆S FORMULATIONS, DIVERGENCE FROM SLOPE OF 0\n",
    "            \n",
    "            # Begin cycles\n",
    "            # data0adder serves as the sorted databank for (non-zero) coefficients\n",
    "            data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[floci[l]] == 1)]\n",
    "            data0adder['abs coefficient'] = abs(data0adder[ploidies[p]+'_'+envts[e]+'_term'])\n",
    "            data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "            \n",
    "            # create a builder dataframe on which changes will be processed\n",
    "            builder = data0.copy(deep=True)\n",
    "            \n",
    "            for n in np.arange(ncyc):\n",
    "            #for n in np.arange(0,1):\n",
    "                # find the top index (i.e., the strongest coefficient)\n",
    "                topind = data0adder.index[n]\n",
    "                \n",
    "                # remove the strongest epistatic coefficient that involves locus l\n",
    "                builder.at[topind,ploidies[p]+'_'+envts[e]+'_term'] = 0\n",
    "                \n",
    "                # add coefficient info in locusadder\n",
    "                locusremover.at[n+1,'num term add'] = n+1\n",
    "                locusremover.at[n+1,'term added'] = data0adder.loc[topind,'genotype'] \n",
    "                locusremover.at[n+1,'term order'] = data0adder.loc[topind,'numMut']\n",
    "                locusremover.at[n+1,'coefficient'] = data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                \n",
    "                # Estimate the new predicted value for each genotype\n",
    "                for g in np.arange(len(data1)):\n",
    "                    temp = builder.copy(deep=True)\n",
    "                    # to make life faster, remove all zero values\n",
    "                    temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                    for t in np.arange(len(temp)):\n",
    "                        temprowlist = []\n",
    "                        for locus in np.arange(len(floci)):\n",
    "                            temprowlist = temprowlist + [temp.loc[t,floci[locus]]*data1.loc[g,floci[locus]]]\n",
    "                        #remove zeros\n",
    "                        temprowlist = [value for value in temprowlist if value != 0]\n",
    "                        # find product\n",
    "                        firstprod = np.prod(temprowlist)\n",
    "                        # subtract out baseline\n",
    "                        firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                        # multiply by term's value\n",
    "                        mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                        temp.at[t,'tosum'] = mytermval\n",
    "                    totalsum = temp['tosum'].sum()\n",
    "                    myintercept = data1.loc[(data1['genotype'] == '0000000000'),'s'].values[0]\n",
    "                    data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "                    \n",
    "                # For the new predictions, get the values I want\n",
    "                tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab0 = tab0[['genotype','new_pred','without_'+floci[l]]]\n",
    "                tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab1 = tab1[['genotype','new_pred','without_'+floci[l]]]\n",
    "\n",
    "                templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "                for i in np.arange(len(floci)):\n",
    "                    templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "                my_x = templ['new_pred_x']\n",
    "                my_y = templ['new_pred_y']\n",
    "                my_x_err = len(templ)*[0]\n",
    "                my_y_err = len(templ)*[0]\n",
    "\n",
    "                myregnew = yorkreg_nocorr(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                locusremover.at[n+1,'inferred_b'] = myregnew[0]\n",
    "                locusremover.at[n+1,'inferred_a'] = myregnew[1]\n",
    "                locusremover.at[n+1,'inferred_S'] = myregnew[3]\n",
    "                \n",
    "                myreg_main = york_slopeanyb(my_x,my_y,my_x_err,my_y_err,yorkn,myreg[0])\n",
    "                locusremover.at[n+1,'main_b'] = myreg[0]\n",
    "                locusremover.at[n+1,'main_a'] = myreg_main[0]\n",
    "                locusremover.at[n+1,'main_S'] = myreg_main[1]\n",
    "                \n",
    "                myreg_1 = york_slope1(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                locusremover.at[n+1,'1_a'] = myreg_1[0]\n",
    "                locusremover.at[n+1,'1_S'] = myreg_1[1]\n",
    "                \n",
    "                locusremover.at[n+1,'N'] = len(templ)\n",
    "                \n",
    "                plt.scatter(my_x,my_y,alpha=0.7,linestyle='None')\n",
    "                mainx = np.linspace(-1,1)\n",
    "                plt.plot(mainx,mainx*myreg[0]+myreg_main[0],color='xkcd:cerulean',zorder=0)\n",
    "                plt.plot(mainx,mainx*1+myreg_1[0],color='xkcd:orange',zorder=0)\n",
    "                plt.plot(mainx,mainx*myregnew[0]+myregnew[1],color='xkcd:gold',zorder=0)\n",
    "                plt.plot(mainx,mainx,color='k',zorder=0)\n",
    "                plt.xlim(xs)\n",
    "                plt.ylim(ys)\n",
    "                plt.show()\n",
    "                \n",
    "                print(abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum())\n",
    "\n",
    "                if n >= ncycthresh:\n",
    "                    if (abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum()) > ethresh:\n",
    "                        break\n",
    " \n",
    "            locusremover.at[1:,'term added'] = locusremover.loc[1:,'term added'].astype(int).astype(str).str.zfill(10)\n",
    "            locusremover.insert(0,'main locus', floci[l])\n",
    "            locusremover.insert(0,'envt',envts[e])\n",
    "            locusremover.insert(0,'ploidy',ploidies[p])\n",
    "            \n",
    "            megaremoverv2 = megaremoverv2.append(locusremover)\n",
    "            print(floci[l])\n",
    "\n",
    "mystop = time.perf_counter()\n",
    "elapsed = mystop-mystart\n",
    "print(str(elapsed)+' seconds to run')\n",
    "\n",
    "#export_csv = megaremoverv2.to_csv(r'20210801_megaremoverv2_hap_salt_PMA1_pred.csv',index=True,header=True) \n",
    "#export_csv = megaremoverv2.to_csv(r'20210712_megaremoverv2_haphom.csv',index=True,header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to do ABC vs aBC plots now where, instead of adding terms by rank to form predicted values,\n",
    "# we subtract them by rank to adjust observed values.\n",
    "TAKES SOMETHING LIKE 9H\n",
    "#time the code\n",
    "mystart = time.perf_counter()\n",
    "\n",
    "megaremoverv2 = pd.DataFrame()\n",
    "o=10\n",
    "ncyc = 30\n",
    "ncycthresh = 10\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "#diffthresh = 0.01\n",
    "ethresh = 0.50\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):\n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "        \n",
    "        print(ploidies[p]+'_'+envts[e])\n",
    "        \n",
    "        # First, import a list of genotypes to predict\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "        \n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "        \n",
    "        # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "        # represented multiple times.\n",
    "        glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "        data2 = pd.DataFrame()\n",
    "\n",
    "        for g in np.arange(len(glist)):\n",
    "            tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "            if len(tempg) == 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "                data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "            elif len(tempg) > 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "                #my_svar = statistics.variance(tempg['s'])\n",
    "                my_svar = 0\n",
    "                mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "                data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "            data2.at[g,'s_pred'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_Alex prediction']\n",
    "\n",
    "        data1 = data2\n",
    "        \n",
    "        # Calculate the difference between observed and predicted, such that s_obs = s_pred + diff\n",
    "        data1['opdiff'] = data1['s'] - data1['s_pred']\n",
    "\n",
    "        # Create a column for each locus\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "        \n",
    "        # Now import a set of coefficients\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                                             sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_term','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "        \n",
    "        # Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "        for i in np.arange(len(data0)):\n",
    "            if data0.loc[i,'numMut'] % 2 == 1:\n",
    "                data0.at[i,'baseline'] = -1\n",
    "            else:\n",
    "                data0.at[i,'baseline'] = 1\n",
    "        \n",
    "        #for l in np.arange(2,3):\n",
    "        for l in np.arange(len(floci)):\n",
    "        \n",
    "            # Want to start with \"all-epistasis\" observed fitnesses in data1\n",
    "            # Get b_obs, TLS_b_obs, TLS_1 (and intercept values)\n",
    "            tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "\n",
    "            templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "            \n",
    "            # York regression to get sum of total least squares deviations, S\n",
    "            myreg = yorkreg_nocorr(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "            myreg1 = york_slope1(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "            \n",
    "            # Create a subtable - we'll append these together into megaremoverv2\n",
    "            locusremover = pd.DataFrame()\n",
    "            \n",
    "            # wrap up the below, haven't edited yet...\n",
    "            locusremover.at[0,'num term add'] = 0\n",
    "            locusremover.at[0,'term added'] = 'na'\n",
    "            locusremover.at[0,'term order'] = 'na'\n",
    "            locusremover.at[0,'coefficient'] = 'na'\n",
    "            locusremover.at[0,'inferred_b'] = myreg[0]\n",
    "            locusremover.at[0,'inferred_a'] = myreg[1]\n",
    "            locusremover.at[0,'inferred_S'] = myreg[3]\n",
    "            locusremover.at[0,'main_b'] = myreg[0]\n",
    "            locusremover.at[0,'main_a'] = myreg[1]\n",
    "            locusremover.at[0,'main_S'] = myreg[3]\n",
    "            locusremover.at[0,'N'] = len(templ)\n",
    "            locusremover.at[0,'1_a'] = myreg1[0]\n",
    "            locusremover.at[0,'1_S'] = myreg1[1]\n",
    "            \n",
    "            # OPTION TO CONSIDER: ADD IN ∆S FORMULATIONS, DIVERGENCE FROM SLOPE OF 0\n",
    "            \n",
    "            # Begin cycles\n",
    "            # data0adder serves as the sorted databank for (non-zero) coefficients\n",
    "            data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[floci[l]] == 1)]\n",
    "            data0adder['abs coefficient'] = abs(data0adder[ploidies[p]+'_'+envts[e]+'_term'])\n",
    "            data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "            \n",
    "            # create a builder dataframe on which changes will be processed\n",
    "            builder = data0.copy(deep=True)\n",
    "            \n",
    "            for n in np.arange(ncyc):\n",
    "            #for n in np.arange(0,1):\n",
    "                # find the top index (i.e., the strongest coefficient)\n",
    "                topind = data0adder.index[n]\n",
    "                \n",
    "                # remove the strongest epistatic coefficient that involves locus l\n",
    "                builder.at[topind,ploidies[p]+'_'+envts[e]+'_term'] = 0\n",
    "                \n",
    "                # add coefficient info in locusadder\n",
    "                locusremover.at[n+1,'num term add'] = n+1\n",
    "                locusremover.at[n+1,'term added'] = data0adder.loc[topind,'genotype'] \n",
    "                locusremover.at[n+1,'term order'] = data0adder.loc[topind,'numMut']\n",
    "                locusremover.at[n+1,'coefficient'] = data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                \n",
    "                # Estimate the new predicted value for each genotype\n",
    "                for g in np.arange(len(data1)):\n",
    "                    temp = builder.copy(deep=True)\n",
    "                    # to make life faster, remove all zero values\n",
    "                    temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                    for t in np.arange(len(temp)):\n",
    "                        temprowlist = []\n",
    "                        for locus in np.arange(len(floci)):\n",
    "                            temprowlist = temprowlist + [temp.loc[t,floci[locus]]*data1.loc[g,floci[locus]]]\n",
    "                        #remove zeros\n",
    "                        temprowlist = [value for value in temprowlist if value != 0]\n",
    "                        # find product\n",
    "                        firstprod = np.prod(temprowlist)\n",
    "                        # subtract out baseline\n",
    "                        firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                        # multiply by term's value\n",
    "                        mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                        temp.at[t,'tosum'] = mytermval\n",
    "                    totalsum = temp['tosum'].sum()\n",
    "                    myintercept = data1.loc[(data1['genotype'] == '0000000000'),'s_pred'].values[0]\n",
    "                    data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "                data1['new_obs'] = data1['new_pred'] + data1['opdiff']\n",
    "                    \n",
    "                # For the new predictions, get the values I want\n",
    "                tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab0 = tab0[['genotype','new_obs','stderr(s)','without_'+floci[l]]]\n",
    "                tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab1 = tab1[['genotype','new_obs','stderr(s)','without_'+floci[l]]]\n",
    "\n",
    "                templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "                for i in np.arange(len(floci)):\n",
    "                    templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "                my_x = templ['new_obs_x']\n",
    "                my_y = templ['new_obs_y']\n",
    "                my_x_err = templ['stderr(s)_x']\n",
    "                my_y_err = templ['stderr(s)_y']\n",
    "\n",
    "                myregnew = yorkreg_nocorr(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                locusremover.at[n+1,'inferred_b'] = myregnew[0]\n",
    "                locusremover.at[n+1,'inferred_a'] = myregnew[1]\n",
    "                locusremover.at[n+1,'inferred_S'] = myregnew[3]\n",
    "                \n",
    "                myreg_main = york_slopeanyb(my_x,my_y,my_x_err,my_y_err,yorkn,myreg[0])\n",
    "                locusremover.at[n+1,'main_b'] = myreg[0]\n",
    "                locusremover.at[n+1,'main_a'] = myreg_main[0]\n",
    "                locusremover.at[n+1,'main_S'] = myreg_main[1]\n",
    "                \n",
    "                myreg_1 = york_slope1(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                locusremover.at[n+1,'1_a'] = myreg_1[0]\n",
    "                locusremover.at[n+1,'1_S'] = myreg_1[1]\n",
    "                \n",
    "                locusremover.at[n+1,'N'] = len(templ)\n",
    "                \n",
    "                print(abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum())\n",
    "\n",
    "                if n >= ncycthresh:\n",
    "                    if (abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum()) > ethresh:\n",
    "                        break\n",
    " \n",
    "            locusremover.at[1:,'term added'] = locusremover.loc[1:,'term added'].astype(int).astype(str).str.zfill(10)\n",
    "            locusremover.insert(0,'main locus', floci[l])\n",
    "            locusremover.insert(0,'envt',envts[e])\n",
    "            locusremover.insert(0,'ploidy',ploidies[p])\n",
    "            \n",
    "            megaremoverv2 = megaremoverv2.append(locusremover)\n",
    "            print(floci[l])\n",
    "\n",
    "mystop = time.perf_counter()\n",
    "elapsed = mystop-mystart\n",
    "print(str(elapsed)+' seconds to run')\n",
    "                \n",
    "export_csv = megaremoverv2.to_csv(r'20210712_megaremoverv2_haphom.csv',index=True,header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the megaremoverv2\n",
    "mr2 = pd.read_csv('20210712_megaremoverv2_haphom.csv')\n",
    "mr2 = mr2.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'main_b'] <= 1:\n",
    "        mr2.at[i,'main_b_-1to1'] = mr2.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2.at[i,'main_b_-1to1'] = 1/mr2.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'inferred_b'] <= 1:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = mr2.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = 1/mr2.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2['main/inferred'] = mr2['main_S']/mr2['inferred_S']\n",
    "mr2['1/inferred'] = mr2['1_S']/mr2['inferred_S']\n",
    "mr2['1/main'] = mr2['1_S']/mr2['main_S']\n",
    "\n",
    "# For each ploidy-envt-locus, plot ratios\n",
    "# Start by doing just those below the FCT threshold b, can toggle\n",
    "\n",
    "fct_thresh = 0.9\n",
    "\n",
    "for p in np.arange(2):\n",
    "    for e in np.arange(len(envts)):\n",
    "        tempfloci = []\n",
    "        for l in np.arange(len(floci)):\n",
    "            mr2sub = mr2.loc[(mr2['ploidy'] == ploidies[p])&(mr2['envt'] == envts[e])&(mr2['main locus'] == floci[l])].copy(deep=True).reset_index(drop=True)\n",
    "            if mr2sub.loc[0,'main_b_-1to1'] <= fct_thresh:\n",
    "                tempfloci = tempfloci + [floci[l]]\n",
    "                plt.plot(mr2sub['num term add'],mr2sub['1/main'])\n",
    "                #plt.plot(mr2sub['num term add'],mr2sub['main/inferred'])\n",
    "                #plt.plot(mr2sub['num term add'],mr2sub['1/inferred'])\n",
    "                #plt.title(ploidies[p]+'-'+envts[e]+'_'+floci[l])\n",
    "                #plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "                #plt.show()\n",
    "        plt.title(ploidies[p]+'-'+envts[e])\n",
    "        plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "        plt.legend(tempfloci)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the above plots as a subplots figure for the supplement, 2 x 6\n",
    "\n",
    "fig,axes = plt.subplots(nrows=6, ncols=2,sharex=True,sharey=False,figsize=(6,9),constrained_layout=False)\n",
    "\n",
    "rows = envts\n",
    "cols = ['Haploid','Homozygous']\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col, size=8,fontweight='bold')\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, size=8,fontweight='bold')\n",
    "fig.align_ylabels()\n",
    "\n",
    "fig.text(0.4,0.08,'# terms removed (by rank)',size=8,fontweight='bold')\n",
    "\n",
    "fig.text(0, 0.5, 'Relative fit ratio, SSE$_{b=1}$ / SSE$_{b=global}$', va='center', rotation='vertical',size=8, fontweight = 'bold')\n",
    "\n",
    "mr2 = pd.read_csv('20210712_megaremoverv2_haphom.csv')\n",
    "mr2 = mr2.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'main_b'] <= 1:\n",
    "        mr2.at[i,'main_b_-1to1'] = mr2.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2.at[i,'main_b_-1to1'] = 1/mr2.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'inferred_b'] <= 1:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = mr2.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = 1/mr2.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2['main/inferred'] = mr2['main_S']/mr2['inferred_S']\n",
    "mr2['1/inferred'] = mr2['1_S']/mr2['inferred_S']\n",
    "mr2['1/main'] = mr2['1_S']/mr2['main_S']\n",
    "\n",
    "# For each ploidy-envt-locus, plot ratios\n",
    "# Start by doing just those below the FCT threshold b, can toggle\n",
    "\n",
    "fct_thresh = 1\n",
    "\n",
    "for p in np.arange(2):\n",
    "    for e in np.arange(len(envts)):\n",
    "        tempfloci = []\n",
    "        for l in np.arange(len(floci)):\n",
    "            mr2sub = mr2.loc[(mr2['ploidy'] == ploidies[p])&(mr2['envt'] == envts[e])&(mr2['main locus'] == floci[l])&(mr2['num term add'] <= 10)].copy(deep=True).reset_index(drop=True)\n",
    "            if mr2sub.loc[0,'main_b_-1to1'] <= fct_thresh:\n",
    "                tempfloci = tempfloci + [floci[l]]\n",
    "                axes[e][p].plot(mr2sub['num term add'],mr2sub['1/main'],alpha=0.5)\n",
    "        axes[e][p].set_xlim(-0.5,10.5)\n",
    "        axes[e][p].axhline(y=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "leg = axes[0][1].legend(tempfloci,title='Locus',loc='upper left',ncol=5,bbox_to_anchor=(-0.95,1.8))\n",
    "#leg._legend_box.align = \"left\"\n",
    "fig.savefig('msfigs/Fig3/removalbype_v01.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same supplementary table as above, except extend to removing all terms (like in fig 3E)\n",
    "\n",
    "fig,axes = plt.subplots(nrows=6, ncols=2,sharex=True,sharey=False,figsize=(6,9),constrained_layout=False)\n",
    "\n",
    "rows = envts\n",
    "cols = ['Haploid','Homozygous']\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col, size=8,fontweight='bold')\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, size=8,fontweight='bold')\n",
    "fig.align_ylabels()\n",
    "\n",
    "fig.text(0.4,0.08,'# terms removed (by rank)',size=8,fontweight='bold')\n",
    "\n",
    "fig.text(0, 0.5, 'Relative fit ratio, SSE$_{b=1}$ / SSE$_{b=global}$', va='center', rotation='vertical',size=8, fontweight = 'bold')\n",
    "\n",
    "mr2 = pd.read_csv('20210712_megaremoverv2_haphom.csv')\n",
    "mr2 = mr2.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'main_b'] <= 1:\n",
    "        mr2.at[i,'main_b_-1to1'] = mr2.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2.at[i,'main_b_-1to1'] = 1/mr2.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'inferred_b'] <= 1:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = mr2.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = 1/mr2.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2['main/inferred'] = mr2['main_S']/mr2['inferred_S']\n",
    "mr2['1/inferred'] = mr2['1_S']/mr2['inferred_S']\n",
    "mr2['1/main'] = mr2['1_S']/mr2['main_S']\n",
    "\n",
    "# For each ploidy-envt-locus, plot ratios\n",
    "# Start by doing just those below the FCT threshold b, can toggle\n",
    "\n",
    "fct_thresh = 1\n",
    "\n",
    "for p in np.arange(2):\n",
    "    for e in np.arange(len(envts)):\n",
    "        tempfloci = []\n",
    "        for l in np.arange(len(floci)):\n",
    "            mr2sub = mr2.loc[(mr2['ploidy'] == ploidies[p])&(mr2['envt'] == envts[e])&(mr2['main locus'] == floci[l])&(mr2['num term add'] <= 10)].copy(deep=True).reset_index(drop=True)\n",
    "            if mr2sub.loc[0,'main_b_-1to1'] <= fct_thresh:\n",
    "                tempfloci = tempfloci + [floci[l]]\n",
    "                axes[e][p].plot(mr2sub['num term add'],mr2sub['1/main'],alpha=0.5)\n",
    "        axes[e][p].set_xlim(-0.5,10.5)\n",
    "        axes[e][p].axhline(y=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "leg = axes[0][1].legend(tempfloci,title='Locus',loc='upper left',ncol=5,bbox_to_anchor=(-0.95,1.8))\n",
    "#leg._legend_box.align = \"left\"\n",
    "fig.savefig('msfigs/Fig3/removalbype_v01.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same supplementary table as above, except extend to removing all terms (like in fig 3E)\n",
    "\n",
    "# Import the \"all removed\" and normal megaremoverv2 tables\n",
    "mr2 = pd.read_csv('20210712_megaremoverv2_haphom.csv')\n",
    "mr2 = mr2.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "mr2all = pd.read_csv('20210712_megaremoverv2_haphom_ALLTERMSREMOVED.csv')\n",
    "mr2all = mr2all.drop(columns=['Unnamed: 0'])\n",
    "mr2all = mr2all.loc[(mr2all['coefficient'] == 'all')].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'main_b'] <= 1:\n",
    "        mr2.at[i,'main_b_-1to1'] = mr2.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2.at[i,'main_b_-1to1'] = 1/mr2.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'inferred_b'] <= 1:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = mr2.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = 1/mr2.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2['main/inferred'] = mr2['main_S']/mr2['inferred_S']\n",
    "mr2['1/inferred'] = mr2['1_S']/mr2['inferred_S']\n",
    "mr2['1/main'] = mr2['1_S']/mr2['main_S']\n",
    "\n",
    "# Do the same for the mr2all table\n",
    "for i in np.arange(len(mr2all)):\n",
    "    if mr2all.loc[i,'main_b'] <= 1:\n",
    "        mr2all.at[i,'main_b_-1to1'] = mr2all.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2all.at[i,'main_b_-1to1'] = 1/mr2all.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2all)):\n",
    "    if mr2all.loc[i,'inferred_b'] <= 1:\n",
    "        mr2all.at[i,'inferred_b_-1to1'] = mr2all.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2all.at[i,'inferred_b_-1to1'] = 1/mr2all.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2all['main/inferred'] = mr2all['main_S']/mr2all['inferred_S']\n",
    "mr2all['1/inferred'] = mr2all['1_S']/mr2all['inferred_S']\n",
    "mr2all['1/main'] = mr2all['1_S']/mr2all['main_S']\n",
    "\n",
    "fig,axes = plt.subplots(nrows=6, ncols=2,sharex=True,sharey=False,figsize=(6,9),constrained_layout=False)\n",
    "\n",
    "rows = envts\n",
    "cols = ['Haploid','Homozygous']\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col, size=8,fontweight='bold')\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, size=8,fontweight='bold')\n",
    "fig.align_ylabels()\n",
    "\n",
    "fig.text(0.4,0.08,'# terms removed (by rank)',size=8,fontweight='bold')\n",
    "\n",
    "fig.text(0, 0.5, 'Relative fit ratio, SSE$_{b=1}$ / SSE$_{b=global}$', va='center', rotation='vertical',size=8, fontweight = 'bold')\n",
    "\n",
    "# For each ploidy-envt-locus, plot ratios\n",
    "# Start by doing just those below the FCT threshold b, can toggle\n",
    "\n",
    "fct_thresh = 1\n",
    "\n",
    "for p in np.arange(2):\n",
    "    for e in np.arange(len(envts)):\n",
    "        tempfloci = []\n",
    "        for l in np.arange(len(floci)):\n",
    "            mr2sub = mr2.loc[(mr2['ploidy'] == ploidies[p])&(mr2['envt'] == envts[e])&(mr2['main locus'] == floci[l])&(mr2['num term add'] <= 10)].copy(deep=True).reset_index(drop=True)\n",
    "            if mr2sub.loc[0,'main_b_-1to1'] <= fct_thresh:\n",
    "                tempfloci = tempfloci + [floci[l]]\n",
    "                termlist = list(mr2sub['num term add'])\n",
    "                termlist = termlist + [float(11)]\n",
    "                ratlist = list(mr2sub['1/main'])\n",
    "                tempall = mr2all.loc[(mr2all['ploidy'] == ploidies[p])&(mr2all['envt'] == envts[e])&(mr2all['main locus'] == floci[l])].copy(deep=True).reset_index(drop=True)\n",
    "                ratlist = ratlist + [tempall.loc[0,'1/main']]\n",
    "                \n",
    "                axes[e][p].plot(termlist,ratlist,alpha=0.5)\n",
    "        axes[e][p].set_xlim(-0.5,11.5)\n",
    "        axes[e][p].axhline(y=1,color='xkcd:grey',zorder=0,lw=0.5)\n",
    "        axes[e][p].set_xticks([0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "        axes[e][p].set_xticklabels([0,1,2,3,4,5,6,7,8,9,10,'all'])\n",
    "leg = axes[0][1].legend(tempfloci,title='Locus',loc='upper left',ncol=5,bbox_to_anchor=(-0.95,1.8))\n",
    "\n",
    "#leg._legend_box.align = \"left\"\n",
    "fig.savefig('msfigs/Fig3/removalbype_v02.pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the final 1/main to the all-sense heritability\n",
    "# Put it all in a table\n",
    "\n",
    "# first get the relevant 1/main SSE ratios\n",
    "detab = pd.DataFrame()\n",
    "myind = 0\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for l in np.arange(len(floci)):\n",
    "            detab.at[myind,'ploidy'] = ploidies[p]\n",
    "            detab.at[myind,'envt'] = envts[e]\n",
    "            detab.at[myind,'main locus'] = floci[l]\n",
    "            \n",
    "            temp = mr2.loc[(mr2['ploidy'] == ploidies[p])&(mr2['envt'] == envts[e])&(mr2['main locus'] == floci[l])].reset_index(drop=True)\n",
    "            \n",
    "            detab.at[myind,'inferred_b_-1to1_original'] = temp.loc[0,'inferred_b_-1to1']\n",
    "            \n",
    "            mymaxind = temp.iloc[-1]['num term add']\n",
    "            \n",
    "            detab.at[myind,'1/main_final'] = temp.loc[int(mymaxind),'1/main']\n",
    "            detab.at[myind,'num term add'] = mymaxind\n",
    "            \n",
    "            myind = myind + 1\n",
    "\n",
    "detab['ploidy-envt'] = detab['ploidy']+'-'+detab['envt']\n",
    "\n",
    "# now figure out how to merge it with the biological replicate data\n",
    "workingbr = bioreplr.copy(deep=True)\n",
    "workingbr['ploidy-envt'] = workingbr['ploidy']+'-'+workingbr['envt']\n",
    "\n",
    "detab = pd.merge(detab,workingbr,on='ploidy-envt',how='left')\n",
    "\n",
    "# plot the results, locus by locus\n",
    "\n",
    "for l in np.arange(len(floci)):\n",
    "    temp = detab.loc[detab['main locus'] == floci[l]]\n",
    "    plt.scatter(temp['r^2'],temp['1/main_final'])\n",
    "    myreg = linregress(temp['r^2'],temp['1/main_final'])\n",
    "    plt.plot(temp['r^2'],temp['r^2']*myreg.slope+myreg.intercept,alpha=0.7,lw=0.5)\n",
    "    print(floci[l]+': r^2 is '+str(round(myreg.rvalue**2,3)))\n",
    "plt.legend(floci)\n",
    "plt.xlabel('r^2, biological replicates (for ploidy-envt)')\n",
    "plt.ylabel('SSE b=1 / SSE b=global (\"final\")')\n",
    "plt.show()\n",
    "\n",
    "# now do it for only those slopes which we consider to be FCTs (threshold of 0.9)\n",
    "# also limit it to ones where we have 3 points or more\n",
    "plottedfloci = []\n",
    "tempfct = detab.loc[detab['inferred_b_-1to1_original'] <= 0.9]\n",
    "for l in np.arange(len(floci)):\n",
    "    temp = tempfct.loc[tempfct['main locus'] == floci[l]]\n",
    "    if len(temp) > 2:\n",
    "        plt.scatter(temp['r^2'],temp['1/main_final'])\n",
    "        myreg = linregress(temp['r^2'],temp['1/main_final'])\n",
    "        plt.plot(temp['r^2'],temp['r^2']*myreg.slope+myreg.intercept,alpha=0.7,lw=0.5)\n",
    "        print(floci[l]+': r^2 is '+str(round(myreg.rvalue**2,3)))\n",
    "        plottedfloci = plottedfloci + [floci[l]]\n",
    "plt.legend(plottedfloci)\n",
    "plt.xlabel('r^2, biological replicates (for ploidy-envt)')\n",
    "plt.ylabel('SSE b=1 / SSE b=global (\"final\")')\n",
    "plt.show()\n",
    "\n",
    "# same as above but without hom-ypda\n",
    "plottedfloci = []\n",
    "tempfctr = tempfct.loc[tempfct['ploidy-envt'] != 'hom-YPDA']\n",
    "for l in np.arange(len(floci)):\n",
    "    temp = tempfctr.loc[tempfctr['main locus'] == floci[l]]\n",
    "    if len(temp) > 2:\n",
    "        plt.scatter(temp['r^2'],temp['1/main_final'])\n",
    "        myreg = linregress(temp['r^2'],temp['1/main_final'])\n",
    "        plt.plot(temp['r^2'],temp['r^2']*myreg.slope+myreg.intercept,alpha=0.7,lw=0.5)\n",
    "        print(floci[l]+': r^2 is '+str(round(myreg.rvalue**2,3)))\n",
    "        plottedfloci = plottedfloci + [floci[l]]\n",
    "plt.legend(plottedfloci)\n",
    "plt.xlabel('r^2, biological replicates (for ploidy-envt)')\n",
    "plt.ylabel('SSE b=1 / SSE b=global (\"final\")')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just do the FCTs (> 2 points) loci\n",
    "\n",
    "# Want to update the above so that it has truly the \"final\" SSE ratio\n",
    "# Right now, it's just final after removing at most 30 terms, which won't be enough in many cases to get to the residual\n",
    "# Starting point is mr2all\n",
    "\n",
    "mr2all = pd.read_csv('20210712_megaremoverv2_haphom_ALLTERMSREMOVED.csv')\n",
    "mr2all = mr2all.drop(columns=['Unnamed: 0'])\n",
    "#mr2all = mr2all.loc[(mr2all['coefficient'] == 'all')].reset_index(drop=True)\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "\n",
    "# Do the same for the mr2all table\n",
    "for i in np.arange(len(mr2all)):\n",
    "    if mr2all.loc[i,'main_b'] <= 1:\n",
    "        mr2all.at[i,'main_b_-1to1'] = mr2all.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2all.at[i,'main_b_-1to1'] = 1/mr2all.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2all)):\n",
    "    if mr2all.loc[i,'inferred_b'] <= 1:\n",
    "        mr2all.at[i,'inferred_b_-1to1'] = mr2all.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2all.at[i,'inferred_b_-1to1'] = 1/mr2all.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2all['main/inferred'] = mr2all['main_S']/mr2all['inferred_S']\n",
    "mr2all['1/inferred'] = mr2all['1_S']/mr2all['inferred_S']\n",
    "mr2all['1/main'] = mr2all['1_S']/mr2all['main_S']\n",
    "\n",
    "\n",
    "detab = pd.DataFrame()\n",
    "myind = 0\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for l in np.arange(len(floci)):\n",
    "            detab.at[myind,'ploidy'] = ploidies[p]\n",
    "            detab.at[myind,'envt'] = envts[e]\n",
    "            detab.at[myind,'main locus'] = floci[l]\n",
    "            \n",
    "            temp = mr2all.loc[(mr2all['ploidy'] == ploidies[p])&(mr2all['envt'] == envts[e])&(mr2all['main locus'] == floci[l])].reset_index(drop=True)\n",
    "            \n",
    "            detab.at[myind,'inferred_b_-1to1_original'] = temp.loc[0,'inferred_b_-1to1']\n",
    "            \n",
    "            #mymaxind = temp.iloc[-1]['num term add']\n",
    "            \n",
    "            detab.at[myind,'1/main_final'] = temp.loc[1,'1/main']\n",
    "            #detab.at[myind,'num term add'] = 'all'\n",
    "            \n",
    "            myind = myind + 1\n",
    "\n",
    "detab['ploidy-envt'] = detab['ploidy']+'-'+detab['envt']\n",
    "\n",
    "# now figure out how to merge it with the biological replicate data\n",
    "workingbr = bioreplr.copy(deep=True)\n",
    "workingbr['ploidy-envt'] = workingbr['ploidy']+'-'+workingbr['envt']\n",
    "\n",
    "detab = pd.merge(detab,workingbr,on='ploidy-envt',how='left')\n",
    "\n",
    "# plot the results, locus by locus\n",
    "# now do it for only those slopes which we consider to be FCTs (threshold of 0.9)\n",
    "# also limit it to ones where we have 3 points or more\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5.5,3))\n",
    "\n",
    "plottedfloci = []\n",
    "fct_thresh = 0.9\n",
    "tempfct = detab.loc[detab['inferred_b_-1to1_original'] <= fct_thresh]\n",
    "for l in np.arange(len(floci)):\n",
    "    temp = tempfct.loc[tempfct['main locus'] == floci[l]]\n",
    "    if len(temp) > 2:\n",
    "        ax.scatter(temp['r^2'],temp['1/main_final'],alpha=0.7,s=15)\n",
    "        #myreg = linregress(temp['r^2'],temp['1/main_final'])\n",
    "        #ax.plot(temp['r^2'],temp['r^2']*myreg.slope+myreg.intercept,alpha=0.7,lw=0.8)\n",
    "        ##print(floci[l]+': r^2 is '+str(round(myreg.rvalue**2,3)))\n",
    "        plottedfloci = plottedfloci + [floci[l]]\n",
    "#ax.legend(plottedfloci)\n",
    "leg = ax.legend(plottedfloci,ncol=3,\n",
    "                handlelength = 1,labelspacing=0.4,columnspacing=1,handletextpad=0.4,frameon=True,\n",
    "                loc='upper right',bbox_to_anchor=(0.32,0.22),borderpad=0.5,fontsize=6)\n",
    "leg._legend_box.align = \"left\"\n",
    "leg.set_title('Locus')\n",
    "#leg.get_frame().set_linewidth(0.5)\n",
    "ax.set_xlabel('Bio rep R$^2$')\n",
    "ax.set_ylabel('\\n'.join(wrap('Final SSE$_{b=1}$ / SSE$_{b=global}$, all epistasis removed',40)),labelpad=2)\n",
    "ax.axhline(y=1,zorder=0,lw=0.5,color='xkcd:grey')\n",
    "\n",
    "plt.savefig(\"msfigs/SIfigs/residual-ratio_r2_justfcts_v2.pdf\",bbox_inches='tight',dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the megaremoverv2 on JUST the residuals by setting all values in builder to 0\n",
    "\n",
    "#time the code\n",
    "mystart = time.perf_counter()\n",
    "\n",
    "megaremoverv2allremoved = pd.DataFrame()\n",
    "o=10\n",
    "ncyc = 30\n",
    "ncycthresh = 10\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "#diffthresh = 0.01\n",
    "ethresh = 0.50\n",
    "\n",
    "#for p in np.arange(0,1):\n",
    "for p in np.arange(len(ploidies)):\n",
    "    #for e in np.arange(0,1):\n",
    "    for e in np.arange(len(envts)):\n",
    "        \n",
    "        print(ploidies[p]+'_'+envts[e])\n",
    "        \n",
    "        # First, import a list of genotypes to predict\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "        \n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "        \n",
    "        # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "        # represented multiple times.\n",
    "        glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "        data2 = pd.DataFrame()\n",
    "\n",
    "        for g in np.arange(len(glist)):\n",
    "            tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "            if len(tempg) == 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "                data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "            elif len(tempg) > 1:\n",
    "                data2.at[g,'genotype'] = glist[g]\n",
    "                data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "                #my_svar = statistics.variance(tempg['s'])\n",
    "                my_svar = 0\n",
    "                mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "                data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "            data2.at[g,'s_pred'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_Alex prediction']\n",
    "\n",
    "        data1 = data2\n",
    "        \n",
    "        # Calculate the difference between observed and predicted, such that s_obs = s_pred + diff\n",
    "        data1['opdiff'] = data1['s'] - data1['s_pred']\n",
    "\n",
    "        # Create a column for each locus\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "        \n",
    "        # Now import a set of coefficients\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                                             sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_term','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "        \n",
    "        # Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "        for i in np.arange(len(data0)):\n",
    "            if data0.loc[i,'numMut'] % 2 == 1:\n",
    "                data0.at[i,'baseline'] = -1\n",
    "            else:\n",
    "                data0.at[i,'baseline'] = 1\n",
    "        \n",
    "        #for l in np.arange(2,3):\n",
    "        for l in np.arange(len(floci)):\n",
    "        \n",
    "            # Want to start with \"all-epistasis\" observed fitnesses in data1\n",
    "            # Get b_obs, TLS_b_obs, TLS_1 (and intercept values)\n",
    "            tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "\n",
    "            templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "            \n",
    "            # York regression to get sum of total least squares deviations, S\n",
    "            myreg = yorkreg_nocorr(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "            myreg1 = york_slope1(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "            \n",
    "            # Create a subtable - we'll append these together into megaremoverv2allremoved\n",
    "            locusremover = pd.DataFrame()\n",
    "            \n",
    "            # wrap up the below, haven't edited yet...\n",
    "            locusremover.at[0,'num term add'] = 0\n",
    "            locusremover.at[0,'term added'] = 'na'\n",
    "            locusremover.at[0,'term order'] = 'na'\n",
    "            locusremover.at[0,'coefficient'] = 'na'\n",
    "            locusremover.at[0,'inferred_b'] = myreg[0]\n",
    "            locusremover.at[0,'inferred_a'] = myreg[1]\n",
    "            locusremover.at[0,'inferred_S'] = myreg[3]\n",
    "            locusremover.at[0,'main_b'] = myreg[0]\n",
    "            locusremover.at[0,'main_a'] = myreg[1]\n",
    "            locusremover.at[0,'main_S'] = myreg[3]\n",
    "            locusremover.at[0,'N'] = len(templ)\n",
    "            locusremover.at[0,'1_a'] = myreg1[0]\n",
    "            locusremover.at[0,'1_S'] = myreg1[1]\n",
    "            \n",
    "            #plot it\n",
    "            plt.errorbar(templ['s_x'],templ['s_y'],xerr=templ['stderr(s)_x'],yerr=templ['stderr(s)_y'],alpha=0.7,linestyle='None')\n",
    "            mainx = np.linspace(-1,1)\n",
    "            xs = plt.gca().get_xlim()\n",
    "            ys = plt.gca().get_ylim()\n",
    "            plt.plot(mainx,mainx*myreg[0]+myreg[1],color='xkcd:cerulean',zorder=0)\n",
    "            plt.plot(mainx,mainx*1+myreg1[0],color='xkcd:orange',zorder=0)\n",
    "            plt.plot(mainx,mainx,color='k',zorder=0)\n",
    "            plt.xlim(xs)\n",
    "            plt.ylim(ys)\n",
    "            plt.title(ploidies[p]+'-'+envts[e]+'_'+floci[l]+': none removed')\n",
    "            plt.show()\n",
    "                        \n",
    "            # OPTION TO CONSIDER: ADD IN ∆S FORMULATIONS, DIVERGENCE FROM SLOPE OF 0\n",
    "            \n",
    "            # Begin cycles\n",
    "            # data0adder serves as the sorted databank for (non-zero) coefficients\n",
    "            data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[floci[l]] == 1)]\n",
    "            data0adder['abs coefficient'] = abs(data0adder[ploidies[p]+'_'+envts[e]+'_term'])\n",
    "            data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "            \n",
    "            # create a builder dataframe on which changes will be processed\n",
    "            builder = data0.copy(deep=True)\n",
    "\n",
    "            # find the top index (i.e., the strongest coefficient)\n",
    "            allind = data0adder.index\n",
    "\n",
    "            # remove all epistatic coefficients that involve locus l\n",
    "            builder.at[allind,ploidies[p]+'_'+envts[e]+'_term'] = 0\n",
    "\n",
    "            # add coefficient info in locusadder\n",
    "            locusremover.at[1,'num term add'] = 'all'\n",
    "            locusremover.at[1,'term added'] = 'all' \n",
    "            locusremover.at[1,'term order'] = 'all'\n",
    "            locusremover.at[1,'coefficient'] = 'all'\n",
    "\n",
    "            # Estimate the new predicted value for each genotype\n",
    "            for g in np.arange(len(data1)):\n",
    "                temp = builder.copy(deep=True)\n",
    "                # to make life faster, remove all zero values\n",
    "                temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                for t in np.arange(len(temp)):\n",
    "                    temprowlist = []\n",
    "                    for locus in np.arange(len(floci)):\n",
    "                        temprowlist = temprowlist + [temp.loc[t,floci[locus]]*data1.loc[g,floci[locus]]]\n",
    "                    #remove zeros\n",
    "                    temprowlist = [value for value in temprowlist if value != 0]\n",
    "                    # find product\n",
    "                    firstprod = np.prod(temprowlist)\n",
    "                    # subtract out baseline\n",
    "                    firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                    # multiply by term's value\n",
    "                    mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                    temp.at[t,'tosum'] = mytermval\n",
    "                totalsum = temp['tosum'].sum()\n",
    "                myintercept = data1.loc[(data1['genotype'] == '0000000000'),'s_pred'].values[0]\n",
    "                data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "            data1['new_obs'] = data1['new_pred'] + data1['opdiff']\n",
    "\n",
    "            # For the new predictions, get the values I want\n",
    "            tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','new_obs','stderr(s)','without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','new_obs','stderr(s)','without_'+floci[l]]]\n",
    "\n",
    "            templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "            my_x = templ['new_obs_x']\n",
    "            my_y = templ['new_obs_y']\n",
    "            my_x_err = templ['stderr(s)_x']\n",
    "            my_y_err = templ['stderr(s)_y']\n",
    "\n",
    "            myregnew = yorkreg_nocorr(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "            locusremover.at[1,'inferred_b'] = myregnew[0]\n",
    "            locusremover.at[1,'inferred_a'] = myregnew[1]\n",
    "            locusremover.at[1,'inferred_S'] = myregnew[3]\n",
    "\n",
    "            myreg_main = york_slopeanyb(my_x,my_y,my_x_err,my_y_err,yorkn,myreg[0])\n",
    "            locusremover.at[1,'main_b'] = myreg[0]\n",
    "            locusremover.at[1,'main_a'] = myreg_main[0]\n",
    "            locusremover.at[1,'main_S'] = myreg_main[1]\n",
    "\n",
    "            myreg_1 = york_slope1(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "            locusremover.at[1,'1_a'] = myreg_1[0]\n",
    "            locusremover.at[1,'1_S'] = myreg_1[1]\n",
    "\n",
    "            locusremover.at[1,'N'] = len(templ)\n",
    "\n",
    "            #print(abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum())\n",
    "            \n",
    "            #plot it\n",
    "            plt.errorbar(my_x,my_y,xerr=my_x_err,yerr=my_y_err,alpha=0.7,linestyle='None')\n",
    "            mainx = np.linspace(-1,1)\n",
    "            xs = plt.gca().get_xlim()\n",
    "            ys = plt.gca().get_ylim()\n",
    "            plt.plot(mainx,mainx*myreg[0]+myreg_main[0],color='xkcd:cerulean',zorder=0)\n",
    "            plt.plot(mainx,mainx*1+myreg_1[0],color='xkcd:orange',zorder=0)\n",
    "            plt.plot(mainx,mainx*myregnew[0]+myregnew[1],color='xkcd:gold',zorder=0)\n",
    "            plt.plot(mainx,mainx,color='k',zorder=0)\n",
    "            plt.xlim(xs)\n",
    "            plt.ylim(ys)\n",
    "            plt.title(ploidies[p]+'-'+envts[e]+'_'+floci[l]+': all removed')\n",
    "            plt.show()\n",
    "\n",
    "            locusremover.insert(0,'main locus', floci[l])\n",
    "            locusremover.insert(0,'envt',envts[e])\n",
    "            locusremover.insert(0,'ploidy',ploidies[p])\n",
    "\n",
    "            megaremoverv2allremoved = megaremoverv2allremoved.append(locusremover)\n",
    "            print(floci[l])\n",
    "\n",
    "mystop = time.perf_counter()\n",
    "elapsed = mystop-mystart\n",
    "print(str(elapsed)+' seconds to run')\n",
    "                \n",
    "export_csv = megaremoverv2allremoved.to_csv(r'20210712_megaremoverv2_haphom_ALLTERMSREMOVED.csv',index=True,header=True)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal for now is to get a table, then can figure out a bit how to present best\n",
    "\n",
    "# Import the \"all removed\" and normal megaremoverv2 tables\n",
    "mr2 = pd.read_csv('20210712_megaremoverv2_haphom.csv')\n",
    "mr2 = mr2.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "mr2all = pd.read_csv('20210712_megaremoverv2_haphom_ALLTERMSREMOVED.csv')\n",
    "mr2all = mr2all.drop(columns=['Unnamed: 0'])\n",
    "mr2all = mr2all.loc[(mr2all['coefficient'] == 'all')].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'main_b'] <= 1:\n",
    "        mr2.at[i,'main_b_-1to1'] = mr2.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2.at[i,'main_b_-1to1'] = 1/mr2.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'inferred_b'] <= 1:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = mr2.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = 1/mr2.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2['main/inferred'] = mr2['main_S']/mr2['inferred_S']\n",
    "mr2['1/inferred'] = mr2['1_S']/mr2['inferred_S']\n",
    "mr2['1/main'] = mr2['1_S']/mr2['main_S']\n",
    "\n",
    "# Do the same for the mr2all table\n",
    "for i in np.arange(len(mr2all)):\n",
    "    if mr2all.loc[i,'main_b'] <= 1:\n",
    "        mr2all.at[i,'main_b_-1to1'] = mr2all.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2all.at[i,'main_b_-1to1'] = 1/mr2all.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2all)):\n",
    "    if mr2all.loc[i,'inferred_b'] <= 1:\n",
    "        mr2all.at[i,'inferred_b_-1to1'] = mr2all.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2all.at[i,'inferred_b_-1to1'] = 1/mr2all.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2all['main/inferred'] = mr2all['main_S']/mr2all['inferred_S']\n",
    "mr2all['1/inferred'] = mr2all['1_S']/mr2all['inferred_S']\n",
    "mr2all['1/main'] = mr2all['1_S']/mr2all['main_S']\n",
    "\n",
    "# count stuff, getting a proportion\n",
    "mr2rep = pd.DataFrame()\n",
    "\n",
    "fct_thresh = 0.9\n",
    "\n",
    "for n in np.arange(1,11):\n",
    "    mr2rep.at[0,str(n)+'_denom'] = len(mr2.loc[(mr2['num term add'] == n)])\n",
    "    mr2rep.at[0,str(n)+'_num1better'] = len(mr2.loc[(mr2['num term add'] == n)&(mr2['1/main'] < 1)])\n",
    "    mr2rep.at[1,str(n)+'_denom'] = len(mr2.loc[(mr2['num term add'] == n)&(mr2['main_b_-1to1'] <= fct_thresh)])\n",
    "    mr2rep.at[1,str(n)+'_num1better'] = len(mr2.loc[(mr2['num term add'] == n)&(mr2['1/main'] < 1)&(mr2['main_b_-1to1'] <= fct_thresh)])\n",
    "    mr2rep.at[2,str(n)+'_denom'] = len(mr2.loc[(mr2['num term add'] == n)&(mr2['main_b_-1to1'] > fct_thresh)])\n",
    "    mr2rep.at[2,str(n)+'_num1better'] = len(mr2.loc[(mr2['num term add'] == n)&(mr2['1/main'] < 1)&(mr2['main_b_-1to1'] > fct_thresh)])\n",
    "    mr2rep[str(n)+'_frac'] = mr2rep[str(n)+'_num1better'] / mr2rep[str(n)+'_denom']\n",
    "\n",
    "mr2rep.at[0,'allremoved_denom'] = len(mr2all)\n",
    "mr2rep.at[0,'allremoved_num1better'] = len(mr2all.loc[(mr2all['1/main'] < 1)])\n",
    "mr2rep.at[1,'allremoved_denom'] = len(mr2all.loc[(mr2all['main_b_-1to1'] <= fct_thresh)])\n",
    "mr2rep.at[1,'allremoved_num1better'] = len(mr2all.loc[(mr2all['1/main'] < 1)&(mr2all['main_b_-1to1'] <= fct_thresh)])\n",
    "mr2rep.at[2,'allremoved_denom'] = len(mr2all.loc[(mr2all['main_b_-1to1'] > fct_thresh)])\n",
    "mr2rep.at[2,'allremoved_num1better'] = len(mr2all.loc[(mr2['1/main'] < 1)&(mr2all['main_b_-1to1'] > fct_thresh)])\n",
    "mr2rep['allremoved_frac'] = mr2rep['allremoved_num1better'] / mr2rep['allremoved_denom']\n",
    "\n",
    "mr2rep.insert(0,'category',['all','<='+str(fct_thresh),'>'+str(fct_thresh)])\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresh_list = [1.0,0.9,0.8]\n",
    "color_list = ['xkcd:cerulean','xkcd:orange','xkcd:wine red']\n",
    "\n",
    "offsets = [-0.1,0,0.1]\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1, ncols=1, figsize=(4*0.65,2*0.58))\n",
    "\n",
    "for t in np.arange(len(thresh_list)):\n",
    "    #boxvals = []\n",
    "    medlist = []\n",
    "    uplist = []\n",
    "    downlist = []\n",
    "    for n in np.arange(11):\n",
    "        temp = mr2.loc[(mr2['num term add'] == n)&(mr2['main_b_-1to1'] <= thresh_list[t])].copy(deep=True)\n",
    "        my_median = np.median(temp['1/main'])\n",
    "        medlist = medlist + [my_median]\n",
    "        my_25 = np.quantile(temp['1/main'],0.25)\n",
    "        lower_error = my_median - my_25 \n",
    "        downlist = downlist + [lower_error]\n",
    "        my_75 = np.quantile(temp['1/main'],0.75)\n",
    "        upper_error = my_75 - my_median\n",
    "        uplist = uplist + [upper_error]\n",
    "        #boxvals = boxvals + [temp['1/main']]\n",
    "        #plt.boxplot(temp['1/main'])\n",
    "    #plt.boxplot(boxvals)\n",
    "    temp = mr2all.loc[mr2all['main_b_-1to1'] <= thresh_list[t]].copy(deep=True)\n",
    "    medlist = medlist + [np.median(temp['1/main'])]\n",
    "    my_25 = np.quantile(temp['1/main'],0.25)\n",
    "    lower_error = my_median - my_25 \n",
    "    downlist = downlist + [lower_error]\n",
    "    my_75 = np.quantile(temp['1/main'],0.75)\n",
    "    upper_error = my_75 - my_median\n",
    "    uplist = uplist + [upper_error]\n",
    "    \n",
    "    my_error = [downlist,uplist]\n",
    "    #plt.errorbar(np.arange(11),medlist,yerr=my_error,fmt='o')\n",
    "    markers,caps,bars = ax.errorbar(np.arange(12)+offsets[t],medlist,yerr=my_error,elinewidth=0.8,alpha=0.7,color=color_list[t])#,capsize=10)\n",
    "    [bar.set_alpha(0.7) for bar in bars]\n",
    "    #ax.plot(np.arange(1,11),medlist)\n",
    "\n",
    "#leg = ax.legend(['all','≤ 0.9','≤ 0.8'],title='Global $b$',ncol=1,\n",
    "#                handlelength = 1,labelspacing=0.4,columnspacing=1,handletextpad=0.4,frameon=False,\n",
    "#                loc='upper right',bbox_to_anchor=(1.03,1.06))\n",
    "leg = ax.legend(['all','≤ 0.9','≤ 0.8'],ncol=3,\n",
    "                handlelength = 1,labelspacing=0.4,columnspacing=1,handletextpad=0.4,frameon=True,\n",
    "                loc='upper right',bbox_to_anchor=(1.0,1),borderpad=0.3,fontsize=6)\n",
    "leg._legend_box.align = \"left\"\n",
    "leg.set_title('Global $b$',prop={'size':6})\n",
    "leg.get_frame().set_linewidth(0.5)\n",
    "\n",
    "ax.set_xlabel('# terms removed (by rank)',labelpad=2)\n",
    "ax.set_ylabel('\\n'.join(wrap('Relative fit ratio, SSE$_{b=1}$ / SSE$_{b=global}$',30)),labelpad=2)\n",
    "\n",
    "ax.axhline(y=1,lw=0.5,c='xkcd:grey',zorder=0)\n",
    "\n",
    "ax.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "ax.set_xticklabels([0,1,2,3,4,5,6,7,8,9,10,'all'])\n",
    "# create some more white space vertically\n",
    "ax.set_ylim(0.3,1.9)\n",
    "\n",
    "# add text in the white space\n",
    "ax.text(-0.34,0.35,'$b = 1$ favored',fontsize=6,color='k')\n",
    "ax.text(-0.34,1.74,'Global $b$ favored',fontsize=6)\n",
    "\n",
    "# Apply shading to above and below zero\n",
    "xmin = ax.get_xlim()[0]\n",
    "xmax = ax.get_xlim()[1]\n",
    "ymin = ax.get_ylim()[0]\n",
    "ymax = ax.get_ylim()[1]\n",
    "ax.autoscale(False)\n",
    "ax.fill([xmin,xmin,xmax,xmax], [ymin,1,1,ymin], color='xkcd:light grey', alpha=0.5, edgecolor=None, zorder=0)\n",
    "\n",
    "\n",
    "fig.savefig('msfigs/Fig3/removal_v02.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subplots version of the above for the SI\n",
    "fig,axes = plt.subplots(nrows=3, ncols=1,sharex=True,sharey=False,figsize=(5*0.65,6*0.58),constrained_layout=True)\n",
    "\n",
    "thresh_list = [1.0,0.9,0.8]\n",
    "\n",
    "offsets = [-0.1,0,0.1]\n",
    "\n",
    "rlist = ['1/inferred','main/inferred']\n",
    "\n",
    "for t in np.arange(len(thresh_list)):\n",
    "    for r in np.arange(len(rlist)):\n",
    "        medlist = []\n",
    "        uplist = []\n",
    "        downlist = []\n",
    "        for n in np.arange(11):\n",
    "            temp = mr2.loc[(mr2['num term add'] == n)&(mr2['main_b_-1to1'] <= thresh_list[t])].copy(deep=True)\n",
    "            my_median = np.median(temp[rlist[r]])\n",
    "            medlist = medlist + [my_median]\n",
    "            my_25 = np.quantile(temp[rlist[r]],0.25)\n",
    "            lower_error = my_median - my_25 \n",
    "            downlist = downlist + [lower_error]\n",
    "            my_75 = np.quantile(temp[rlist[r]],0.75)\n",
    "            upper_error = my_75 - my_median\n",
    "            uplist = uplist + [upper_error]\n",
    "        temp = mr2all.loc[mr2all['main_b_-1to1'] <= thresh_list[t]].copy(deep=True)\n",
    "        medlist = medlist + [np.median(temp[rlist[r]])]\n",
    "        my_25 = np.quantile(temp[rlist[r]],0.25)\n",
    "        lower_error = my_median - my_25 \n",
    "        downlist = downlist + [lower_error]\n",
    "        my_75 = np.quantile(temp[rlist[r]],0.75)\n",
    "        upper_error = my_75 - my_median\n",
    "        uplist = uplist + [upper_error]\n",
    "\n",
    "        my_error = [downlist,uplist]\n",
    "        markers,caps,bars = axes[t].errorbar(np.arange(12)+offsets[r],medlist,yerr=my_error,elinewidth=0.8,alpha=0.7)#,capsize=10)\n",
    "        [bar.set_alpha(0.7) for bar in bars]\n",
    "        if t == 0:\n",
    "            axes[t].legend(['SSE$_{b=1}$ / SSE$_{min}$','SSE$_{b=global}$ / SSE$_{min}$'],loc='upper right',bbox_to_anchor=(1.7,1))\n",
    "    axes[t].axhline(y=1,lw=0.5,c='xkcd:grey',zorder=0)\n",
    "    axes[t].text(0.03,0.87,'b ≤ '+str(thresh_list[t]),transform=axes[t].transAxes,\n",
    "                                                ha='left',fontsize=7,color='k')\n",
    "        \n",
    "axes[2].set_xlabel('# terms removed (by rank)')\n",
    "#axes[t].set_ylabel('SSE ratio')\n",
    "axes[2].set_xticks([0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "axes[2].set_xticklabels([0,1,2,3,4,5,6,7,8,9,10,'all'])\n",
    "\n",
    "fig.text(-0.03, 0.5, 'SSE ratio', va='center', rotation='vertical',size=8, fontweight = 'bold')\n",
    "\n",
    "fig.savefig('msfigs/Fig3/removal_2rats_v02.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMA1 adding analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a figure of adding terms one by one to PMA1 hap_4NQO prediction\n",
    "# First generate the data\n",
    "\n",
    "# Want to do ABC vs aBC plots now where, instead of subtracting terms by rank,\n",
    "# we add them by rank.\n",
    "\n",
    "#time the code\n",
    "#mystart = time.perf_counter()\n",
    "\n",
    "#megaadder = pd.DataFrame()\n",
    "o=10\n",
    "ncyc = 30\n",
    "ncycthresh = 2\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "diffthresh = 0.01\n",
    "\n",
    "for p in np.arange(0,1):\n",
    "#for p in np.arange(len(ploidies)):\n",
    "    for e in np.arange(1,2):\n",
    "    #for e in np.arange(len(envts)):\n",
    "        \n",
    "        #print(ploidies[p]+'_'+envts[e])\n",
    "        \n",
    "        # First, import a list of genotypes to predict\n",
    "        data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                             sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "        data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "        # Binary style for genotype\n",
    "        data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        # Since we're just using the predicted s, remove the obs columns\n",
    "        data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "\n",
    "        # Create a column for each locus\n",
    "        for l in np.arange(len(floci)):\n",
    "            data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "        \n",
    "        # Now import a set of coefficients\n",
    "        data0 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                                             sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_term','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "        data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "        data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "        data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "        for l in np.arange(len(floci)):\n",
    "            data0[floci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "            data1['without_'+floci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "        data0['numMut'] = data0[floci].sum(axis=1)\n",
    "        \n",
    "        # Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "        for i in np.arange(len(data0)):\n",
    "            if data0.loc[i,'numMut'] % 2 == 1:\n",
    "                data0.at[i,'baseline'] = -1\n",
    "            else:\n",
    "                data0.at[i,'baseline'] = 1\n",
    "\n",
    "        for l in np.arange(4,5):\n",
    "        #for l in np.arange(len(floci)):\n",
    "        \n",
    "            # Want to start with \"no-epistasis\" predictions in data1\n",
    "            # builder is where we build up coefficients for fitness predictions\n",
    "            # Start by setting all epistatic terms INVOLVING THE FOCAL LOCUS to 0\n",
    "\n",
    "            builder = data0.copy(deep=True)\n",
    "            builder.at[(builder['numMut'] > 1)&(builder[floci[l]] == 1),ploidies[p]+'_'+envts[e]+'_term'] = 0\n",
    "\n",
    "            for g in np.arange(len(data1)):\n",
    "                temp = builder.copy(deep=True)\n",
    "                # to make life faster, remove all zero values\n",
    "                temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                for t in np.arange(len(temp)):\n",
    "                    temprowlist = []\n",
    "                    for locus in np.arange(len(floci)):\n",
    "                        temprowlist = temprowlist + [temp.loc[t,floci[locus]]*data1.loc[g,floci[locus]]]\n",
    "                    #remove zeros\n",
    "                    temprowlist = [value for value in temprowlist if value != 0]\n",
    "                    # find product\n",
    "                    firstprod = np.prod(temprowlist)\n",
    "                    # subtract out baseline\n",
    "                    firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                    # multiply by term's value\n",
    "                    mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                    temp.at[t,'tosum'] = mytermval\n",
    "                totalsum = temp['tosum'].sum()\n",
    "                myintercept = data1.loc[(data1['genotype'] == '0000000000'),ploidies[p]+'_'+envts[e]+'_'+'Alex prediction'].values[0]\n",
    "                data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "            \n",
    "            locusadder = pd.DataFrame()\n",
    "            locusadder_full = pd.DataFrame()\n",
    "            \n",
    "            tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab0 = tab0[['genotype','new_pred','without_'+floci[l]]]\n",
    "            tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "            tab1 = tab1[['genotype','new_pred','without_'+floci[l]]]\n",
    "\n",
    "            templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "            for i in np.arange(len(floci)):\n",
    "                templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "                \n",
    "            templ['num term add'] = 0\n",
    "            locusadder_full = locusadder_full.append(templ,sort=False)\n",
    "                \n",
    "            # Get the 1:1 R^2 value\n",
    "            my_x = templ['new_pred_x']\n",
    "            my_y = templ['new_pred_y']\n",
    "            #my_y_mean = np.mean(my_y)\n",
    "            #my_x_mean = np.mean(my_x)\n",
    "            #TSS = np.sum((my_y - my_y_mean)**2)\n",
    "            #RSS = np.sum((my_y - my_y_mean - my_x + my_x_mean)**2)\n",
    "            \n",
    "            locusadder.at[0,'num term add'] = 0\n",
    "            locusadder.at[0,'term added'] = np.nan\n",
    "            locusadder.at[0,'term order'] = np.nan\n",
    "            locusadder.at[0,'coefficient'] = np.nan\n",
    "            #locusadder.at[0,'1:1 R2'] = 1-RSS/TSS\n",
    "            \n",
    "            # Perform standard least-squares linear regression\n",
    "            myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "            locusadder.at[0,'slope'] = myreg[0]\n",
    "            locusadder.at[0,'intercept'] = myreg[1]      \n",
    "            \n",
    "            # Begin cycles\n",
    "            data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[floci[l]] == 1)]\n",
    "            data0adder['abs coefficient'] = abs(data0adder[ploidies[p]+'_'+envts[e]+'_term'])\n",
    "            data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "            \n",
    "            for n in np.arange(5):\n",
    "            #for n in np.arange(ncyc):\n",
    "                # add the strongest epistatic coefficient that involves locus l\n",
    "                topind = data0adder.index[0]\n",
    "                builder.at[topind,ploidies[p]+'_'+envts[e]+'_term'] = data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                \n",
    "                # log coefficient info in locusadder\n",
    "                locusadder.at[n+1,'num term add'] = n+1\n",
    "                locusadder.at[n+1,'term added'] = data0adder.loc[topind,'genotype'] \n",
    "                locusadder.at[n+1,'term order'] = data0adder.loc[topind,'numMut']\n",
    "                locusadder.at[n+1,'coefficient'] = data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                \n",
    "                # Estimate the new predicted value for each genotype\n",
    "                for g in np.arange(len(data1)):\n",
    "                    temp = builder.copy(deep=True)\n",
    "                    # to make life faster, remove all zero values\n",
    "                    temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                    for t in np.arange(len(temp)):\n",
    "                        temprowlist = []\n",
    "                        for locus in np.arange(len(floci)):\n",
    "                            temprowlist = temprowlist + [temp.loc[t,floci[locus]]*data1.loc[g,floci[locus]]]\n",
    "                        #remove zeros\n",
    "                        temprowlist = [value for value in temprowlist if value != 0]\n",
    "                        # find product\n",
    "                        firstprod = np.prod(temprowlist)\n",
    "                        # subtract out baseline\n",
    "                        firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                        # multiply by term's value\n",
    "                        mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                        temp.at[t,'tosum'] = mytermval\n",
    "                    totalsum = temp['tosum'].sum()\n",
    "                    myintercept = data1.loc[(data1['genotype'] == '0000000000'),ploidies[p]+'_'+envts[e]+'_'+'Alex prediction'].values[0]\n",
    "                    data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "                    \n",
    "                # For the new predictions, get the values I want\n",
    "                tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab0 = tab0[['genotype','new_pred','without_'+floci[l]]]\n",
    "                tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab1 = tab1[['genotype','new_pred','without_'+floci[l]]]\n",
    "\n",
    "                templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "                \n",
    "                for i in np.arange(len(floci)):\n",
    "                    templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "                    \n",
    "                templ['num term add'] = n+1\n",
    "                locusadder_full = locusadder_full.append(templ,sort=False)\n",
    "\n",
    "                # Get the 1:1 R^2 value\n",
    "                my_x = templ['new_pred_x']\n",
    "                my_y = templ['new_pred_y']\n",
    "\n",
    "                # Perform standard least-squares linear regression\n",
    "                myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "                locusadder.at[n+1,'slope'] = myreg[0]\n",
    "                locusadder.at[n+1,'intercept'] = myreg[1]\n",
    "                \n",
    "                # Remove strongest epistatic coefficient from data0adder\n",
    "                data0adder = data0adder[1:]               \n",
    "                \n",
    "                if n >= ncycthresh:\n",
    "                    diff3 = abs(locusadder.loc[n+1,'slope'] - locusadder.loc[n-2,'slope'])\n",
    "                    diff2 = abs(locusadder.loc[n+1,'slope'] - locusadder.loc[n-1,'slope'])\n",
    "                    diff1 = abs(locusadder.loc[n+1,'slope'] - locusadder.loc[n,'slope'])   \n",
    "                    \n",
    "                    if diff3 <= diffthresh and diff2 <= diffthresh and diff1 <= diffthresh:\n",
    "                        break\n",
    " \n",
    "            locusadder.at[1:,'term added'] = locusadder.loc[1:,'term added'].astype(int).astype(str).str.zfill(10)\n",
    "            locusadder.insert(0,'main locus', floci[l])\n",
    "            locusadder.insert(0,'envt',envts[e])\n",
    "            locusadder.insert(0,'ploidy',ploidies[p])\n",
    "\n",
    "#mystop = time.perf_counter()\n",
    "#elapsed = mystop-mystart\n",
    "#print(str(elapsed)+' seconds to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as immediately above except 2 rows\n",
    "\n",
    "colorsnow = ['xkcd:cerulean','xkcd:orange','xkcd:bright green','xkcd:indigo','xkcd:pink','xkcd:rust','xkcd:gold','xkcd:bright aqua']\n",
    "markersize = 0.25\n",
    "myalpha = 0.5\n",
    "mylw = 0.6\n",
    "maincolor = 'xkcd:dark grey'\n",
    "linecolor = 'xkcd:grey'\n",
    "mainx=np.linspace(-1,1)\n",
    "bgl = ['WHI2','RHO5','MKT1']\n",
    "gs = [[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]]\n",
    "yorkn = 100\n",
    "\n",
    "p = 0\n",
    "e = 1\n",
    "l = 4\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True,figsize=(3.5*0.8,2.05*0.85))\n",
    "fig.subplots_adjust(wspace=0.2*0.85,hspace=0.65)\n",
    "\n",
    "#figd,axd = plt.subplots(nrows=1,ncols=8,sharex=True,sharey=True,figsize=(5,1.1),constrained_layout=True)\n",
    "\n",
    "for g in np.arange(len(gs)):\n",
    "    my_x = locusadder_full.loc[(locusadder_full['num term add'] == 0)&(locusadder_full[bgl[0]] == gs[g][0])&(locusadder_full[bgl[1]] == gs[g][1])&(locusadder_full[bgl[2]] == gs[g][2]),'new_pred_x']\n",
    "    my_y = locusadder_full.loc[(locusadder_full['num term add'] == 0)&(locusadder_full[bgl[0]] == gs[g][0])&(locusadder_full[bgl[1]] == gs[g][1])&(locusadder_full[bgl[2]] == gs[g][2]),'new_pred_y']\n",
    "\n",
    "    ax[0][0].scatter(my_x,my_y,s=markersize,alpha=myalpha,color=colorsnow[g])\n",
    "\n",
    "xs = (-0.5,0.1)\n",
    "ys = (-0.5,0.1)\n",
    "\n",
    "thisslope = locusadder.loc[(locusadder['num term add'] == 0),'slope'].values[0]\n",
    "thisintercept = locusadder.loc[(locusadder['num term add'] == 0),'intercept'].values[0]\n",
    "ax[0][0].plot(mainx,mainx*thisslope+thisintercept,color=linecolor,lw=mylw,zorder=0)\n",
    "ax[0][0].plot(mainx,mainx,color='k',lw=mylw,zorder=0)\n",
    "\n",
    "ax[0][0].set_xlim(xs)\n",
    "ax[0][0].set_ylim(ys)\n",
    "\n",
    "# Generate the full-model plot\n",
    "data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                     sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "# Since we're just using the predicted s, remove the obs columns\n",
    "data1 = data1.drop(columns=[ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err']).drop_duplicates('genotype').reset_index(drop=True)\n",
    "\n",
    "# Create a column for each locus\n",
    "for l1 in np.arange(len(floci)):\n",
    "    data1[floci[l1]] = data1.loc[:,'genotype'].str[l1].astype(int)\n",
    "    data1.loc[(data1[floci[l1]] == 0),floci[l1]] = -1\n",
    "    data1['without_'+floci[l1]] = data1.loc[:,'genotype'].str[:l1] + data1.loc[:,'genotype'].str[l1+1:]\n",
    "\n",
    "tab0 = data1.loc[(data1[floci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "tab0 = tab0[['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction','without_'+floci[l]]]\n",
    "tab1 = data1.loc[(data1[floci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "tab1 = tab1[['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction','without_'+floci[l]]]\n",
    "\n",
    "templ = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "for i in np.arange(len(floci)):\n",
    "    templ[floci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "# Get the 1:1 R^2 value\n",
    "my_x = templ[ploidies[p]+'_'+envts[e]+'_Alex prediction_x']\n",
    "my_y = templ[ploidies[p]+'_'+envts[e]+'_Alex prediction_y']\n",
    "myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "\n",
    "for g in np.arange(len(gs)):\n",
    "    my_x = templ.loc[(templ[bgl[0]] == gs[g][0])&(templ[bgl[1]] == gs[g][1])&(templ[bgl[2]] == gs[g][2]),ploidies[p]+'_'+envts[e]+'_Alex prediction_x']\n",
    "    my_y = templ.loc[(templ[bgl[0]] == gs[g][0])&(templ[bgl[1]] == gs[g][1])&(templ[bgl[2]] == gs[g][2]),ploidies[p]+'_'+envts[e]+'_Alex prediction_y']\n",
    "\n",
    "    ax[1][2].scatter(my_x,my_y,s=markersize,alpha=myalpha,color=colorsnow[g])\n",
    "\n",
    "ax[1][2].plot(mainx,mainx*myreg[0]+myreg[1],color=linecolor,lw=mylw,zorder=0)\n",
    "ax[1][2].plot(mainx,mainx,color='k',lw=mylw,zorder=0)\n",
    "\n",
    "ax[1][2].set_xlim(xs)\n",
    "ax[1][2].set_ylim(ys)\n",
    "\n",
    "nref = [[0,1],[0,2],[0,3],[1,0],[1,1]]\n",
    "for n in np.arange(5):\n",
    "    for g in np.arange(len(gs)):\n",
    "        my_x = locusadder_full.loc[(locusadder_full['num term add'] == n+1)&(locusadder_full[bgl[0]] == gs[g][0])&(locusadder_full[bgl[1]] == gs[g][1])&(locusadder_full[bgl[2]] == gs[g][2]),'new_pred_x']\n",
    "        my_y = locusadder_full.loc[(locusadder_full['num term add'] == n+1)&(locusadder_full[bgl[0]] == gs[g][0])&(locusadder_full[bgl[1]] == gs[g][1])&(locusadder_full[bgl[2]] == gs[g][2]),'new_pred_y']\n",
    "\n",
    "        ax[nref[n][0]][nref[n][1]].scatter(my_x,my_y,s=markersize,alpha=myalpha,color=colorsnow[g])\n",
    "    \n",
    "    thisslope = locusadder.loc[(locusadder['num term add'] == n+1),'slope'].values[0]\n",
    "    thisintercept = locusadder.loc[(locusadder['num term add'] == n+1),'intercept'].values[0]\n",
    "    \n",
    "    ax[nref[n][0]][nref[n][1]].plot(mainx,mainx*thisslope+thisintercept,color=linecolor,lw=mylw,zorder=0)\n",
    "    ax[nref[n][0]][nref[n][1]].plot(mainx,mainx,color='k',lw=mylw,zorder=0)\n",
    "\n",
    "    ax[nref[n][0]][nref[n][1]].set_xlim(xs)\n",
    "    ax[nref[n][0]][nref[n][1]].set_ylim(ys)\n",
    "\n",
    "    #axd[n+1].scatter(my_x,my_y-my_x,s=markersize,alpha=myalpha,color=maincolor)\n",
    "    #myreg = yorkreg_nocorr(my_x,my_y-my_x,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "    #axd[n+1].plot(mainx,mainx*myreg[0]+myreg[1],color=linecolor,lw=mylw,zorder=0)\n",
    "    #axd[n+1].axhline(y=0,color='grey',lw=0.5)\n",
    "\n",
    "    #axd[n+1].set_xlim(xsd)\n",
    "    #axd[n+1].set_ylim(ysd)\n",
    "    \n",
    "# Add a panel with the observed data\n",
    "data1 = pd.read_csv('CRISPR_10xmer_BFA_data/7_var_partition/lasso_v2_fa_'+ploidies[p]+'_'+envts[e]+'_'+str(o)+'.txt',\n",
    "                     sep='\\t',names=['genotype','todel',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "# Since we're just using the observed s, remove the pred columns\n",
    "data1 = data1.drop(columns=['todel']).reset_index(drop=True)\n",
    "\n",
    "# Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "# represented multiple times.\n",
    "glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "data2 = pd.DataFrame()\n",
    "\n",
    "for g in np.arange(len(glist)):\n",
    "    tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "    if len(tempg) == 1:\n",
    "        data2.at[g,'genotype'] = glist[g]\n",
    "        data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "        data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "    elif len(tempg) > 1:\n",
    "        data2.at[g,'genotype'] = glist[g]\n",
    "        data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "        my_svar = statistics.variance(tempg[ploidies[p]+'_'+envts[e]+'_s-obs'])\n",
    "        my_svar = 0\n",
    "        mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "        data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "        #data2.at[g,'stderr(s)'] = np.sqrt(np.sum(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2))/len(tempg)\n",
    "\n",
    "data1 = data2\n",
    "\n",
    "# Create a column for each locus, and for the genotype with that locus removed\n",
    "for l1 in np.arange(len(floci)):\n",
    "    data1[floci[l1]] = data1.loc[:,'genotype'].str[l1]\n",
    "for l1 in np.arange(len(floci)):\n",
    "    data1['without_'+floci[l1]] = data1.loc[:,'genotype'].str[:l1] + data1.loc[:,'genotype'].str[l1+1:]\n",
    "\n",
    "mysize = 1\n",
    "myalpha = 0.7\n",
    "elw = 0.5\n",
    "ealpha = 0.3\n",
    "\n",
    "tab0 = data1.loc[(data1[floci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "tab0 = tab0[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "tab1 = data1.loc[(data1[floci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "tab1 = tab1[['genotype','s','stderr(s)','without_'+floci[l]]]\n",
    "\n",
    "temp = pd.merge(tab0,tab1,how='inner',on='without_'+floci[l])\n",
    "\n",
    "for i in np.arange(len(floci)):\n",
    "    temp[floci[i]] = temp.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "\n",
    "for g in np.arange(len(gs)):\n",
    "    my_x = temp.loc[(temp[bgl[0]] == gs[g][0])&(temp[bgl[1]] == gs[g][1])&(temp[bgl[2]] == gs[g][2]),'s_x']\n",
    "    my_xerr = temp.loc[(temp[bgl[0]] == gs[g][0])&(temp[bgl[1]] == gs[g][1])&(temp[bgl[2]] == gs[g][2]),'stderr(s)_x']\n",
    "    my_y = temp.loc[(temp[bgl[0]] == gs[g][0])&(temp[bgl[1]] == gs[g][1])&(temp[bgl[2]] == gs[g][2]),'s_y']\n",
    "    my_yerr = temp.loc[(temp[bgl[0]] == gs[g][0])&(temp[bgl[1]] == gs[g][1])&(temp[bgl[2]] == gs[g][2]),'stderr(s)_y']\n",
    "    \n",
    "    markers0,caps0,bars0 = ax[1][3].errorbar(my_x,my_y,\n",
    "                            xerr = my_xerr,\n",
    "                            yerr = my_yerr,\n",
    "                            alpha=myalpha,linestyle='None',marker='.',ms=mysize,color=colorsnow[g],elinewidth=elw)\n",
    "\n",
    "    [bar.set_alpha(ealpha) for bar in bars0]\n",
    "\n",
    "myreg = yorkreg_nocorr(temp['s_x'],temp['s_y'],temp['stderr(s)_x'],temp['stderr(s)_y'],yorkn)\n",
    "\n",
    "ax[1][3].plot(mainx,mainx*myreg[0]+myreg[1],color=linecolor,lw=mylw,zorder=0)\n",
    "ax[1][3].plot(mainx,mainx,color='k',lw=mylw,zorder=0)\n",
    "\n",
    "ax[1][3].set_xlim(xs)\n",
    "ax[1][3].set_ylim(ys)\n",
    "\n",
    "for i in np.arange(4):\n",
    "    for j in np.arange(2):\n",
    "        ax[j][i].set_xticks([-0.4,0])\n",
    "        ax[j][i].set_yticks([-0.4,0])\n",
    "        \n",
    "fig.text(-0.02,0.25,'Fitness, PMA1 234C',fontsize=7,rotation='vertical')\n",
    "fig.text(0.37,-0.05,'Fitness, PMA1 234S',fontsize=7)\n",
    "\n",
    "plt.show()\n",
    "            \n",
    "fig.savefig('msfigs/Fig3/PMA14NQOhapadd_wObs_magical_2rows.pdf',bbox_inches='tight',dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets analysis (for revision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing: Analyze the Chou data and the Khan data from 2011\n",
    "# The first step will be to just plot the wt vs mut plots for all 9 mutations, run the regressions, and plot\n",
    "# these things\n",
    "\n",
    "chouloci = ['fghA','pntAB','gshA','GB']\n",
    "khanloci = ['rbs','topA','spoT','pykF','glmUS']\n",
    "\n",
    "yorkn = 100\n",
    "\n",
    "data1 = pd.read_csv('20211123_subsets-output/Chou_coeffs.txt',sep='\\t',names=['genotype','s_pred','s','stderr(s)'],\n",
    "                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(len(chouloci))\n",
    "        \n",
    "\n",
    "# Since we're just using the obs s, remove the pred columns\n",
    "data2 = data1.drop(columns=['s_pred'])\n",
    "            \n",
    "# Create a column for each locus, and for the genotype with that locus removed\n",
    "for l in np.arange(len(chouloci)):\n",
    "    data2[chouloci[l]] = data2.loc[:,'genotype'].str[l]\n",
    "for l in np.arange(len(chouloci)):\n",
    "    data2['without_'+chouloci[l]] = data2.loc[:,'genotype'].str[:l] + data2.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "# get ready to plot all 4 mutations\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(chouloci),figsize=(8,1.8))\n",
    "figd, axesd = plt.subplots(nrows=1, ncols=len(chouloci),figsize=(8,1.8))\n",
    "    \n",
    "for l in np.arange(len(chouloci)):\n",
    "    abc = pd.DataFrame()\n",
    "\n",
    "    tab0 = data2.loc[(data2[chouloci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "    tab0 = tab0[['genotype','s','stderr(s)','without_'+chouloci[l]]]\n",
    "    tab1 = data2.loc[(data2[chouloci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "    tab1 = tab1[['genotype','s','stderr(s)','without_'+chouloci[l]]]\n",
    "\n",
    "    temp = pd.merge(tab0,tab1,how='inner',on='without_'+chouloci[l])\n",
    "    #temp['s_diff'] = temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'] - temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x']\n",
    "\n",
    "    # get the x and y limits\n",
    "    #plt.scatter(temp['s_x'],temp['s_y'])\n",
    "    #xs = ajunk[l][e].get_xlim()\n",
    "    #ys = ajunk[l][e].get_ylim()\n",
    "\n",
    "    #lower = min(xs[0],ys[0])\n",
    "    #upper = max(xs[1],ys[1])\n",
    "\n",
    "    # plot the data\n",
    "    markers,caps,bars = axes[l].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                          xerr = temp['stderr(s)_x'],yerr = temp['stderr(s)_y'],alpha=0.7,\n",
    "                                          linestyle='None',elinewidth=0.5,marker='.',ms=5)\n",
    "\n",
    "    [bar.set_alpha(0.5) for bar in bars]\n",
    "    \n",
    "    xs = axes[l].get_xlim()\n",
    "    ys = axes[l].get_ylim()\n",
    "\n",
    "    lower = min(xs[0],ys[0])\n",
    "    upper = max(xs[1],ys[1])\n",
    "\n",
    "    # plot x = y\n",
    "    xlist = np.linspace(lower,upper)\n",
    "    axes[l].plot(xlist,xlist,c='k',lw=1,zorder=0)\n",
    "\n",
    "    # get the regression line and plot it\n",
    "    res = yorkreg_nocorr(temp['s_x'],temp['s_y'],temp['stderr(s)_x'],temp['stderr(s)_y'],yorkn)\n",
    "    axes[l].plot(np.linspace(lower,upper),np.linspace(lower,upper)*res[0]+res[1],color='xkcd:blue',alpha=1,zorder=0,lw=0.7)\n",
    "\n",
    "    axes[l].set_xlim(lower,upper)\n",
    "    axes[l].set_ylim(lower,upper)\n",
    "\n",
    "    axes[l].text(upper*0.97,lower*1.15,'b = '+str(round(res[0],3)),ha='right')\n",
    "    axes[l].text(upper*0.97,lower*1.05,chouloci[l],ha='right')\n",
    "    \n",
    "    # do ∆s plots too\n",
    "    temp['s_diff'] = temp['s_y'] - temp['s_x']\n",
    "\n",
    "    # plot the data\n",
    "    markers,caps,bars = axesd[l].errorbar(temp['s_x'],temp['s_diff'],\n",
    "                                          xerr = temp['stderr(s)_x'],yerr = temp['stderr(s)_y'],alpha=0.7,\n",
    "                                          linestyle='None',elinewidth=0.5,marker='.',ms=5)\n",
    "\n",
    "    [bar.set_alpha(0.5) for bar in bars]\n",
    "    \n",
    "    axesd[l].axhline(y=0,zorder=0,color='k',lw=0.5)\n",
    "    \n",
    "    xs = axesd[l].get_xlim()\n",
    "    ys = axesd[l].get_ylim()\n",
    "\n",
    "    lower = min(xs[0],ys[0])\n",
    "    upper = max(xs[1],ys[1])\n",
    "\n",
    "    # plot x = y\n",
    "    #xlist = np.linspace(lower,upper)\n",
    "    #axes[l].plot(xlist,xlist,c='k',lw=1,zorder=0)\n",
    "\n",
    "    # get the regression line and plot it\n",
    "    res = linregress(temp['s_x'],temp['s_diff'])\n",
    "    axesd[l].plot(np.linspace(xs[0],xs[1]),np.linspace(xs[0],xs[1])*res.slope+res.intercept,color='xkcd:blue',alpha=1,zorder=0,lw=0.7)\n",
    "\n",
    "    #axes[l].set_xlim(lower,upper)\n",
    "    #axes[l].set_ylim(lower,upper)\n",
    "\n",
    "    #axes[l].text(upper*0.97,lower*1.15,'b = '+str(round(res[0],3)),ha='right')\n",
    "    #axes[l].text(upper*0.97,lower*1.05,chouloci[l],ha='right')\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('************************')\n",
    "yorkn = 100\n",
    "\n",
    "data1 = pd.read_csv('20211123_subsets-output/khan_coeffs.txt',sep='\\t',names=['genotype','s_pred','s','stderr(s)'],\n",
    "                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "        \n",
    "\n",
    "# Since we're just using the obs s, remove the pred columns\n",
    "data2 = data1.drop(columns=['s_pred'])\n",
    "            \n",
    "# Create a column for each locus, and for the genotype with that locus removed\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data2[khanloci[l]] = data2.loc[:,'genotype'].str[l]\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data2['without_'+khanloci[l]] = data2.loc[:,'genotype'].str[:l] + data2.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "# get ready to plot all mutations\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(khanloci),figsize=(10,1.8))\n",
    "figd, axesd = plt.subplots(nrows=1, ncols=len(khanloci),figsize=(10,1.8))\n",
    "    \n",
    "for l in np.arange(len(khanloci)):\n",
    "    abc = pd.DataFrame()\n",
    "\n",
    "    tab0 = data2.loc[(data2[khanloci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "    tab0 = tab0[['genotype','s','stderr(s)','without_'+khanloci[l]]]\n",
    "    tab1 = data2.loc[(data2[khanloci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "    tab1 = tab1[['genotype','s','stderr(s)','without_'+khanloci[l]]]\n",
    "\n",
    "    temp = pd.merge(tab0,tab1,how='inner',on='without_'+khanloci[l])\n",
    "    #temp['s_diff'] = temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_y'] - temp[ploidies[p]+'_'+envts[e]+'_s-pred_'+str(o)+'_x']\n",
    "\n",
    "    # get the x and y limits\n",
    "    #plt.scatter(temp['s_x'],temp['s_y'])\n",
    "    #xs = ajunk[l][e].get_xlim()\n",
    "    #ys = ajunk[l][e].get_ylim()\n",
    "\n",
    "    #lower = min(xs[0],ys[0])\n",
    "    #upper = max(xs[1],ys[1])\n",
    "\n",
    "    # plot the data\n",
    "    markers,caps,bars = axes[l].errorbar(temp['s_x'],temp['s_y'],\n",
    "                                          xerr = temp['stderr(s)_x'],yerr = temp['stderr(s)_y'],alpha=0.7,\n",
    "                                          linestyle='None',elinewidth=0.5,marker='.',ms=5)\n",
    "\n",
    "    [bar.set_alpha(0.5) for bar in bars]\n",
    "    \n",
    "    xs = axes[l].get_xlim()\n",
    "    ys = axes[l].get_ylim()\n",
    "\n",
    "    lower = min(xs[0],ys[0])\n",
    "    upper = max(xs[1],ys[1])\n",
    "\n",
    "    # plot x = y\n",
    "    xlist = np.linspace(lower,upper)\n",
    "    axes[l].plot(xlist,xlist,c='k',lw=1,zorder=0)\n",
    "\n",
    "    # get the regression line and plot it\n",
    "    res = yorkreg_nocorr(temp['s_x'],temp['s_y'],temp['stderr(s)_x'],temp['stderr(s)_y'],yorkn)\n",
    "    axes[l].plot(np.linspace(lower,upper),np.linspace(lower,upper)*res[0]+res[1],color='xkcd:blue',alpha=1,zorder=0,lw=0.7)\n",
    "\n",
    "    axes[l].set_xlim(lower,upper)\n",
    "    axes[l].set_ylim(lower,upper)\n",
    "\n",
    "    axes[l].text(upper*0.98,lower*1.1,'b = '+str(round(res[0],3)),ha='right')\n",
    "    axes[l].text(upper*0.98,lower*1.01,khanloci[l],ha='right')\n",
    "    \n",
    "    # do ∆s plots too\n",
    "    temp['s_diff'] = temp['s_y'] - temp['s_x']\n",
    "    temp['s_diff-err'] = np.sqrt(temp['stderr(s)_x']**2+temp['stderr(s)_y']**2)\n",
    "\n",
    "    # plot the data\n",
    "    markers,caps,bars = axesd[l].errorbar(temp['s_x'],temp['s_diff'],\n",
    "                                          xerr = temp['stderr(s)_x'],yerr = temp['s_diff-err'],alpha=0.7,\n",
    "                                          linestyle='None',elinewidth=0.5,marker='.',ms=5)\n",
    "\n",
    "    [bar.set_alpha(0.5) for bar in bars]\n",
    "    \n",
    "    axesd[l].axhline(y=0,zorder=0,color='k',lw=0.5)\n",
    "    \n",
    "    xs = axesd[l].get_xlim()\n",
    "    ys = axesd[l].get_ylim()\n",
    "\n",
    "    lower = min(xs[0],ys[0])\n",
    "    upper = max(xs[1],ys[1])\n",
    "\n",
    "    # plot x = y\n",
    "    #xlist = np.linspace(lower,upper)\n",
    "    #axes[l].plot(xlist,xlist,c='k',lw=1,zorder=0)\n",
    "\n",
    "    # get the regression line and plot it\n",
    "    res = linregress(temp['s_x'],temp['s_diff'])\n",
    "    axesd[l].plot(np.linspace(xs[0],xs[1]),np.linspace(xs[0],xs[1])*res.slope+res.intercept,color='xkcd:blue',alpha=1,zorder=0,lw=0.7)\n",
    "\n",
    "    #axes[l].set_xlim(lower,upper)\n",
    "    #axes[l].set_ylim(lower,upper)\n",
    "\n",
    "    #axes[l].text(upper*0.97,lower*1.15,'b = '+str(round(res[0],3)),ha='right')\n",
    "    #axes[l].text(upper*0.97,lower*1.05,khanloci[l],ha='right')\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed with Khan data.\n",
    "# First, without getting into coefficients, how does it look if we color the clouds?\n",
    "\n",
    "yorkn = 100\n",
    "\n",
    "data1 = pd.read_csv('20211123_subsets-output/khan_coeffs.txt',sep='\\t',names=['genotype','s_pred','s','stderr(s)'],\n",
    "                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "        \n",
    "\n",
    "# Since we're just using the obs s, remove the pred columns\n",
    "data2 = data1.drop(columns=['s_pred'])\n",
    "            \n",
    "# Create a column for each locus, and for the genotype with that locus removed\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data2[khanloci[l]] = data2.loc[:,'genotype'].str[l]\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data2['without_'+khanloci[l]] = data2.loc[:,'genotype'].str[:l] + data2.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "# plot all mutations w/ 2 color\n",
    "fig, axes = plt.subplots(nrows=len(khanloci), ncols=len(khanloci),figsize=(8,8))\n",
    "    \n",
    "for l in np.arange(len(khanloci)):\n",
    "    abc = pd.DataFrame()\n",
    "\n",
    "    tab0 = data2.loc[(data2[khanloci[l]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "    tab0 = tab0[['genotype','s','stderr(s)','without_'+khanloci[l]]]\n",
    "    tab1 = data2.loc[(data2[khanloci[l]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "    tab1 = tab1[['genotype','s','stderr(s)','without_'+khanloci[l]]]\n",
    "\n",
    "    temp = pd.merge(tab0,tab1,how='inner',on='without_'+khanloci[l])\n",
    "    \n",
    "    for l2 in np.arange(len(khanloci)):\n",
    "        if l2 != l:\n",
    "\n",
    "            tab00 = data2.loc[(data2[khanloci[l]] == '0')&(data2[khanloci[l2]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab01 = data2.loc[(data2[khanloci[l]] == '0')&(data2[khanloci[l2]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "            tab10 = data2.loc[(data2[khanloci[l]] == '1')&(data2[khanloci[l2]] == '0')].copy(deep=True).reset_index(drop=True)\n",
    "            tab11 = data2.loc[(data2[khanloci[l]] == '1')&(data2[khanloci[l2]] == '1')].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "            temp0 = pd.merge(tab00,tab10,how='inner',on='without_'+khanloci[l])\n",
    "            temp1 = pd.merge(tab01,tab11,how='inner',on='without_'+khanloci[l])\n",
    "\n",
    "            # plot the data\n",
    "            markers,caps,bars = axes[l2][l].errorbar(temp0['s_x'],temp0['s_y'],\n",
    "                                                  xerr = temp0['stderr(s)_x'],yerr = temp0['stderr(s)_y'],alpha=0.7,\n",
    "                                                  linestyle='None',elinewidth=0.5,marker='.',ms=5,color='xkcd:cerulean')\n",
    "\n",
    "            [bar.set_alpha(0.5) for bar in bars]\n",
    "\n",
    "            markers,caps,bars = axes[l2][l].errorbar(temp1['s_x'],temp1['s_y'],\n",
    "                                                  xerr = temp1['stderr(s)_x'],yerr = temp1['stderr(s)_y'],alpha=0.7,\n",
    "                                                  linestyle='None',elinewidth=0.5,marker='.',ms=5,color='xkcd:orange')\n",
    "\n",
    "            [bar.set_alpha(0.5) for bar in bars]\n",
    "\n",
    "            xs = axes[l2][l].get_xlim()\n",
    "            ys = axes[l2][l].get_ylim()\n",
    "\n",
    "            lower = min(xs[0],ys[0])\n",
    "            upper = max(xs[1],ys[1])\n",
    "\n",
    "            # plot x = y\n",
    "            xlist = np.linspace(lower,upper)\n",
    "            axes[l2][l].plot(xlist,xlist,c='k',lw=1,zorder=0)\n",
    "\n",
    "            # get the regression line and plot it\n",
    "            res = yorkreg_nocorr(temp['s_x'],temp['s_y'],temp['stderr(s)_x'],temp['stderr(s)_y'],yorkn)\n",
    "            axes[l2][l].plot(np.linspace(lower,upper),np.linspace(lower,upper)*res[0]+res[1],color='xkcd:goldenrod',alpha=1,zorder=0,lw=0.7)\n",
    "\n",
    "            res = yorkreg_nocorr(temp0['s_x'],temp0['s_y'],temp0['stderr(s)_x'],temp0['stderr(s)_y'],yorkn)\n",
    "            axes[l2][l].plot(np.linspace(lower,upper),np.linspace(lower,upper)*res[0]+res[1],color='xkcd:cerulean',alpha=1,zorder=0,lw=0.7)\n",
    "\n",
    "            res = yorkreg_nocorr(temp1['s_x'],temp1['s_y'],temp1['stderr(s)_x'],temp1['stderr(s)_y'],yorkn)\n",
    "            axes[l2][l].plot(np.linspace(lower,upper),np.linspace(lower,upper)*res[0]+res[1],color='xkcd:orange',alpha=1,zorder=0,lw=0.7)\n",
    "\n",
    "            axes[l2][l].set_xlim(lower,upper)\n",
    "            axes[l2][l].set_ylim(lower,upper)\n",
    "            \n",
    "            if l2 == 0:\n",
    "                axes[l2][l].set_xlabel(khanloci[l])\n",
    "            \n",
    "            if l == 0:\n",
    "                axes[l2][l].set_ylabel(khanloci[l2])\n",
    "\n",
    "            #axes[l2][l].text(upper*0.98,lower*1.1,'b = '+str(round(res[0],3)),ha='right')\n",
    "            #axes[l2][l].text(upper*0.98,lower*1.01,khanloci[l],ha='right')\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having seen that we get some clustering, let's do a proper \"remover\" analysis on these bad boys from Khan et al 2011\n",
    "\n",
    "#time the code\n",
    "mystart = time.perf_counter()\n",
    "\n",
    "megaremoverkhan = pd.DataFrame()\n",
    "ncyc = 30\n",
    "ncycthresh = 10\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "#diffthresh = 0.01\n",
    "ethresh = 0.50\n",
    "\n",
    "# First, import a list of genotypes to predict\n",
    "data1 = pd.read_csv('20211123_subsets-output/khan_coeffs.txt',sep='\\t',names=['genotype','s_pred','s','stderr(s)'],\n",
    "                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "            \n",
    "# Create a column for each locus, and for the genotype with that locus removed\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data1[khanloci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "    data1.loc[(data1[khanloci[l]] == 0),khanloci[l]] = -1\n",
    "\n",
    "# Calculate the difference between observed and predicted, such that s_obs = s_pred + diff\n",
    "data1['opdiff'] = data1['s'] - data1['s_pred']\n",
    "\n",
    "# Now import a set of coefficients\n",
    "data0 = pd.read_csv('20211123_subsets-output/khan_coeffs.txt',sep='\\t',names=['todelete','genotype','term','na'],\n",
    "                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "\n",
    "data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data0[khanloci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "    data1['without_'+khanloci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "data0['numMut'] = data0[khanloci].sum(axis=1)\n",
    "\n",
    "# Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "for i in np.arange(len(data0)):\n",
    "    if data0.loc[i,'numMut'] % 2 == 1:\n",
    "        data0.at[i,'baseline'] = -1\n",
    "    else:\n",
    "        data0.at[i,'baseline'] = 1\n",
    "\n",
    "#for l in np.arange(0,1):\n",
    "for l in np.arange(len(khanloci)):\n",
    "\n",
    "    # Want to start with \"all-epistasis\" observed fitnesses in data1\n",
    "    # Get b_obs, TLS_b_obs, TLS_1 (and intercept values)\n",
    "    tab0 = data1.loc[(data1[khanloci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "    tab0 = tab0[['genotype','s','stderr(s)','without_'+khanloci[l]]]\n",
    "    tab1 = data1.loc[(data1[khanloci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "    tab1 = tab1[['genotype','s','stderr(s)','without_'+khanloci[l]]]\n",
    "\n",
    "    templ = pd.merge(tab0,tab1,how='inner',on='without_'+khanloci[l])\n",
    "\n",
    "    for i in np.arange(len(khanloci)):\n",
    "        templ[khanloci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "    # York regression to get sum of total least squares deviations, S\n",
    "    myreg = yorkreg_nocorr(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "    myreg1 = york_slope1(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "\n",
    "    # Create a subtable - we'll append these together into megaremoverkhan\n",
    "    locusremoverkhan = pd.DataFrame()\n",
    "\n",
    "    # wrap up the below, haven't edited yet...\n",
    "    locusremoverkhan.at[0,'num term add'] = 0\n",
    "    locusremoverkhan.at[0,'term added'] = 'na'\n",
    "    locusremoverkhan.at[0,'term order'] = 'na'\n",
    "    locusremoverkhan.at[0,'coefficient'] = 'na'\n",
    "    locusremoverkhan.at[0,'inferred_b'] = myreg[0]\n",
    "    locusremoverkhan.at[0,'inferred_a'] = myreg[1]\n",
    "    locusremoverkhan.at[0,'inferred_S'] = myreg[3]\n",
    "    locusremoverkhan.at[0,'main_b'] = myreg[0]\n",
    "    locusremoverkhan.at[0,'main_a'] = myreg[1]\n",
    "    locusremoverkhan.at[0,'main_S'] = myreg[3]\n",
    "    locusremoverkhan.at[0,'N'] = len(templ)\n",
    "    locusremoverkhan.at[0,'1_a'] = myreg1[0]\n",
    "    locusremoverkhan.at[0,'1_S'] = myreg1[1]\n",
    "\n",
    "    # OPTION TO CONSIDER: ADD IN ∆S FORMULATIONS, DIVERGENCE FROM SLOPE OF 0\n",
    "\n",
    "    # Begin cycles\n",
    "    # data0adder serves as the sorted databank for (non-zero) coefficients\n",
    "    data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[khanloci[l]] == 1)]\n",
    "    data0adder['abs coefficient'] = abs(data0adder['term'])\n",
    "    data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "\n",
    "    # create a builder dataframe on which changes will be processed\n",
    "    builder = data0.copy(deep=True)\n",
    "\n",
    "    for n in np.arange(ncyc):\n",
    "    #for n in np.arange(0,1):\n",
    "        # find the top index (i.e., the strongest coefficient)\n",
    "        topind = data0adder.index[n]\n",
    "\n",
    "        # remove the strongest epistatic coefficient that involves locus l\n",
    "        builder.at[topind,'term'] = 0\n",
    "\n",
    "        # add coefficient info in locusadder\n",
    "        locusremoverkhan.at[n+1,'num term add'] = n+1\n",
    "        locusremoverkhan.at[n+1,'term added'] = data0adder.loc[topind,'genotype'] \n",
    "        locusremoverkhan.at[n+1,'term order'] = data0adder.loc[topind,'numMut']\n",
    "        locusremoverkhan.at[n+1,'coefficient'] = data0adder.loc[topind,'term']\n",
    "\n",
    "        # Estimate the new predicted value for each genotype\n",
    "        for g in np.arange(len(data1)):\n",
    "            temp = builder.copy(deep=True)\n",
    "            # to make life faster, remove all zero values\n",
    "            temp = temp.loc[(temp['term'] != 0)].reset_index(drop=True)\n",
    "            for t in np.arange(len(temp)):\n",
    "                temprowlist = []\n",
    "                for locus in np.arange(len(khanloci)):\n",
    "                    temprowlist = temprowlist + [temp.loc[t,khanloci[locus]]*data1.loc[g,khanloci[locus]]]\n",
    "                #remove zeros\n",
    "                temprowlist = [value for value in temprowlist if value != 0]\n",
    "                # find product\n",
    "                firstprod = np.prod(temprowlist)\n",
    "                # subtract out baseline\n",
    "                firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                # multiply by term's value\n",
    "                mytermval = firstprod_lessbaseline * temp.loc[t,'term']\n",
    "                temp.at[t,'tosum'] = mytermval\n",
    "            totalsum = temp['tosum'].sum()\n",
    "            myintercept = data1.loc[(data1['genotype'] == '00000'),'s_pred'].values[0]\n",
    "            data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "        data1['new_obs'] = data1['new_pred'] + data1['opdiff']\n",
    "\n",
    "        # For the new predictions, get the values I want\n",
    "        tab0 = data1.loc[(data1[khanloci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "        tab0 = tab0[['genotype','new_obs','stderr(s)','without_'+khanloci[l]]]\n",
    "        tab1 = data1.loc[(data1[khanloci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "        tab1 = tab1[['genotype','new_obs','stderr(s)','without_'+khanloci[l]]]\n",
    "\n",
    "        templ = pd.merge(tab0,tab1,how='inner',on='without_'+khanloci[l])\n",
    "\n",
    "        for i in np.arange(len(khanloci)):\n",
    "            templ[khanloci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "        my_x = templ['new_obs_x']\n",
    "        my_y = templ['new_obs_y']\n",
    "        my_x_err = templ['stderr(s)_x']\n",
    "        my_y_err = templ['stderr(s)_y']\n",
    "\n",
    "        myregnew = yorkreg_nocorr(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "        locusremoverkhan.at[n+1,'inferred_b'] = myregnew[0]\n",
    "        locusremoverkhan.at[n+1,'inferred_a'] = myregnew[1]\n",
    "        locusremoverkhan.at[n+1,'inferred_S'] = myregnew[3]\n",
    "\n",
    "        myreg_main = york_slopeanyb(my_x,my_y,my_x_err,my_y_err,yorkn,myreg[0])\n",
    "        locusremoverkhan.at[n+1,'main_b'] = myreg[0]\n",
    "        locusremoverkhan.at[n+1,'main_a'] = myreg_main[0]\n",
    "        locusremoverkhan.at[n+1,'main_S'] = myreg_main[1]\n",
    "\n",
    "        myreg_1 = york_slope1(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "        locusremoverkhan.at[n+1,'1_a'] = myreg_1[0]\n",
    "        locusremoverkhan.at[n+1,'1_S'] = myreg_1[1]\n",
    "\n",
    "        locusremoverkhan.at[n+1,'N'] = len(templ)\n",
    "\n",
    "        print(abs(locusremoverkhan.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum())\n",
    "\n",
    "        #if n >= ncycthresh:\n",
    "        #    if (abs(locusremoverkhan.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum()) > ethresh:\n",
    "        #        break\n",
    "        if data0adder.loc[topind,'term'] == 0:\n",
    "            break\n",
    "\n",
    "    locusremoverkhan.at[1:,'term added'] = locusremoverkhan.loc[1:,'term added'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "    locusremoverkhan.insert(0,'main locus', khanloci[l])\n",
    "    #locusremoverkhan.insert(0,'envt',envts[e])\n",
    "    #locusremoverkhan.insert(0,'ploidy',ploidies[p])\n",
    "\n",
    "    megaremoverkhan = megaremoverkhan.append(locusremoverkhan)\n",
    "    print(khanloci[l])\n",
    "\n",
    "mystop = time.perf_counter()\n",
    "elapsed = mystop-mystart\n",
    "print(str(elapsed)+' seconds to run')\n",
    "                \n",
    "export_csv = megaremoverkhan.to_csv(r'20210712_megaremoverkhan.csv',index=True,header=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the megaremoverv2\n",
    "mr2 = pd.read_csv('20210712_megaremoverkhan.csv')\n",
    "mr2 = mr2.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'main_b'] <= 1:\n",
    "        mr2.at[i,'main_b_-1to1'] = mr2.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2.at[i,'main_b_-1to1'] = 1/mr2.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2)):\n",
    "    if mr2.loc[i,'inferred_b'] <= 1:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = mr2.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2.at[i,'inferred_b_-1to1'] = 1/mr2.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2['main/inferred'] = mr2['main_S']/mr2['inferred_S']\n",
    "mr2['1/inferred'] = mr2['1_S']/mr2['inferred_S']\n",
    "mr2['1/main'] = mr2['1_S']/mr2['main_S']\n",
    "\n",
    "# Plot ratios\n",
    "for l in np.arange(len(khanloci)):\n",
    "    mr2sub = mr2.loc[(mr2['main locus'] == khanloci[l])].copy(deep=True).reset_index(drop=True)\n",
    "    plt.plot(mr2sub['num term add'],mr2sub['1/main'])\n",
    "plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "plt.legend(khanloci)\n",
    "plt.xlabel('num term removed')\n",
    "plt.ylabel('1/main')\n",
    "plt.show()\n",
    "\n",
    "# do same thing for 1/inferred\n",
    "for l in np.arange(len(khanloci)):\n",
    "    mr2sub = mr2.loc[(mr2['main locus'] == khanloci[l])].copy(deep=True).reset_index(drop=True)\n",
    "    plt.plot(mr2sub['num term add'],mr2sub['1/inferred'])\n",
    "plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "plt.legend(khanloci)\n",
    "plt.xlabel('num term removed')\n",
    "plt.ylabel('1/inferred')\n",
    "plt.show()\n",
    "\n",
    "# do same thing for main/inferred\n",
    "for l in np.arange(len(khanloci)):\n",
    "    mr2sub = mr2.loc[(mr2['main locus'] == khanloci[l])].copy(deep=True).reset_index(drop=True)\n",
    "    plt.plot(mr2sub['num term add'],mr2sub['main/inferred'])\n",
    "plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "plt.legend(khanloci)\n",
    "plt.xlabel('num term removed')\n",
    "plt.ylabel('main/inferred')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do adding analysis, this time with just pykF since it seems to have the strangest behavior\n",
    "megaadder = pd.DataFrame()\n",
    "o=10\n",
    "ncyc = 30\n",
    "ncycthresh = 2\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "diffthresh = 0.01\n",
    "\n",
    "\n",
    "data1 = pd.read_csv('20211123_subsets-output/khan_coeffs.txt',sep='\\t',names=['genotype','s_pred','s','stderr(s)'],\n",
    "                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "# Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "# Binary style for genotype\n",
    "data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "            \n",
    "# Create a column for each locus, and for the genotype with that locus removed\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data1[khanloci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "    data1.loc[(data1[khanloci[l]] == 0),khanloci[l]] = -1\n",
    "\n",
    "# Calculate the difference between observed and predicted, such that s_obs = s_pred + diff\n",
    "data1['opdiff'] = data1['s'] - data1['s_pred']\n",
    "\n",
    "# Now import a set of coefficients\n",
    "data0 = pd.read_csv('20211123_subsets-output/khan_coeffs.txt',sep='\\t',names=['todelete','genotype','term','na'],\n",
    "                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "\n",
    "data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "for l in np.arange(len(khanloci)):\n",
    "    data0[khanloci[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "    data1['without_'+khanloci[l]] = data1.loc[:,'genotype'].str[:l] + data1.loc[:,'genotype'].str[l+1:]\n",
    "\n",
    "data0['numMut'] = data0[khanloci].sum(axis=1)\n",
    "\n",
    "# Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "for i in np.arange(len(data0)):\n",
    "    if data0.loc[i,'numMut'] % 2 == 1:\n",
    "        data0.at[i,'baseline'] = -1\n",
    "    else:\n",
    "        data0.at[i,'baseline'] = 1\n",
    "\n",
    "for l in np.arange(3,4):\n",
    "#for l in np.arange(len(khanloci)):\n",
    "    \n",
    "    # plot to start out, as sanity check\n",
    "    tab0 = data1.loc[(data1[khanloci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "    tab0 = tab0[['genotype','s_pred','without_'+khanloci[l]]]\n",
    "    tab1 = data1.loc[(data1[khanloci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "    tab1 = tab1[['genotype','s_pred','without_'+khanloci[l]]]\n",
    "    \n",
    "    templ = pd.merge(tab0,tab1,how='inner',on='without_'+khanloci[l])\n",
    "    \n",
    "    my_x = templ['s_pred_x']\n",
    "    my_y = templ['s_pred_y']\n",
    "\n",
    "    myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "    print('york slope = '+str(myreg[0]))\n",
    "\n",
    "    # Want to start with \"no-epistasis\" predictions in data1\n",
    "    # builder is where we build up coefficients for fitness predictions\n",
    "    # Start by setting all epistatic terms INVOLVING THE FOCAL LOCUS to 0\n",
    "\n",
    "    builder = data0.copy(deep=True)\n",
    "    builder.at[(builder['numMut'] > 1)&(builder[khanloci[l]] == 1),'term'] = 0\n",
    "\n",
    "    for g in np.arange(len(data1)):\n",
    "        temp = builder.copy(deep=True)\n",
    "        # to make life faster, remove all zero values\n",
    "        temp = temp.loc[(temp['term'] != 0)].reset_index(drop=True)\n",
    "        for t in np.arange(len(temp)):\n",
    "            temprowlist = []\n",
    "            for locus in np.arange(len(khanloci)):\n",
    "                temprowlist = temprowlist + [temp.loc[t,khanloci[locus]]*data1.loc[g,khanloci[locus]]]\n",
    "            #remove zeros\n",
    "            temprowlist = [value for value in temprowlist if value != 0]\n",
    "            # find product\n",
    "            firstprod = np.prod(temprowlist)\n",
    "            # subtract out baseline\n",
    "            firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "            # multiply by term's value\n",
    "            mytermval = firstprod_lessbaseline * temp.loc[t,'term']\n",
    "            temp.at[t,'tosum'] = mytermval\n",
    "        totalsum = temp['tosum'].sum()\n",
    "        myintercept = data1.loc[(data1['genotype'] == '00000'),'s_pred'].values[0]\n",
    "        data1.at[g,'noEp_pred'] = totalsum + myintercept\n",
    "\n",
    "    locusadder = pd.DataFrame()\n",
    "\n",
    "    tab0 = data1.loc[(data1[khanloci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "    tab0 = tab0[['genotype','noEp_pred','without_'+khanloci[l]]]\n",
    "    tab1 = data1.loc[(data1[khanloci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "    tab1 = tab1[['genotype','noEp_pred','without_'+khanloci[l]]]\n",
    "\n",
    "    templ = pd.merge(tab0,tab1,how='inner',on='without_'+khanloci[l])\n",
    "\n",
    "    for i in np.arange(len(khanloci)):\n",
    "        templ[khanloci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "    # Get the 1:1 R^2 value\n",
    "    my_x = templ['noEp_pred_x']\n",
    "    my_y = templ['noEp_pred_y']\n",
    "\n",
    "    locusadder.at[0,'num term add'] = 0\n",
    "    locusadder.at[0,'term added'] = np.nan\n",
    "    locusadder.at[0,'term order'] = np.nan\n",
    "    locusadder.at[0,'coefficient'] = np.nan\n",
    "    #locusadder.at[0,'1:1 R2'] = 1-RSS/TSS\n",
    "\n",
    "    # Perform standard least-squares linear regression\n",
    "    #myreg = linregress(my_x,my_y)\n",
    "    myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "    locusadder.at[0,'abc-original_slope'] = myreg[0]\n",
    "    locusadder.at[0,'abc-original_intercept'] = myreg[1]\n",
    "    myreg = yorkreg_nocorr(my_x,my_y-my_x,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "    locusadder.at[0,'deltas-original_slope'] = myreg[0]\n",
    "    locusadder.at[0,'deltas-original_intercept'] = myreg[1]\n",
    "    myreg = yorkreg_nocorr(my_y,my_x-my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "    locusadder.at[0,'deltas-reversion_slope'] = myreg[0]\n",
    "    locusadder.at[0,'deltas-reversion_intercept'] = myreg[1]\n",
    "    #locusadder.at[0,'linreg r2'] = myreg.rvalue**2\n",
    "    #locusadder.at[0,'stderr_slope'] = myreg.stderr\n",
    "    #locusadder.at[0,'stderr_intercept'] = myreg.intercept_stderr\n",
    "    # Compare this slope to 1\n",
    "    #myt = (myreg.slope - 1) / (myreg.stderr - 0)\n",
    "    #locusadder.at[0,'t_stat'] = myt\n",
    "    #mydf = len(templ) - 2\n",
    "    # two-sided t test p value\n",
    "    #myp = stats.t.sf(np.abs(myt), mydf)*2\n",
    "    #locusadder.at[0,'p_val'] = myp\n",
    "\n",
    "    #plt.scatter(my_x,my_y)\n",
    "    #plt.show()\n",
    "\n",
    "    # Begin cycles\n",
    "    data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[khanloci[l]] == 1)]\n",
    "    data0adder['abs coefficient'] = abs(data0adder['term'])\n",
    "    data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "\n",
    "    for n in np.arange(ncyc):\n",
    "        # add the strongest epistatic coefficient that involves locus l\n",
    "        topind = data0adder.index[0]\n",
    "        builder.at[topind,'term'] = data0adder.loc[topind,'term']\n",
    "\n",
    "        # log coefficient info in locusadder\n",
    "        locusadder.at[n+1,'num term add'] = n+1\n",
    "        locusadder.at[n+1,'term added'] = data0adder.loc[topind,'genotype'] \n",
    "        locusadder.at[n+1,'term order'] = data0adder.loc[topind,'numMut']\n",
    "        locusadder.at[n+1,'coefficient'] = data0adder.loc[topind,'term']\n",
    "\n",
    "        # Estimate the new predicted value for each genotype\n",
    "        for g in np.arange(len(data1)):\n",
    "            temp = builder.copy(deep=True)\n",
    "            # to make life faster, remove all zero values\n",
    "            temp = temp.loc[(temp['term'] != 0)].reset_index(drop=True)\n",
    "            for t in np.arange(len(temp)):\n",
    "                temprowlist = []\n",
    "                for locus in np.arange(len(khanloci)):\n",
    "                    temprowlist = temprowlist + [temp.loc[t,khanloci[locus]]*data1.loc[g,khanloci[locus]]]\n",
    "                #remove zeros\n",
    "                temprowlist = [value for value in temprowlist if value != 0]\n",
    "                # find product\n",
    "                firstprod = np.prod(temprowlist)\n",
    "                # subtract out baseline\n",
    "                firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                # multiply by term's value\n",
    "                mytermval = firstprod_lessbaseline * temp.loc[t,'term']\n",
    "                temp.at[t,'tosum'] = mytermval\n",
    "            totalsum = temp['tosum'].sum()\n",
    "            myintercept = data1.loc[(data1['genotype'] == '00000'),'s_pred'].values[0]\n",
    "            data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "\n",
    "        # For the new predictions, get the values I want\n",
    "        tab0 = data1.loc[(data1[khanloci[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "        tab0 = tab0[['genotype','new_pred','without_'+khanloci[l]]]\n",
    "        tab1 = data1.loc[(data1[khanloci[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "        tab1 = tab1[['genotype','new_pred','without_'+khanloci[l]]]\n",
    "\n",
    "        templ = pd.merge(tab0,tab1,how='inner',on='without_'+khanloci[l])\n",
    "\n",
    "        for i in np.arange(len(khanloci)):\n",
    "            templ[khanloci[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "        # Get the 1:1 R^2 value\n",
    "        my_x = templ['new_pred_x']\n",
    "        my_y = templ['new_pred_y']\n",
    "        #my_y_mean = np.mean(my_y)\n",
    "        #my_x_mean = np.mean(my_x)\n",
    "        #TSS = np.sum((my_y - my_y_mean)**2)\n",
    "        #RSS = np.sum((my_y - my_y_mean - my_x + my_x_mean)**2)\n",
    "\n",
    "        #locusadder.at[n+1,'1:1 R2'] = 1-RSS/TSS\n",
    "\n",
    "        # Perform standard least-squares linear regression\n",
    "        #myreg = linregress(my_x,my_y)\n",
    "        myreg = yorkreg_nocorr(my_x,my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "        locusadder.at[n+1,'abc-original_slope'] = myreg[0]\n",
    "        locusadder.at[n+1,'abc-original_intercept'] = myreg[1]\n",
    "        myreg = yorkreg_nocorr(my_x,my_y-my_x,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "        locusadder.at[n+1,'deltas-original_slope'] = myreg[0]\n",
    "        locusadder.at[n+1,'deltas-original_intercept'] = myreg[1]\n",
    "        myreg = yorkreg_nocorr(my_y,my_x-my_y,[0]*len(my_x),[0]*len(my_y),yorkn)\n",
    "        locusadder.at[n+1,'deltas-reversion_slope'] = myreg[0]\n",
    "        locusadder.at[n+1,'deltas-reversion_intercept'] = myreg[1]\n",
    "        #locusadder.at[n+1,'slope'] = myreg.slope\n",
    "        #locusadder.at[n+1,'intercept'] = myreg.intercept\n",
    "        #locusadder.at[n+1,'linreg r2'] = myreg.rvalue**2\n",
    "        #locusadder.at[n+1,'stderr_slope'] = myreg.stderr\n",
    "        #locusadder.at[n+1,'stderr_intercept'] = myreg.intercept_stderr\n",
    "        # Compare this slope to 1\n",
    "        #myt = (myreg.slope - 1) / (myreg.stderr - 0)\n",
    "        #locusadder.at[n+1,'t_stat'] = myt\n",
    "        #mydf = len(templ) - 2\n",
    "        # two-sided t test p value\n",
    "        #myp = stats.t.sf(np.abs(myt), mydf)*2\n",
    "        #locusadder.at[n+1,'p_val'] = myp\n",
    "\n",
    "        # Remove strongest epistatic coefficient from data0adder\n",
    "        data0adder = data0adder[1:]\n",
    "\n",
    "        #plt.scatter(my_x,my_y)\n",
    "        #plt.plot(np.linspace(-0.4,0.4),np.linspace(-0.4,0.4))\n",
    "        #plt.plot(np.linspace(-0.4,0.4),np.linspace(-0.4,0.4)*myreg[0]+myreg[1])\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "        if n >= ncycthresh:\n",
    "            diff3 = abs(locusadder.loc[n+1,'abc-original_slope'] - locusadder.loc[n-2,'abc-original_slope'])\n",
    "            diff2 = abs(locusadder.loc[n+1,'abc-original_slope'] - locusadder.loc[n-1,'abc-original_slope'])\n",
    "            diff1 = abs(locusadder.loc[n+1,'abc-original_slope'] - locusadder.loc[n,'abc-original_slope'])\n",
    "\n",
    "            if diff3 <= diffthresh and diff2 <= diffthresh and diff1 <= diffthresh:\n",
    "                diff3do = abs(locusadder.loc[n+1,'deltas-original_slope'] - locusadder.loc[n-2,'deltas-original_slope'])\n",
    "                diff2do = abs(locusadder.loc[n+1,'deltas-original_slope'] - locusadder.loc[n-1,'deltas-original_slope'])\n",
    "                diff1do = abs(locusadder.loc[n+1,'deltas-original_slope'] - locusadder.loc[n,'deltas-original_slope'])\n",
    "\n",
    "                if diff3do <= diffthresh and diff2do <= diffthresh and diff1do <= diffthresh:\n",
    "                    diff3dr = abs(locusadder.loc[n+1,'deltas-reversion_slope'] - locusadder.loc[n-2,'deltas-reversion_slope'])\n",
    "                    diff2dr = abs(locusadder.loc[n+1,'deltas-reversion_slope'] - locusadder.loc[n-1,'deltas-reversion_slope'])\n",
    "                    diff1dr = abs(locusadder.loc[n+1,'deltas-reversion_slope'] - locusadder.loc[n,'deltas-reversion_slope'])\n",
    "\n",
    "                    if diff3dr <= diffthresh and diff2dr <= diffthresh and diff1dr <= diffthresh:\n",
    "                        break\n",
    "\n",
    "    locusadder.at[1:,'term added'] = locusadder.loc[1:,'term added'].astype(int).astype(str).str.zfill(len(khanloci))\n",
    "    locusadder.insert(0,'main locus', khanloci[l])\n",
    "    megaadder = megaadder.append(locusadder)\n",
    "    print(khanloci[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the subset numbers\n",
    "ssnames = pd.read_csv('20211123_subsets-output/subsets_filenames.txt')\n",
    "ssnums = ssnames['filename'].str[15:].str.split('.').str[0]\n",
    "ssnums = list(ssnums.astype(int).sort_values().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the removal analysis now with the subsets data\n",
    "\n",
    "# Want to do ABC vs aBC plots now where, instead of adding terms by rank to form predicted values,\n",
    "# we subtract them by rank to adjust observed values.\n",
    "#time the code\n",
    "mystart = time.perf_counter()\n",
    "\n",
    "megaremoverss = pd.DataFrame()\n",
    "#ss stands for subset\n",
    "\n",
    "o=10\n",
    "ncyc = 7\n",
    "ncycthresh = 10\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "#diffthresh = 0.01\n",
    "ethresh = 0.50\n",
    "\n",
    "#for ss in np.arange(0,1):\n",
    "for ss in np.arange(len(ssnums)):\n",
    "    \n",
    "    ## for now, just do this for one in every 5!\n",
    "    #if ss % 5 == 1:\n",
    "    \n",
    "        megaremoverv2 = pd.DataFrame()\n",
    "        myssnum = ssnums[ss]\n",
    "        print(str(myssnum))\n",
    "\n",
    "        for p in np.arange(0,1):\n",
    "        #for p in np.arange(len(ploidies)):\n",
    "            for e in np.arange(1,2):\n",
    "            #for e in np.arange(len(envts)):\n",
    "\n",
    "                #print(ploidies[p]+'_'+envts[e])\n",
    "\n",
    "                # First, import a list of genotypes to predict\n",
    "                data1 = pd.read_csv('20211123_subsets-output/subsets/'+ploidies[p]+'_'+envts[e]+'_subset_'+str(myssnum)+'.txt',\n",
    "                                    sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],\n",
    "                                    skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "                # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "                data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "                # Binary style for genotype\n",
    "                data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "                # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "                # represented multiple times.\n",
    "                glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "                data2 = pd.DataFrame()\n",
    "\n",
    "                for g in np.arange(len(glist)):\n",
    "                    tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                    if len(tempg) == 1:\n",
    "                        data2.at[g,'genotype'] = glist[g]\n",
    "                        data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "                        data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "                    elif len(tempg) > 1:\n",
    "                        data2.at[g,'genotype'] = glist[g]\n",
    "                        data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "                        #my_svar = statistics.variance(tempg['s'])\n",
    "                        my_svar = 0\n",
    "                        mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "                        data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                    data2.at[g,'s_pred'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_Alex prediction']\n",
    "\n",
    "                data1 = data2\n",
    "\n",
    "                # Calculate the difference between observed and predicted, such that s_obs = s_pred + diff\n",
    "                data1['opdiff'] = data1['s'] - data1['s_pred']\n",
    "\n",
    "                # Create a column for each locus\n",
    "                for l in np.arange(len(floci)):\n",
    "                    data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "                    data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "\n",
    "                locihere = []\n",
    "                for l in np.arange(len(floci)):\n",
    "                    if len(data1.loc[data1[floci[l]] == 1]) > 0:\n",
    "                        locihere = locihere + [floci[l]]\n",
    "                    \n",
    "                \n",
    "                # Now import a set of coefficients\n",
    "                data0 = pd.read_csv('20211123_subsets-output/subsets/'+ploidies[p]+'_'+envts[e]+'_subset_'+str(myssnum)+'.txt',\n",
    "                                                     sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_term','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "                data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "                data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(len(locihere))\n",
    "\n",
    "                data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "                for l in np.arange(len(locihere)):\n",
    "                    data0[locihere[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "                    data1['without_'+locihere[l]] = data1.loc[:,'genotype'].str[:floci.index(locihere[l])] + data1.loc[:,'genotype'].str[floci.index(locihere[l])+1:]\n",
    "\n",
    "                data0['numMut'] = data0[locihere].sum(axis=1)\n",
    "\n",
    "                # Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "                for i in np.arange(len(data0)):\n",
    "                    if data0.loc[i,'numMut'] % 2 == 1:\n",
    "                        data0.at[i,'baseline'] = -1\n",
    "                    else:\n",
    "                        data0.at[i,'baseline'] = 1\n",
    "\n",
    "                #for l in np.arange(0,1):\n",
    "                for l in np.arange(len(locihere)):\n",
    "\n",
    "                    # Want to start with \"all-epistasis\" observed fitnesses in data1\n",
    "                    # Get b_obs, TLS_b_obs, TLS_1 (and intercept values)\n",
    "                    tab0 = data1.loc[(data1[locihere[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                    tab0 = tab0[['genotype','s','stderr(s)','without_'+locihere[l]]]\n",
    "                    tab1 = data1.loc[(data1[locihere[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                    tab1 = tab1[['genotype','s','stderr(s)','without_'+locihere[l]]]\n",
    "\n",
    "                    templ = pd.merge(tab0,tab1,how='inner',on='without_'+locihere[l])\n",
    "\n",
    "                    for i in np.arange(len(locihere)):\n",
    "                        templ[locihere[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "                    # York regression to get sum of total least squares deviations, S\n",
    "                    myreg = yorkreg_nocorr(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "                    myreg1 = york_slope1(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "\n",
    "                    #TEMPORARY\n",
    "                    ## plot\n",
    "                    #plt.errorbar(templ['s_x'],templ['s_y'],xerr=templ['stderr(s)_x'],yerr=templ['stderr(s)_y'],ls='none')\n",
    "                    #ys = plt.gca().get_ylim()\n",
    "                    #xs = plt.gca().get_xlim()\n",
    "                    #lower = min(xs[0],ys[0])\n",
    "                    #upper = max(xs[1],ys[1])\n",
    "                    #plt.plot(np.linspace(lower,upper),np.linspace(lower,upper),color='k')\n",
    "                    #plt.plot(np.linspace(lower,upper),np.linspace(lower,upper)*myreg[0]+myreg[1],color='xkcd:cerulean')\n",
    "                    #plt.plot(np.linspace(lower,upper),np.linspace(lower,upper)+myreg1[0],color='xkcd:goldenrod')\n",
    "                    #plt.xlim(upper,lower)\n",
    "                    #plt.ylim(upper,lower)\n",
    "                    #plt.show()\n",
    "\n",
    "                    # Create a subtable - we'll append these together into megaremoverv2\n",
    "                    locusremover = pd.DataFrame()\n",
    "\n",
    "                    # wrap up the below, haven't edited yet...\n",
    "                    locusremover.at[0,'num term add'] = 0\n",
    "                    locusremover.at[0,'term added'] = 'na'\n",
    "                    locusremover.at[0,'term order'] = 'na'\n",
    "                    locusremover.at[0,'coefficient'] = 'na'\n",
    "                    locusremover.at[0,'inferred_b'] = myreg[0]\n",
    "                    locusremover.at[0,'inferred_a'] = myreg[1]\n",
    "                    locusremover.at[0,'inferred_S'] = myreg[3]\n",
    "                    locusremover.at[0,'main_b'] = myreg[0]\n",
    "                    locusremover.at[0,'main_a'] = myreg[1]\n",
    "                    locusremover.at[0,'main_S'] = myreg[3]\n",
    "                    locusremover.at[0,'N'] = len(templ)\n",
    "                    locusremover.at[0,'1_a'] = myreg1[0]\n",
    "                    locusremover.at[0,'1_S'] = myreg1[1]\n",
    "\n",
    "\n",
    "                    # Begin cycles\n",
    "                    # data0adder serves as the sorted databank for (non-zero) coefficients\n",
    "                    data0adder = data0.copy(deep=True).loc[(data0['numMut'] > 1)&(data0[locihere[l]] == 1)]\n",
    "                    data0adder['abs coefficient'] = abs(data0adder[ploidies[p]+'_'+envts[e]+'_term'])\n",
    "                    data0adder = data0adder.sort_values(by='abs coefficient',ascending=False)\n",
    "\n",
    "                    # create a builder dataframe on which changes will be processed\n",
    "                    builder = data0.copy(deep=True)\n",
    "\n",
    "                    for n in np.arange(ncyc):\n",
    "                    #for n in np.arange(0,1):\n",
    "                        # find the top index (i.e., the strongest coefficient)\n",
    "                        if n > len(data0adder)-1:\n",
    "                            break\n",
    "\n",
    "                        topind = data0adder.index[n]\n",
    "\n",
    "                        # if no epistasis, end\n",
    "                        if data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term'] == 0:\n",
    "                            break\n",
    "\n",
    "                        # remove the strongest epistatic coefficient that involves locus l\n",
    "                        builder.at[topind,ploidies[p]+'_'+envts[e]+'_term'] = 0\n",
    "\n",
    "                        # add coefficient info in locusadder\n",
    "                        locusremover.at[n+1,'num term add'] = n+1\n",
    "                        locusremover.at[n+1,'term added'] = data0adder.loc[topind,'genotype'] \n",
    "                        locusremover.at[n+1,'term order'] = data0adder.loc[topind,'numMut']\n",
    "                        locusremover.at[n+1,'coefficient'] = data0adder.loc[topind,ploidies[p]+'_'+envts[e]+'_term']\n",
    "\n",
    "                        # Estimate the new predicted value for each genotype\n",
    "                        for g in np.arange(len(data1)):\n",
    "                            temp = builder.copy(deep=True)\n",
    "                            # to make life faster, remove all zero values\n",
    "                            temp = temp.loc[(temp[ploidies[p]+'_'+envts[e]+'_term'] != 0)].reset_index(drop=True)\n",
    "                            # occasionally, we are in all-zero-epistasis world (edge case), so if that's the case just use all the terms\n",
    "                            if len(temp) == 0:\n",
    "                                temp = builder.copy(deep=True)\n",
    "                            for t in np.arange(len(temp)):\n",
    "                                temprowlist = []\n",
    "                                for locus in np.arange(len(locihere)):\n",
    "                                    temprowlist = temprowlist + [temp.loc[t,locihere[locus]]*data1.loc[g,locihere[locus]]]\n",
    "                                #remove zeros\n",
    "                                temprowlist = [value for value in temprowlist if value != 0]\n",
    "                                # find product\n",
    "                                firstprod = np.prod(temprowlist)\n",
    "                                # subtract out baseline\n",
    "                                firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                                # multiply by term's value\n",
    "                                mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                                temp.at[t,'tosum'] = mytermval\n",
    "                            totalsum = temp['tosum'].sum()\n",
    "                            myintercept = data1.loc[(data1['genotype'] == '0000000000'),'s_pred'].values[0]\n",
    "                            data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "                        data1['new_obs'] = data1['new_pred'] + data1['opdiff']\n",
    "\n",
    "                        # For the new predictions, get the values I want\n",
    "                        tab0 = data1.loc[(data1[locihere[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                        tab0 = tab0[['genotype','new_obs','stderr(s)','without_'+locihere[l]]]\n",
    "                        tab1 = data1.loc[(data1[locihere[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                        tab1 = tab1[['genotype','new_obs','stderr(s)','without_'+locihere[l]]]\n",
    "\n",
    "                        templ = pd.merge(tab0,tab1,how='inner',on='without_'+locihere[l])\n",
    "\n",
    "                        #for i in np.arange(len(locihere)):\n",
    "                        #    templ[locihere[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "                        my_x = templ['new_obs_x']\n",
    "                        my_y = templ['new_obs_y']\n",
    "                        my_x_err = templ['stderr(s)_x']\n",
    "                        my_y_err = templ['stderr(s)_y']\n",
    "\n",
    "                        myregnew = yorkreg_nocorr(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                        locusremover.at[n+1,'inferred_b'] = myregnew[0]\n",
    "                        locusremover.at[n+1,'inferred_a'] = myregnew[1]\n",
    "                        locusremover.at[n+1,'inferred_S'] = myregnew[3]\n",
    "\n",
    "                        myreg_main = york_slopeanyb(my_x,my_y,my_x_err,my_y_err,yorkn,myreg[0])\n",
    "                        locusremover.at[n+1,'main_b'] = myreg[0]\n",
    "                        locusremover.at[n+1,'main_a'] = myreg_main[0]\n",
    "                        locusremover.at[n+1,'main_S'] = myreg_main[1]\n",
    "\n",
    "                        myreg_1 = york_slope1(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                        locusremover.at[n+1,'1_a'] = myreg_1[0]\n",
    "                        locusremover.at[n+1,'1_S'] = myreg_1[1]\n",
    "\n",
    "                        locusremover.at[n+1,'N'] = len(templ)\n",
    "\n",
    "                        print(abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum())\n",
    "\n",
    "                        #if n >= ncycthresh:\n",
    "                        #    if (abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum()) > ethresh:\n",
    "                        #        break\n",
    "\n",
    "                    locusremover.at[1:,'term added'] = locusremover.loc[1:,'term added'].astype(int).astype(str).str.zfill(len(locihere))\n",
    "                    locusremover.insert(0,'main locus', locihere[l])\n",
    "                    locusremover.insert(0,'envt',envts[e])\n",
    "                    locusremover.insert(0,'ploidy',ploidies[p])\n",
    "\n",
    "                    megaremoverv2 = megaremoverv2.append(locusremover)\n",
    "                    print(locihere[l])\n",
    "        megaremoverv2.insert(0,'subset',myssnum)\n",
    "        megaremoverss = megaremoverss.append(megaremoverv2)\n",
    "        # save as I go so I don't lose if computer crashes\n",
    "        export_csv = megaremoverss.to_csv(r'20210712_megaremoverss_hap_4nqo-ncyc7.csv',index=True,header=True) \n",
    "\n",
    "mystop = time.perf_counter()\n",
    "elapsed = mystop-mystart\n",
    "print(str(elapsed)+' seconds to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same megaremoverss thing as above except take a shortcut: only interested in residuals (endpoint)\n",
    "# Should go much much faster and can give us data on all subsets everywhere\n",
    "\n",
    "# Do the removal analysis now with the subsets data\n",
    "\n",
    "# Want to do ABC vs aBC plots now where, instead of adding terms by rank to form predicted values,\n",
    "# we subtract them by rank to adjust observed values.\n",
    "#time the code\n",
    "\n",
    "THIS ACTUALLY TAKES QUITE A LONG TIME TO RUN\n",
    "\n",
    "mystart = time.perf_counter()\n",
    "\n",
    "megaremoverss = pd.DataFrame()\n",
    "#ss stands for subset\n",
    "\n",
    "o=10\n",
    "#ncyc = 30\n",
    "#ncycthresh = 10\n",
    "#pthresh = 0.01\n",
    "yorkn = 100\n",
    "#diffthresh = 0.01\n",
    "#ethresh = 0.50\n",
    "\n",
    "\n",
    "#for p in np.arange(0,2):\n",
    "for p in np.arange(len(ploidies)):\n",
    "    #for e in np.arange(0,2):\n",
    "    for e in np.arange(len(envts)):\n",
    "        print(ploidies[p]+'_'+envts[e])\n",
    "\n",
    "        #for ss in np.arange(0,2):\n",
    "        for ss in np.arange(len(ssnums)):\n",
    "            megaremoverv2 = pd.DataFrame()\n",
    "            \n",
    "            myssnum = ssnums[ss]\n",
    "            print(str(myssnum))\n",
    "\n",
    "            # First, import a list of genotypes to predict\n",
    "            data1 = pd.read_csv('20211123_subsets-output/subsets/'+ploidies[p]+'_'+envts[e]+'_subset_'+str(myssnum)+'.txt',\n",
    "                                sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],\n",
    "                                skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "            # represented multiple times.\n",
    "            glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "            data2 = pd.DataFrame()\n",
    "\n",
    "            for g in np.arange(len(glist)):\n",
    "                tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                if len(tempg) == 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "                    data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "                elif len(tempg) > 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "                    #my_svar = statistics.variance(tempg['s'])\n",
    "                    my_svar = 0\n",
    "                    mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "                    data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                data2.at[g,'s_pred'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_Alex prediction']\n",
    "\n",
    "            data1 = data2\n",
    "\n",
    "            # Calculate the difference between observed and predicted, such that s_obs = s_pred + diff\n",
    "            data1['opdiff'] = data1['s'] - data1['s_pred']\n",
    "\n",
    "            # Create a column for each locus\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "                data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "\n",
    "            locihere = []\n",
    "            for l in np.arange(len(floci)):\n",
    "                if len(data1.loc[data1[floci[l]] == 1]) > 0:\n",
    "                    locihere = locihere + [floci[l]]\n",
    "\n",
    "            # Now import a set of coefficients\n",
    "            data0 = pd.read_csv('20211123_subsets-output/subsets/'+ploidies[p]+'_'+envts[e]+'_subset_'+str(myssnum)+'.txt',\n",
    "                                                 sep='\\t',names=['todelete','genotype',ploidies[p]+'_'+envts[e]+'_term','na'],skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            data0 = data0.loc[data0.loc[(data0['genotype'].isnull())].index.tolist()[0]+1:,:]\n",
    "\n",
    "            data0['genotype'] = data0['genotype'].astype(int).astype(str).str.zfill(len(locihere))\n",
    "\n",
    "            data0 = data0.drop(columns=['todelete','na']).reset_index(drop=True)\n",
    "\n",
    "            for l in np.arange(len(locihere)):\n",
    "                data0[locihere[l]] = data0.loc[:,'genotype'].str[l].astype(int)\n",
    "                data1['without_'+locihere[l]] = data1.loc[:,'genotype'].str[:floci.index(locihere[l])] + data1.loc[:,'genotype'].str[floci.index(locihere[l])+1:]\n",
    "\n",
    "            data0['numMut'] = data0[locihere].sum(axis=1)\n",
    "\n",
    "            # Add a \"baseline\" column for whether these terms are added or subtracted in the WT\n",
    "            for i in np.arange(len(data0)):\n",
    "                if data0.loc[i,'numMut'] % 2 == 1:\n",
    "                    data0.at[i,'baseline'] = -1\n",
    "                else:\n",
    "                    data0.at[i,'baseline'] = 1\n",
    "\n",
    "            #for l in np.arange(0,1):\n",
    "            for l in np.arange(len(locihere)):\n",
    "\n",
    "                # Want to start with \"all-epistasis\" observed fitnesses in data1\n",
    "                # Get b_obs, TLS_b_obs, TLS_1 (and intercept values)\n",
    "                tab0 = data1.loc[(data1[locihere[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab0 = tab0[['genotype','s','stderr(s)','without_'+locihere[l]]]\n",
    "                tab1 = data1.loc[(data1[locihere[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab1 = tab1[['genotype','s','stderr(s)','without_'+locihere[l]]]\n",
    "\n",
    "                templ = pd.merge(tab0,tab1,how='inner',on='without_'+locihere[l])\n",
    "\n",
    "                for i in np.arange(len(locihere)):\n",
    "                    templ[locihere[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "                # York regression to get sum of total least squares deviations, S\n",
    "                myreg = yorkreg_nocorr(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "                myreg1 = york_slope1(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "\n",
    "                # Create a subtable - we'll append these together into megaremoverv2\n",
    "                locusremover = pd.DataFrame()\n",
    "\n",
    "                # wrap up the below, haven't edited yet...\n",
    "                locusremover.at[0,'num term add'] = 0\n",
    "                locusremover.at[0,'term added'] = 'na'\n",
    "                locusremover.at[0,'term order'] = 'na'\n",
    "                locusremover.at[0,'coefficient'] = 'na'\n",
    "                locusremover.at[0,'inferred_b'] = myreg[0]\n",
    "                locusremover.at[0,'inferred_a'] = myreg[1]\n",
    "                locusremover.at[0,'inferred_S'] = myreg[3]\n",
    "                locusremover.at[0,'main_b'] = myreg[0]\n",
    "                locusremover.at[0,'main_a'] = myreg[1]\n",
    "                locusremover.at[0,'main_S'] = myreg[3]\n",
    "                locusremover.at[0,'N'] = len(templ)\n",
    "                locusremover.at[0,'1_a'] = myreg1[0]\n",
    "                locusremover.at[0,'1_S'] = myreg1[1]\n",
    "\n",
    "                # Now remove all terms\n",
    "                builder = data0.copy(deep=True)\n",
    "                builder.at[builder.loc[(builder['numMut'] > 1)&(builder[locihere[l]] == 1)].index,ploidies[p]+'_'+envts[e]+'_term'] = 0\n",
    "\n",
    "\n",
    "                # add coefficient info in locusadder\n",
    "                locusremover.at[1,'num term add'] = len(data0.loc[(data0['numMut'] > 1)&(data0[locihere[l]] == 1)])\n",
    "                locusremover.at[1,'term added'] = 'all' \n",
    "                locusremover.at[1,'term order'] = 'na'\n",
    "                locusremover.at[1,'coefficient'] = 'na'\n",
    "\n",
    "                # Estimate the new predicted value for each genotype\n",
    "                for g in np.arange(len(data1)):\n",
    "                    temp = builder.copy(deep=True)\n",
    "                    for t in np.arange(len(temp)):\n",
    "                        temprowlist = []\n",
    "                        for locus in np.arange(len(locihere)):\n",
    "                            temprowlist = temprowlist + [temp.loc[t,locihere[locus]]*data1.loc[g,locihere[locus]]]\n",
    "                        #remove zeros\n",
    "                        temprowlist = [value for value in temprowlist if value != 0]\n",
    "                        # find product\n",
    "                        firstprod = np.prod(temprowlist)\n",
    "                        # subtract out baseline\n",
    "                        firstprod_lessbaseline = firstprod - temp.loc[t,'baseline']\n",
    "                        # multiply by term's value\n",
    "                        mytermval = firstprod_lessbaseline * temp.loc[t,ploidies[p]+'_'+envts[e]+'_term']\n",
    "                        temp.at[t,'tosum'] = mytermval\n",
    "                    totalsum = temp['tosum'].sum()\n",
    "                    myintercept = data1.loc[(data1['genotype'] == '0000000000'),'s_pred'].values[0]\n",
    "                    data1.at[g,'new_pred'] = totalsum + myintercept\n",
    "                data1['new_obs'] = data1['new_pred'] + data1['opdiff']\n",
    "\n",
    "                # For the new predictions, get the values I want\n",
    "                tab0 = data1.loc[(data1[locihere[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab0 = tab0[['genotype','new_obs','stderr(s)','without_'+locihere[l]]]\n",
    "                tab1 = data1.loc[(data1[locihere[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab1 = tab1[['genotype','new_obs','stderr(s)','without_'+locihere[l]]]\n",
    "\n",
    "                templ = pd.merge(tab0,tab1,how='inner',on='without_'+locihere[l])\n",
    "\n",
    "                my_x = templ['new_obs_x']\n",
    "                my_y = templ['new_obs_y']\n",
    "                my_x_err = templ['stderr(s)_x']\n",
    "                my_y_err = templ['stderr(s)_y']\n",
    "\n",
    "                myregnew = yorkreg_nocorr(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                locusremover.at[1,'inferred_b'] = myregnew[0]\n",
    "                locusremover.at[1,'inferred_a'] = myregnew[1]\n",
    "                locusremover.at[1,'inferred_S'] = myregnew[3]\n",
    "\n",
    "                myreg_main = york_slopeanyb(my_x,my_y,my_x_err,my_y_err,yorkn,myreg[0])\n",
    "                locusremover.at[1,'main_b'] = myreg[0]\n",
    "                locusremover.at[1,'main_a'] = myreg_main[0]\n",
    "                locusremover.at[1,'main_S'] = myreg_main[1]\n",
    "\n",
    "                myreg_1 = york_slope1(my_x,my_y,my_x_err,my_y_err,yorkn)\n",
    "                locusremover.at[1,'1_a'] = myreg_1[0]\n",
    "                locusremover.at[1,'1_S'] = myreg_1[1]\n",
    "\n",
    "                locusremover.at[1,'N'] = len(templ)\n",
    "\n",
    "                #print(abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum())\n",
    "\n",
    "                #if n >= ncycthresh:\n",
    "                #    if (abs(locusremover.loc[1:,'coefficient']).sum()/data0adder['abs coefficient'].sum()) > ethresh:\n",
    "                #        break\n",
    "\n",
    "                locusremover.at[1:,'term added'] = 'na'\n",
    "                locusremover.insert(0,'main locus', locihere[l])\n",
    "                locusremover.insert(0,'envt',envts[e])\n",
    "                locusremover.insert(0,'ploidy',ploidies[p])\n",
    "\n",
    "                megaremoverv2 = megaremoverv2.append(locusremover)\n",
    "                print(locihere[l])\n",
    "            megaremoverv2.insert(0,'subset',myssnum)\n",
    "            megaremoverss = megaremoverss.append(megaremoverv2)\n",
    "                \n",
    "        #export_csv = megaremoverss.to_csv(r'20211218_megaremoverss_allremoved-checkpoint'+ploidies[p]+'_'+envts[e]+'.csv',index=True,header=True) \n",
    "\n",
    "mystop = time.perf_counter()\n",
    "elapsed = mystop-mystart\n",
    "print(str(elapsed)+' seconds to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting things properly, subplots style, for publication\n",
    "\n",
    "# Make the plot of subsets for the SI\n",
    "\n",
    "fct_thresh = 0.9\n",
    "\n",
    "ssbank = pd.read_csv('20211218_megaremoverss_allremoved-hap_37C-4NQO.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# I want to know which mutations and are or are not FCTs for plotting purposes\n",
    "getalls = pd.read_csv('20210712_megaremoverv2_haphom.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "totalbank = pd.DataFrame()\n",
    "\n",
    "for p in np.arange(0,1):\n",
    "    for e in np.arange(0,2):\n",
    "        temp = getalls.loc[(getalls['ploidy'] == ploidies[p])&(getalls['envt'] == envts[e])&(getalls['num term add'] == 0)]\n",
    "        totalbank = totalbank.append(temp)\n",
    "\n",
    "totalbank = totalbank.reset_index(drop=True)\n",
    "\n",
    "for i in np.arange(len(totalbank)):\n",
    "    if totalbank.loc[i,'inferred_b'] <= 1:\n",
    "        totalbank.at[i,'inferred_b_-1to1'] = totalbank.loc[i,'inferred_b']\n",
    "    else:\n",
    "        totalbank.at[i,'inferred_b_-1to1'] = 1/totalbank.loc[i,'inferred_b']\n",
    "\n",
    "\n",
    "\n",
    "# add the num_mut column\n",
    "nm = []\n",
    "for i in np.arange(len(ssnums)):\n",
    "    nm = nm + [len(ssbank.loc[ssbank['subset'] == ssnums[i]])/4]\n",
    "nmadder = pd.DataFrame()\n",
    "nmadder['subset'] = ssnums\n",
    "nmadder['num_mut'] = nm\n",
    "\n",
    "ssbank = pd.merge(ssbank,nmadder,on='subset',how='left')\n",
    "    \n",
    "\n",
    "for p in np.arange(0,1):\n",
    "    for e in np.arange(0,2):\n",
    "\n",
    "        mr2ss = ssbank.copy(deep=True).loc[ssbank['envt'] == envts[e]].reset_index(drop=True)\n",
    "        \n",
    "        # Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "        # This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "        for i in np.arange(len(mr2ss)):\n",
    "            if mr2ss.loc[i,'main_b'] <= 1:\n",
    "                mr2ss.at[i,'main_b_-1to1'] = mr2ss.loc[i,'main_b']\n",
    "            else:\n",
    "                mr2ss.at[i,'main_b_-1to1'] = 1/mr2ss.loc[i,'main_b']\n",
    "\n",
    "        for i in np.arange(len(mr2ss)):\n",
    "            if mr2ss.loc[i,'inferred_b'] <= 1:\n",
    "                mr2ss.at[i,'inferred_b_-1to1'] = mr2ss.loc[i,'inferred_b']\n",
    "            else:\n",
    "                mr2ss.at[i,'inferred_b_-1to1'] = 1/mr2ss.loc[i,'inferred_b']\n",
    "\n",
    "        # Take ratios of the S values\n",
    "        mr2ss['main/inferred'] = mr2ss['main_S']/mr2ss['inferred_S']\n",
    "        mr2ss['1/inferred'] = mr2ss['1_S']/mr2ss['inferred_S']\n",
    "        mr2ss['1/main'] = mr2ss['1_S']/mr2ss['main_S']\n",
    "        \n",
    "        # Create table to analyze decipherability\n",
    "\n",
    "        detabss = pd.DataFrame()\n",
    "\n",
    "        # first, start with all combinations of subsets and loci\n",
    "        ssl = []\n",
    "        for i in np.arange(len(mr2ss)):\n",
    "            ssl = ssl + [[mr2ss.loc[i,'subset'],mr2ss.loc[i,'main locus']]]\n",
    "        ssl2 = []\n",
    "        [ssl2.append(x) for x in ssl if x not in ssl2]\n",
    "        ssl = ssl2\n",
    "\n",
    "        myind = 0\n",
    "        for i in np.arange(len(ssl)):\n",
    "            temp = mr2ss.loc[(mr2ss['subset'] == ssl[i][0])&(mr2ss['main locus'] == ssl[i][1])].reset_index(drop=True)\n",
    "\n",
    "            detabss.at[myind,'subset'] = ssl[i][0]\n",
    "            detabss.at[myind,'ploidy'] = temp.loc[0,'ploidy']\n",
    "            detabss.at[myind,'envt'] =  temp.loc[0,'envt']\n",
    "            detabss.at[myind,'main locus'] = ssl[i][1]\n",
    "\n",
    "            detabss.at[myind,'N'] = temp.loc[0,'N']\n",
    "            detabss.at[myind,'num_mut'] = temp.loc[0,'num_mut']\n",
    "            detabss.at[myind,'inferred_b_-1to1_original'] = temp.loc[0,'inferred_b_-1to1']\n",
    "\n",
    "            if len(temp) == 1:\n",
    "                this1main = temp.loc[0,'1/main']\n",
    "               # thisnumtermadd = temp.loc[0,'num term add']\n",
    "                thisfinalslope = temp.loc[0,'inferred_b_-1to1']\n",
    "\n",
    "            else:\n",
    "                #mymaxind = temp.iloc[-1]['num term add']\n",
    "                this1main = temp.loc[1,'1/main']\n",
    "                #thisnumtermadd = mymaxind\n",
    "                thisfinalslope = temp.loc[1,'inferred_b_-1to1']\n",
    "\n",
    "\n",
    "            detabss.at[myind,'1/main_final'] = this1main\n",
    "            #detabss.at[myind,'num term add'] = thisnumtermadd\n",
    "            detabss.at[myind,'inferred_b_-1to1_final'] = thisfinalslope\n",
    "\n",
    "\n",
    "            myind = myind+1\n",
    "\n",
    "        # export\n",
    "        export_csv = detabss.to_csv(r'20211211_hap_'+envts[e]+'_detabss_allrem.csv',index=True,header=True)\n",
    "        #export\n",
    "        #export_csv = detabssfc.to_csv(r'20211211_hap_37C_detabssfc.csv',index=True,header=True)\n",
    "\n",
    "\n",
    "        #now that we have the table, we can do some analysis\n",
    "        myind=0\n",
    "        mytrack = pd.DataFrame()\n",
    "        temp = detabss.loc[(detabss['N'] > 2)&(detabss['inferred_b_-1to1_original'] <= fct_thresh)].copy(deep=True)\n",
    "        for l in np.arange(len(floci)):\n",
    "            for n in np.arange(3,11):\n",
    "                templn = temp.loc[(temp['main locus'] == floci[l])&(temp['num_mut'] == n)].copy(deep=True)\n",
    "                count1mainlessthan1 = len(templn.loc[templn['1/main_final'] <= 1])\n",
    "                countall = len(templn)\n",
    "                mytrack.at[myind,'ploidy'] = ploidies[p]\n",
    "                mytrack.at[myind,'envt'] = envts[e]\n",
    "                mytrack.at[myind,'main locus'] = floci[l]\n",
    "                mytrack.at[myind,'num_mut'] = n\n",
    "                mytrack.at[myind,'count total'] = countall\n",
    "                mytrack.at[myind,'count 1/main <= 1'] = count1mainlessthan1\n",
    "                if len(templn) == 0:\n",
    "                    mytrack.at[myind,'fraction <= 1'] = np.nan\n",
    "                else:\n",
    "                    mytrack.at[myind,'fraction <= 1'] = count1mainlessthan1 / countall\n",
    "                myind=myind+1\n",
    "                \n",
    "        # SET UP FIGURE\n",
    "        fig,ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False,figsize=(5,2),constrained_layout=True)   \n",
    "        \n",
    "        fctloci = list(totalbank.copy(deep=True).loc[(totalbank['envt'] == envts[e])&(totalbank['inferred_b_-1to1'] <= fct_thresh)]['main locus'])               \n",
    "        # now plot the results, just for guys that are FCTs\n",
    "        for l in np.arange(len(fctloci)):\n",
    "            templ = mytrack.loc[(mytrack['ploidy'] == ploidies[p])&(mytrack['envt'] == envts[e])&(mytrack['main locus'] == fctloci[l])]\n",
    "            ax[1].plot(np.arange(3,11),templ['fraction <= 1'])\n",
    "        ax[1].set_xlabel('subset size')\n",
    "        #ax[1].set_ylabel('fraction where 1/main <= 1 after removal')\n",
    "        #ax[1].set_ylabel('\\n'.join(wrap('Fraction relative fit ratio, SSE$_{b=1}$ / SSE$_{b=global}$ ≤ 1 when all epistasis removed',30)),labelpad=2)\n",
    "        ax[1].set_ylabel('\\n'.join(wrap('Fraction subsets with SSE$_{b=1}$/SSE$_{b=global}$ ≤ 1, all epistasis removed',36)),labelpad=2)\n",
    "        #plt.title(ploidies[p]+'_'+envts[e])\n",
    "        #ax[1].legend(fctloci)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "        # how many go to 1/main <= 1 with fullco vs non-fullco version of the removal analysis?\n",
    "        # actually, don't do this analysis now, not top priority\n",
    "        \n",
    "        # offset loci, for visibility\n",
    "        loff = [-0.2,-0.1,0,0.1,0.2]\n",
    "\n",
    "        # actual next step - look at 1/main final instead of fraction ≤ 1\n",
    "        temp = detabss.loc[(detabss['N'] > 2)&(detabss['inferred_b_-1to1_original'] <= fct_thresh)].copy(deep=True)\n",
    "        for l in np.arange(len(fctloci)):\n",
    "            templ = temp.loc[(temp['main locus'] == fctloci[l])]\n",
    "            ax[0].scatter(templ['num_mut']+loff[l],templ['1/main_final'],alpha=0.7,s=2)\n",
    "            #myreg = linregress(templ['num_mut'],templ['1/main_final'])\n",
    "            #plt.plot(np.arange(3,10),np.arange(3,10)*myreg.slope+myreg.intercept)\n",
    "            mymeds = []\n",
    "            for n in np.arange(3,11):\n",
    "                templn = templ.loc[(templ['num_mut'] == n)]\n",
    "                mymeds = mymeds + [np.median(templn['1/main_final'])]\n",
    "            ax[0].plot(np.arange(3,11)+loff[l],mymeds)\n",
    "        ax[0].set_xlabel('subset size')\n",
    "        #ax[0].set_ylabel('\\n'.join(wrap('Relative fit ratio, SSE$_{b=1}$ / SSE$_{b=global}$, all epistasis removed',30)),labelpad=2)\n",
    "        ax[0].set_ylabel('\\n'.join(wrap('Final SSE$_{b=1}$ / SSE$_{b=global}$, all epistasis removed',40)),labelpad=2)\n",
    "        #ax[0].set_ylabel('final SSE b=1 / b=global')\n",
    "        # do custom y scales\n",
    "        if envts[e] == '37C':\n",
    "            ax[0].set_ylim(-0.5,10) #cuts off some points\n",
    "        else:\n",
    "            ax[0].set_ylim(-0.5,15) #cuts off some points\n",
    "        #plt.ylim(-0.5,10) #I know this cuts off some points\n",
    "        #plt.title(ploidies[p]+'_'+envts[e])\n",
    "        leg = ax[0].legend(fctloci)\n",
    "        leg._legend_box.align = \"left\"\n",
    "        leg.set_title('Locus')\n",
    "        ax[0].axhline(y=1,lw=0.5,color='k',zorder=0)\n",
    "        \n",
    "        #plt.savefig(\"msfigs/SIfigs/residual-ratio_\"+ploidies[p]+'_'+envts[e]+\".pdf\",bbox_inches='tight',dpi=300)\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just do the dumb thing of getting all the global slopes for all the loci in all the subsets\n",
    "\n",
    "bglobalss = pd.DataFrame()\n",
    "\n",
    "o=10\n",
    "\n",
    "yorkn = 100\n",
    "\n",
    "myind=0\n",
    "\n",
    "for ss in np.arange(len(ssnums)):\n",
    "#for ss in np.arange(0,1):\n",
    "    myssnum = ssnums[ss]\n",
    "    \n",
    "    print(str(myssnum))\n",
    "\n",
    "    for p in np.arange(0,1):\n",
    "    #for p in np.arange(len(ploidies)):\n",
    "        for e in np.array([1,2,4]):\n",
    "        #for e in np.arange(len(envts)):\n",
    "\n",
    "            #print(ploidies[p]+'_'+envts[e])\n",
    "\n",
    "            # First, import a list of genotypes to predict\n",
    "            data1 = pd.read_csv('20211123_subsets-output/subsets/'+ploidies[p]+'_'+envts[e]+'_subset_'+str(myssnum)+'.txt',\n",
    "                                sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],\n",
    "                                skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Average genotypes, propagating error. Reason for this is to get rid of weird artifacts from having same genotype\n",
    "            # represented multiple times.\n",
    "            glist = list(OrderedDict.fromkeys(data1['genotype']))\n",
    "\n",
    "            data2 = pd.DataFrame()\n",
    "\n",
    "            for g in np.arange(len(glist)):\n",
    "                tempg = data1.loc[(data1['genotype'] == glist[g])].reset_index(drop=True)\n",
    "                if len(tempg) == 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs']\n",
    "                    data2.at[g,'stderr(s)'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_s-obs-err']\n",
    "                elif len(tempg) > 1:\n",
    "                    data2.at[g,'genotype'] = glist[g]\n",
    "                    data2.at[g,'s'] = tempg[ploidies[p]+'_'+envts[e]+'_s-obs'].mean()\n",
    "                    #my_svar = statistics.variance(tempg['s'])\n",
    "                    my_svar = 0\n",
    "                    mymean_stderr = np.mean(tempg[ploidies[p]+'_'+envts[e]+'_s-obs-err']**2)\n",
    "                    data2.at[g,'stderr(s)'] = np.sqrt(my_svar+mymean_stderr)\n",
    "                data2.at[g,'s_pred'] = tempg.loc[0,ploidies[p]+'_'+envts[e]+'_Alex prediction']\n",
    "\n",
    "            data1 = data2\n",
    "\n",
    "            ## Calculate the difference between observed and predicted, such that s_obs = s_pred + diff\n",
    "            #data1['opdiff'] = data1['s'] - data1['s_pred']\n",
    "\n",
    "            # Create a column for each locus\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "                data1.loc[(data1[floci[l]] == 0),floci[l]] = -1\n",
    "\n",
    "            locihere = []\n",
    "            for l in np.arange(len(floci)):\n",
    "                if len(data1.loc[data1[floci[l]] == 1]) > 0:\n",
    "                    locihere = locihere + [floci[l]]\n",
    "\n",
    "            for l in np.arange(len(locihere)):\n",
    "                    data1['without_'+locihere[l]] = data1.loc[:,'genotype'].str[:floci.index(locihere[l])] + data1.loc[:,'genotype'].str[floci.index(locihere[l])+1:]\n",
    "            \n",
    "            #for l in np.arange(0,1):\n",
    "            for l in np.arange(len(locihere)):\n",
    "\n",
    "                # Want to start with \"all-epistasis\" observed fitnesses in data1\n",
    "                # Get b_obs, TLS_b_obs, TLS_1 (and intercept values)\n",
    "                tab0 = data1.loc[(data1[locihere[l]] == -1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab0 = tab0[['genotype','s','stderr(s)','without_'+locihere[l]]]\n",
    "                tab1 = data1.loc[(data1[locihere[l]] == 1)].copy(deep=True).reset_index(drop=True)\n",
    "                tab1 = tab1[['genotype','s','stderr(s)','without_'+locihere[l]]]\n",
    "\n",
    "                templ = pd.merge(tab0,tab1,how='inner',on='without_'+locihere[l])\n",
    "\n",
    "                for i in np.arange(len(locihere)):\n",
    "                    templ[locihere[i]] = templ.loc[:,'genotype_x'].str[i].astype(int)\n",
    "\n",
    "                # York regression to get sum of total least squares deviations, S\n",
    "                myreg = yorkreg_nocorr(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "                myreg1 = york_slope1(templ['s_x'],templ['s_y'],templ['stderr(s)_x'],templ['stderr(s)_y'],yorkn)\n",
    "\n",
    "                #TEMPORARY\n",
    "                ## plot\n",
    "                #plt.errorbar(templ['s_x'],templ['s_y'],xerr=templ['stderr(s)_x'],yerr=templ['stderr(s)_y'],ls='none')\n",
    "                #ys = plt.gca().get_ylim()\n",
    "                #xs = plt.gca().get_xlim()\n",
    "                #lower = min(xs[0],ys[0])\n",
    "                #upper = max(xs[1],ys[1])\n",
    "                #plt.plot(np.linspace(lower,upper),np.linspace(lower,upper),color='k')\n",
    "                #plt.plot(np.linspace(lower,upper),np.linspace(lower,upper)*myreg[0]+myreg[1],color='xkcd:cerulean')\n",
    "                #plt.plot(np.linspace(lower,upper),np.linspace(lower,upper)+myreg1[0],color='xkcd:goldenrod')\n",
    "                #plt.xlim(upper,lower)\n",
    "                #plt.ylim(upper,lower)\n",
    "                #plt.show()\n",
    "\n",
    "                bglobalss.at[myind,'subset'] = myssnum\n",
    "                bglobalss.at[myind,'num_mut'] = len(locihere)\n",
    "                bglobalss.at[myind,'ploidy'] = ploidies[p]\n",
    "                bglobalss.at[myind,'envt'] = envts[e]\n",
    "                bglobalss.at[myind,'main locus'] = locihere[l]\n",
    "                #bglobalss.at[myind,'num term add'] = 0\n",
    "                #bglobalss.at[myind,'term added'] = 'na'\n",
    "                #bglobalss.at[myind,'term order'] = 'na'\n",
    "                #bglobalss.at[myind,'coefficient'] = 'na'\n",
    "                bglobalss.at[myind,'inferred_b'] = myreg[0]\n",
    "                bglobalss.at[myind,'inferred_a'] = myreg[1]\n",
    "                bglobalss.at[myind,'inferred_S'] = myreg[3]\n",
    "                #bglobalss.at[myind,'main_b'] = myreg[0]\n",
    "                #bglobalss.at[myind,'main_a'] = myreg[1]\n",
    "                #bglobalss.at[myind,'main_S'] = myreg[3]\n",
    "                bglobalss.at[myind,'N'] = len(templ)\n",
    "                bglobalss.at[myind,'1_a'] = myreg1[0]\n",
    "                bglobalss.at[myind,'1_S'] = myreg1[1]\n",
    "                \n",
    "                myind = myind+1\n",
    "                \n",
    "#export_csv = bglobalss.to_csv(r'20211207_subsets_bglobal-values_hap_3envs.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bglobalss = pd.read_csv('20211207_subsets_bglobal-values_hap_37C.csv')\n",
    "bglobalss = bglobalss.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "p=0\n",
    "e=0\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(bglobalss)):\n",
    "    if bglobalss.loc[i,'inferred_b'] <= 1:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = bglobalss.loc[i,'inferred_b']\n",
    "    else:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = 1/bglobalss.loc[i,'inferred_b']\n",
    "    \n",
    "for i in np.arange(len(bglobalss)):\n",
    "    if bglobalss.loc[i,'inferred_b'] <= 1:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = bglobalss.loc[i,'inferred_b']\n",
    "    else:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = 1/bglobalss.loc[i,'inferred_b']\n",
    "\n",
    "for l in np.arange(len(floci)):\n",
    "#for l in np.arange(0,1):\n",
    "    fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "    means = []\n",
    "    stds = []\n",
    "    for n in np.arange(3,10):\n",
    "        tempss = bglobalss.loc[(bglobalss['main locus'] == floci[l])&(bglobalss['num_mut'] == n)]\n",
    "        ax.scatter([n]*len(tempss),tempss['inferred_b'])\n",
    "        means = means + [tempss['inferred_b'].mean()]\n",
    "        stds = stds + [tempss['inferred_b'].std()]\n",
    "    ax.plot(np.arange(3,10),means,color='k')\n",
    "    ax.plot(np.arange(3,10),np.add(means,stds),color='xkcd:grey')\n",
    "    ax.plot(np.arange(3,10),np.subtract(means,stds),color='xkcd:grey')\n",
    "    ax.set_xlabel('subset size')\n",
    "    ax.set_ylabel('global b')\n",
    "    ax.axhline(y=1,zorder=0,color='xkcd:goldenrod',lw=0.5)\n",
    "    fig.text(0,1,ploidies[p]+'_'+envts[e]+'_'+floci[l])\n",
    "    plt.show()\n",
    "#this is how it looks with all the data\n",
    "\n",
    "\n",
    "# Do same plots, except for a random set of 9 subsets across the board\n",
    "for l in np.arange(len(floci)):\n",
    "    fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "    means = []\n",
    "    stds = []\n",
    "    for n in np.arange(3,10):\n",
    "        temp = bglobalss.loc[(bglobalss['num_mut'] == n)&(bglobalss['main locus'] == floci[l])].copy(deep=True)\n",
    "        tempss = list(OrderedDict.fromkeys(list(temp['subset'])))\n",
    "        my9 = sample(tempss,9)\n",
    "        tempss = temp[temp['subset'].isin(my9)]\n",
    "        ax.scatter([n]*len(tempss),tempss['inferred_b'])\n",
    "        means = means + [tempss['inferred_b'].mean()]\n",
    "        stds = stds + [tempss['inferred_b'].std()]\n",
    "        print(my9)\n",
    "    ax.plot(np.arange(3,10),means,color='k')\n",
    "    ax.plot(np.arange(3,10),np.add(means,stds),color='xkcd:grey')\n",
    "    ax.plot(np.arange(3,10),np.subtract(means,stds),color='xkcd:grey')\n",
    "    ax.set_xlabel('subset size')\n",
    "    ax.set_ylabel('global b')\n",
    "    ax.axhline(y=1,zorder=0,color='xkcd:goldenrod')\n",
    "    fig.text(0,1,ploidies[p]+'_'+envts[e]+'_'+floci[l])\n",
    "    plt.show()\n",
    "\n",
    "# Now we want to see what the standard deviations are like across the board\n",
    "iterations = 50\n",
    "\n",
    "mlist = []\n",
    "slist = []\n",
    "for n in np.arange(3,10):\n",
    "    mlist = mlist + ['mean_'+str(n)]\n",
    "    slist = slist + ['std_'+str(n)]\n",
    "clist = mlist + slist\n",
    "\n",
    "locusbank = pd.DataFrame()\n",
    "\n",
    "\n",
    "for l in np.arange(len(floci)):\n",
    "    stdbank = pd.DataFrame(columns=clist)\n",
    "    for i in np.arange(iterations):\n",
    "        means = []\n",
    "        stds = []\n",
    "        for n in np.arange(3,10):\n",
    "            temp = bglobalss.loc[(bglobalss['num_mut'] == n)&(bglobalss['main locus'] == floci[l])].copy(deep=True)\n",
    "            tempss = list(OrderedDict.fromkeys(list(temp['subset'])))\n",
    "            my9 = sample(tempss,9)\n",
    "            tempss = temp[temp['subset'].isin(my9)]\n",
    "            ax.scatter([n]*len(tempss),tempss['inferred_b'])\n",
    "            means = means + [tempss['inferred_b'].mean()]\n",
    "            stds = stds + [tempss['inferred_b'].std()]\n",
    "        appenders = means+stds\n",
    "        appendage = pd.Series(appenders,index=stdbank.columns)\n",
    "        stdbank = stdbank.append(appendage, ignore_index=True)\n",
    "\n",
    "    stdbankjuststds = stdbank[slist].copy(deep=True)\n",
    "    mystds = stdbankjuststds.mean()\n",
    "    locusbank = locusbank.append(mystds,ignore_index=True)\n",
    "\n",
    "locusbank.insert(0,'locus',floci)\n",
    "\n",
    "# plot average standard deviations for all loci, across all subset numbers\n",
    "for l in np.arange(len(floci)):\n",
    "    plt.plot(np.arange(3,10),locusbank.iloc[l,1:])\n",
    "    plt.xlabel('subset size')\n",
    "    plt.ylabel('average standard deviation for N = 9')\n",
    "plt.ylim(-0.5,2)\n",
    "plt.legend(floci)\n",
    "plt.show()\n",
    "\n",
    "# do same plot but just for neutral loci\n",
    "myneutrals = ['FAS1','NCS2','SCH9']\n",
    "\n",
    "for l in np.arange(len(floci)):\n",
    "    if floci[l] in myneutrals:\n",
    "        plt.plot(np.arange(3,10),locusbank.iloc[l,1:])\n",
    "        plt.xlabel('subset size')\n",
    "        plt.ylabel('average standard deviation for N = 9')\n",
    "plt.legend(myneutrals)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# do same plot but just for neutral loci\n",
    "myneutrals = ['FAS1','NCS2','PMA1','SCH9','RPI1','AKL1']\n",
    "\n",
    "for l in np.arange(len(floci)):\n",
    "    if floci[l] in myneutrals:\n",
    "        plt.plot(np.arange(3,10),locusbank.iloc[l,1:])\n",
    "        plt.xlabel('subset size')\n",
    "        plt.ylabel('average standard deviation for N = 9')\n",
    "plt.legend(myneutrals)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bglobalss = pd.read_csv('20211207_subsets_bglobal-values_hap_3envs.csv')\n",
    "bglobalss = bglobalss.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "p = 0\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(bglobalss)):\n",
    "    if bglobalss.loc[i,'inferred_b'] <= 1:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = bglobalss.loc[i,'inferred_b']\n",
    "    else:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = 1/bglobalss.loc[i,'inferred_b']\n",
    "    \n",
    "for i in np.arange(len(bglobalss)):\n",
    "    if bglobalss.loc[i,'inferred_b'] <= 1:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = bglobalss.loc[i,'inferred_b']\n",
    "    else:\n",
    "        bglobalss.at[i,'inferred_b_-1to1'] = 1/bglobalss.loc[i,'inferred_b']\n",
    "\n",
    "for e in np.array([1,2,4]):\n",
    "    for l in np.arange(len(floci)):\n",
    "    #for l in np.arange(0,1):\n",
    "        fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for n in np.arange(3,10):\n",
    "            tempss = bglobalss.loc[(bglobalss['main locus'] == floci[l])&(bglobalss['num_mut'] == n)&(bglobalss['envt'] == envts[e])]\n",
    "            ax.scatter([n]*len(tempss),tempss['inferred_b'])\n",
    "            means = means + [tempss['inferred_b'].mean()]\n",
    "            stds = stds + [tempss['inferred_b'].std()]\n",
    "        ax.plot(np.arange(3,10),means,color='k')\n",
    "        ax.plot(np.arange(3,10),np.add(means,stds),color='xkcd:grey')\n",
    "        ax.plot(np.arange(3,10),np.subtract(means,stds),color='xkcd:grey')\n",
    "        ax.set_xlabel('subset size')\n",
    "        ax.set_ylabel('global b')\n",
    "        ax.axhline(y=1,zorder=0,color='xkcd:goldenrod',lw=0.5)\n",
    "        fig.text(0,1,ploidies[p]+'_'+envts[e]+'_'+floci[l])\n",
    "        plt.show()\n",
    "    #this is how it looks with all the data\n",
    "\n",
    "\n",
    "# Do same plots, except for a random set of 9 subsets across the board\n",
    "\n",
    "for e in np.array([1,2,4]):\n",
    "    for l in np.arange(len(floci)):\n",
    "        fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for n in np.arange(3,10):\n",
    "            temp = bglobalss.loc[(bglobalss['num_mut'] == n)&(bglobalss['main locus'] == floci[l])&(bglobalss['envt'] == envts[e])].copy(deep=True)\n",
    "            tempss = list(OrderedDict.fromkeys(list(temp['subset'])))\n",
    "            my9 = sample(tempss,9)\n",
    "            tempss = temp[temp['subset'].isin(my9)]\n",
    "            ax.scatter([n]*len(tempss),tempss['inferred_b'])\n",
    "            means = means + [tempss['inferred_b'].mean()]\n",
    "            stds = stds + [tempss['inferred_b'].std()]\n",
    "        ax.plot(np.arange(3,10),means,color='k')\n",
    "        ax.plot(np.arange(3,10),np.add(means,stds),color='xkcd:grey')\n",
    "        ax.plot(np.arange(3,10),np.subtract(means,stds),color='xkcd:grey')\n",
    "        ax.set_xlabel('subset size')\n",
    "        ax.set_ylabel('global b')\n",
    "        ax.axhline(y=1,zorder=0,color='xkcd:goldenrod')\n",
    "        fig.text(0,1,ploidies[p]+'_'+envts[e]+'_'+floci[l])\n",
    "        plt.show()\n",
    "\n",
    "# Now we want to see what the standard deviations are like across the board\n",
    "iterations = 50\n",
    "\n",
    "mlist = []\n",
    "slist = []\n",
    "for n in np.arange(3,10):\n",
    "    mlist = mlist + ['mean_'+str(n)]\n",
    "    slist = slist + ['std_'+str(n)]\n",
    "clist = mlist + slist\n",
    "\n",
    "\n",
    "for e in np.array([1,2,4]):\n",
    "    locusbank = pd.DataFrame()\n",
    "    for l in np.arange(len(floci)):\n",
    "        stdbank = pd.DataFrame(columns=clist)\n",
    "        for i in np.arange(iterations):\n",
    "            means = []\n",
    "            stds = []\n",
    "            for n in np.arange(3,10):\n",
    "                temp = bglobalss.loc[(bglobalss['num_mut'] == n)&(bglobalss['main locus'] == floci[l])&(bglobalss['envt'] == envts[e])].copy(deep=True)\n",
    "                tempss = list(OrderedDict.fromkeys(list(temp['subset'])))\n",
    "                my9 = sample(tempss,9)\n",
    "                tempss = temp[temp['subset'].isin(my9)]\n",
    "                ax.scatter([n]*len(tempss),tempss['inferred_b'])\n",
    "                means = means + [tempss['inferred_b'].mean()]\n",
    "                stds = stds + [tempss['inferred_b'].std()]\n",
    "            appenders = means+stds\n",
    "            appendage = pd.Series(appenders,index=stdbank.columns)\n",
    "            stdbank = stdbank.append(appendage, ignore_index=True)\n",
    "\n",
    "        stdbankjuststds = stdbank[slist].copy(deep=True)\n",
    "        mystds = stdbankjuststds.mean()\n",
    "        locusbank = locusbank.append(mystds,ignore_index=True)\n",
    "\n",
    "    locusbank.insert(0,'locus',floci)\n",
    "\n",
    "    # plot average standard deviations for all neutral loci, across all subset numbers\n",
    "    myl = []\n",
    "    for l in np.arange(len(floci)):\n",
    "        if mr2.loc[(mr2['ploidy'] == 'hap')&(mr2['envt'] == envts[e])&(mr2['main locus'] == floci[l])&(mr2['num term add'] == 0),'inferred_b_-1to1'].values[0] > 0.95:\n",
    "            plt.plot(np.arange(3,10),locusbank.iloc[l,1:])\n",
    "            plt.xlabel('subset size')\n",
    "            plt.ylabel('average standard deviation for N = 9')\n",
    "            myl = myl + [floci[l]]\n",
    "    plt.ylim(-0.5,4)\n",
    "    fig.text(0,1,ploidies[p]+'_'+envts[e])\n",
    "    plt.legend(myl)\n",
    "    plt.show()\n",
    "\n",
    "# do same plot but just for neutral loci\n",
    "# first for 4nqo\n",
    "#myneutrals = ['BUL2','FAS1','MKT1','NCS2','RPI1']\n",
    "\n",
    "#for e in np.arange(1,2):\n",
    "#    for l in np.arange(len(floci)):\n",
    "#        if floci[l] in myneutrals:\n",
    "#            plt.plot(np.arange(3,10),locusbank.iloc[l,1:])\n",
    "#            plt.xlabel('subset size')\n",
    "#            plt.ylabel('average standard deviation for N = 9')\n",
    "#    plt.legend(myneutrals)\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the megaremoverv2 variants\n",
    "\n",
    "# this has been set up so we will get all the guys, not just 1 in 5!\n",
    "mr2ss1 = pd.read_csv('20210712_megaremoverss_1in5.csv')\n",
    "mr2ss1 = mr2ss1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "mr2ss4 = pd.read_csv('20210712_megaremoverss_4in5.csv')\n",
    "mr2ss4 = mr2ss4.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "mr2ss = mr2ss1.append(mr2ss4).reset_index(drop=True)\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2ss)):\n",
    "    if mr2ss.loc[i,'main_b'] <= 1:\n",
    "        mr2ss.at[i,'main_b_-1to1'] = mr2ss.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2ss.at[i,'main_b_-1to1'] = 1/mr2ss.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2ss)):\n",
    "    if mr2ss.loc[i,'inferred_b'] <= 1:\n",
    "        mr2ss.at[i,'inferred_b_-1to1'] = mr2ss.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2ss.at[i,'inferred_b_-1to1'] = 1/mr2ss.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2ss['main/inferred'] = mr2ss['main_S']/mr2ss['inferred_S']\n",
    "mr2ss['1/inferred'] = mr2ss['1_S']/mr2ss['inferred_S']\n",
    "mr2ss['1/main'] = mr2ss['1_S']/mr2ss['main_S']\n",
    "\n",
    "\n",
    "\n",
    "# For each ploidy-envt-locus, plot ratios\n",
    "# Start by doing just those below the FCT threshold b, can toggle\n",
    "\n",
    "fct_thresh = 0.9\n",
    "\n",
    "#for p in np.arange(2):\n",
    "#    for e in np.arange(len(envts)):\n",
    "#        tempfloci = []\n",
    "#        for l in np.arange(len(floci)):\n",
    "#            mr2sssub = mr2ss.loc[(mr2ss['ploidy'] == ploidies[p])&(mr2ss['envt'] == envts[e])&(mr2ss['main locus'] == floci[l])].copy(deep=True).reset_index(drop=True)\n",
    "#            if mr2sssub.loc[0,'main_b_-1to1'] <= fct_thresh:\n",
    "#                tempfloci = tempfloci + [floci[l]]\n",
    "#                plt.plot(mr2sssub['num term add'],mr2sssub['1/main'])\n",
    "#                #plt.plot(mr2sssub['num term add'],mr2sssub['main/inferred'])\n",
    "#                #plt.plot(mr2sssub['num term add'],mr2sssub['1/inferred'])\n",
    "#                #plt.title(ploidies[p]+'-'+envts[e]+'_'+floci[l])\n",
    "#                #plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "#                #plt.show()\n",
    "#        plt.title(ploidies[p]+'-'+envts[e])\n",
    "#        plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "#        plt.legend(tempfloci)\n",
    "#        plt.show()\n",
    "\n",
    "\n",
    "mr2ssfullco = pd.read_csv('20210712_megaremoverssfullco_1in5.csv')\n",
    "mr2ssfullco = mr2ssfullco.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Create a main_b_-1to1 column and inferred_b_-1to1 column\n",
    "# This will help in partitioning the data to look just at those that have a FCT by our criteria\n",
    "for i in np.arange(len(mr2ssfullco)):\n",
    "    if mr2ssfullco.loc[i,'main_b'] <= 1:\n",
    "        mr2ssfullco.at[i,'main_b_-1to1'] = mr2ssfullco.loc[i,'main_b']\n",
    "    else:\n",
    "        mr2ssfullco.at[i,'main_b_-1to1'] = 1/mr2ssfullco.loc[i,'main_b']\n",
    "    \n",
    "for i in np.arange(len(mr2ssfullco)):\n",
    "    if mr2ssfullco.loc[i,'inferred_b'] <= 1:\n",
    "        mr2ssfullco.at[i,'inferred_b_-1to1'] = mr2ssfullco.loc[i,'inferred_b']\n",
    "    else:\n",
    "        mr2ssfullco.at[i,'inferred_b_-1to1'] = 1/mr2ssfullco.loc[i,'inferred_b']\n",
    "\n",
    "# Take ratios of the S values\n",
    "mr2ssfullco['main/inferred'] = mr2ssfullco['main_S']/mr2ssfullco['inferred_S']\n",
    "mr2ssfullco['1/inferred'] = mr2ssfullco['1_S']/mr2ssfullco['inferred_S']\n",
    "mr2ssfullco['1/main'] = mr2ssfullco['1_S']/mr2ssfullco['main_S']\n",
    "\n",
    "# For each ploidy-envt-locus, plot ratios\n",
    "# Start by doing just those below the FCT threshold b, can toggle\n",
    "\n",
    "fct_thresh = 0.9\n",
    "\n",
    "#for p in np.arange(2):\n",
    "#    for e in np.arange(len(envts)):\n",
    "#        tempfloci = []\n",
    "#        for l in np.arange(len(floci)):\n",
    "#            mr2ssfullcosub = mr2ssfullco.loc[(mr2ssfullco['ploidy'] == ploidies[p])&(mr2ssfullco['envt'] == envts[e])&(mr2ssfullco['main locus'] == floci[l])].copy(deep=True).reset_index(drop=True)\n",
    "#            if mr2ssfullcosub.loc[0,'main_b_-1to1'] <= fct_thresh:\n",
    "#                tempfloci = tempfloci + [floci[l]]\n",
    "#                plt.plot(mr2ssfullcosub['num term add'],mr2ssfullcosub['1/main'])\n",
    "#                #plt.plot(mr2ssfullcosub['num term add'],mr2ssfullcosub['main/inferred'])\n",
    "#                #plt.plot(mr2ssfullcosub['num term add'],mr2ssfullcosub['1/inferred'])\n",
    "#                #plt.title(ploidies[p]+'-'+envts[e]+'_'+floci[l])\n",
    "#                #plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "#                #plt.show()\n",
    "#        plt.title(ploidies[p]+'-'+envts[e])\n",
    "#        plt.axhline(y=1,color='xkcd:grey',zorder=0)\n",
    "#        plt.legend(tempfloci)\n",
    "#        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to get subset sizes, extract now\n",
    "sssize = pd.DataFrame()\n",
    "for ss in np.arange(len(ssnums)):\n",
    "    \n",
    "    myssnum = ssnums[ss]\n",
    "    \n",
    "    for p in np.arange(0,1):\n",
    "        for e in np.arange(0,1):\n",
    "            data1 = pd.read_csv('20211123_subsets-output/subsets/'+ploidies[p]+'_'+envts[e]+'_subset_'+str(myssnum)+'.txt',\n",
    "                                        sep='\\t',names=['genotype',ploidies[p]+'_'+envts[e]+'_Alex prediction',ploidies[p]+'_'+envts[e]+'_s-obs',ploidies[p]+'_'+envts[e]+'_s-obs-err'],\n",
    "                                        skiprows=2,skip_blank_lines=False)\n",
    "\n",
    "            # Chuck out the bottom lines that estimate fitness effects of specific combos of mutations\n",
    "            data1 = data1.loc[:data1.loc[(data1['genotype'].isnull())].index.tolist()[0]-1,:]\n",
    "\n",
    "            # Binary style for genotype\n",
    "            data1['genotype'] = data1['genotype'].astype(int).astype(str).str.zfill(10)\n",
    "\n",
    "            # Create a column for each locus\n",
    "            for l in np.arange(len(floci)):\n",
    "                data1[floci[l]] = data1.loc[:,'genotype'].str[l].astype(int)\n",
    "\n",
    "            locihere = []\n",
    "            for l in np.arange(len(floci)):\n",
    "                if len(data1.loc[data1[floci[l]] == 1]) > 0:\n",
    "                    locihere = locihere + [floci[l]]\n",
    "            \n",
    "            temp = pd.DataFrame()\n",
    "            temp.at[0,'subset'] = myssnum\n",
    "            temp.at[0,'num_mut'] = len(locihere)\n",
    "            sssize = sssize.append(temp)\n",
    "sssize = sssize.reset_index(drop=True)\n",
    "mr2ssfullco = pd.merge(mr2ssfullco,sssize,on='subset',how='left')\n",
    "mr2ss = pd.merge(mr2ss,sssize,on='subset',how='left')\n",
    "\n",
    "# export\n",
    "export_csv = mr2ss.to_csv(r'20211211_hap_37C_mr2ss.csv',index=True,header=True)\n",
    "#export\n",
    "export_csv = mr2ssfullco.to_csv(r'20211211_hap_37C_mr2ssfullco.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to analyze decipherability\n",
    "\n",
    "detabss = pd.DataFrame()\n",
    "detabssfc = pd.DataFrame()\n",
    "\n",
    "# first, start with all combinations of subsets and loci\n",
    "ssl = []\n",
    "for i in np.arange(len(mr2ss)):\n",
    "    ssl = ssl + [[mr2ss.loc[i,'subset'],mr2ss.loc[i,'main locus']]]\n",
    "ssl2 = []\n",
    "[ssl2.append(x) for x in ssl if x not in ssl2]\n",
    "ssl = ssl2\n",
    "\n",
    "myind = 0\n",
    "for i in np.arange(len(ssl)):\n",
    "    temp = mr2ss.loc[(mr2ss['subset'] == ssl[i][0])&(mr2ss['main locus'] == ssl[i][1])].reset_index(drop=True)\n",
    "    #tempfc = mr2ssfullco.loc[(mr2ssfullco['subset'] == ssl[i][0])&(mr2ssfullco['main locus'] == ssl[i][1])].reset_index(drop=True)\n",
    "    \n",
    "    detabss.at[myind,'subset'] = ssl[i][0]\n",
    "    detabss.at[myind,'ploidy'] = temp.loc[0,'ploidy']\n",
    "    detabss.at[myind,'envt'] =  temp.loc[0,'envt']\n",
    "    detabss.at[myind,'main locus'] = ssl[i][1]\n",
    "    \n",
    "    detabss.at[myind,'N'] = temp.loc[0,'N']\n",
    "    detabss.at[myind,'num_mut'] = temp.loc[0,'num_mut']\n",
    "    detabss.at[myind,'inferred_b_-1to1_original'] = temp.loc[0,'inferred_b_-1to1']\n",
    "    \n",
    "    if len(temp) == 1:\n",
    "        this1main = temp.loc[0,'1/main']\n",
    "        thisnumtermadd = temp.loc[0,'num term add']\n",
    "        thisfinalslope = temp.loc[0,'inferred_b_-1to1']\n",
    "   #     this1mainfc = tempfc.loc[0,'1/main']\n",
    "   #     thisnumtermaddfc = tempfc.loc[0,'num term add']\n",
    "   #     thisfinalslopefc = tempfc.loc[0,'inferred_b_-1to1']\n",
    "    else:\n",
    "        mymaxind = temp.iloc[-1]['num term add']\n",
    "        this1main = temp.loc[int(mymaxind),'1/main']\n",
    "        thisnumtermadd = mymaxind\n",
    "        thisfinalslope = temp.loc[int(mymaxind),'inferred_b_-1to1']\n",
    "   #     mymaxindfc = tempfc.iloc[-1]['num term add']\n",
    "   #     this1mainfc = tempfc.loc[int(mymaxindfc),'1/main']\n",
    "   #     thisnumtermaddfc = mymaxindfc\n",
    "   #     thisfinalslopefc = tempfc.loc[int(mymaxindfc),'inferred_b_-1to1']\n",
    "    \n",
    "    detabss.at[myind,'1/main_final'] = this1main\n",
    "    detabss.at[myind,'num term add'] = thisnumtermadd\n",
    "    detabss.at[myind,'inferred_b_-1to1_final'] = thisfinalslope\n",
    "    \n",
    "    #detabssfc.at[myind,'subset'] = ssl[i][0]\n",
    "    #detabssfc.at[myind,'ploidy'] = tempfc.loc[0,'ploidy']\n",
    "    #detabssfc.at[myind,'envt'] =  tempfc.loc[0,'envt']\n",
    "    #detabssfc.at[myind,'main locus'] = ssl[i][1]\n",
    "    \n",
    "    #detabssfc.at[myind,'N'] = tempfc.loc[0,'N']\n",
    "    #detabssfc.at[myind,'num_mut'] = tempfc.loc[0,'num_mut']\n",
    "    #detabssfc.at[myind,'inferred_b_-1to1_original'] = tempfc.loc[0,'inferred_b_-1to1']\n",
    "    \n",
    "    #detabssfc.at[myind,'1/main_final'] = this1mainfc\n",
    "    #detabssfc.at[myind,'num term add'] = thisnumtermaddfc\n",
    "    #detabssfc.at[myind,'inferred_b_-1to1_final'] = thisfinalslopefc\n",
    "    \n",
    "    myind = myind+1\n",
    "        \n",
    "# export\n",
    "export_csv = detabss.to_csv(r'20211211_hap_37C_detabss.csv',index=True,header=True)\n",
    "#export\n",
    "#export_csv = detabssfc.to_csv(r'20211211_hap_37C_detabssfc.csv',index=True,header=True)\n",
    "\n",
    "\n",
    "#now that we have the table, we can do some analysis\n",
    "myind=0\n",
    "mytrack = pd.DataFrame()\n",
    "temp = detabss.loc[(detabss['N'] > 2)&(detabss['inferred_b_-1to1_original'] <= 0.9)].copy(deep=True)\n",
    "for p in np.arange(0,1):\n",
    "    for e in np.arange(0,1):\n",
    "        for l in np.arange(len(floci)):\n",
    "            for n in np.arange(3,10):\n",
    "                templn = temp.loc[(temp['main locus'] == floci[l])&(temp['num_mut'] == n)].copy(deep=True)\n",
    "                count1mainlessthan1 = len(templn.loc[templn['1/main_final'] <= 1])\n",
    "                countall = len(templn)\n",
    "                mytrack.at[myind,'ploidy'] = ploidies[p]\n",
    "                mytrack.at[myind,'envt'] = envts[e]\n",
    "                mytrack.at[myind,'main locus'] = floci[l]\n",
    "                mytrack.at[myind,'num_mut'] = n\n",
    "                mytrack.at[myind,'count total'] = countall\n",
    "                mytrack.at[myind,'count 1/main <= 1'] = count1mainlessthan1\n",
    "                if len(templn) == 0:\n",
    "                    mytrack.at[myind,'fraction <= 1'] = np.nan\n",
    "                else:\n",
    "                    mytrack.at[myind,'fraction <= 1'] = count1mainlessthan1 / countall\n",
    "                myind=myind+1\n",
    "                \n",
    "# now plot the results\n",
    "for p in np.arange(0,1):\n",
    "    for e in np.arange(0,1):\n",
    "        for l in np.arange(len(floci)):\n",
    "            templ = mytrack.loc[(mytrack['ploidy'] == ploidies[p])&(mytrack['envt'] == envts[e])&(mytrack['main locus'] == floci[l])]\n",
    "            plt.plot(np.arange(3,10),templ['fraction <= 1'])\n",
    "        plt.xlabel('subset size')\n",
    "        plt.ylabel('fraction where 1/main <= 1 after removal')\n",
    "        plt.title(ploidies[p]+'_'+envts[e])\n",
    "        plt.legend(floci)\n",
    "    plt.show()\n",
    "\n",
    "# do for fullco now\n",
    "\n",
    "#now that we have the table, we can do some analysis\n",
    "myind=0\n",
    "mytrackfc = pd.DataFrame()\n",
    "#temp = detabssfc.loc[(detabssfc['N'] > 2)&(detabssfc['inferred_b_-1to1_original'] <= 0.9)].copy(deep=True)\n",
    "#for p in np.arange(0,1):\n",
    "#    for e in np.arange(0,1):\n",
    "#        for l in np.arange(len(floci)):\n",
    "#            for n in np.arange(3,10):\n",
    "#                templn = temp.loc[(temp['main locus'] == floci[l])&(temp['num_mut'] == n)].copy(deep=True)\n",
    "#                count1mainlessthan1 = len(templn.loc[templn['1/main_final'] <= 1])\n",
    "#                countall = len(templn)\n",
    "#                mytrackfc.at[myind,'ploidy'] = ploidies[p]\n",
    "#                mytrackfc.at[myind,'envt'] = envts[e]\n",
    "#                mytrackfc.at[myind,'main locus'] = floci[l]\n",
    "#                mytrackfc.at[myind,'num_mut'] = n\n",
    "#                mytrackfc.at[myind,'count total'] = countall\n",
    "#                mytrackfc.at[myind,'count 1/main <= 1'] = count1mainlessthan1\n",
    "#                if len(templn) == 0:\n",
    "#                    mytrackfc.at[myind,'fraction <= 1'] = np.nan\n",
    "#                else:\n",
    "#                    mytrackfc.at[myind,'fraction <= 1'] = count1mainlessthan1 / countall\n",
    "#                myind=myind+1\n",
    "                \n",
    "# now plot the results\n",
    "#for p in np.arange(0,1):\n",
    "#    for e in np.arange(0,1):\n",
    "#        for l in np.arange(len(floci)):\n",
    "#            templ = mytrackfc.loc[(mytrackfc['ploidy'] == ploidies[p])&(mytrackfc['envt'] == envts[e])&(mytrackfc['main locus'] == floci[l])]\n",
    "#            plt.plot(np.arange(3,10),templ['fraction <= 1'])\n",
    "#        plt.xlabel('subset size')\n",
    "#        plt.ylabel('fraction where 1/main <= 1 after removal')\n",
    "#        plt.title(ploidies[p]+'_'+envts[e])\n",
    "#        plt.legend(floci)\n",
    "#    plt.show()\n",
    "\n",
    "    \n",
    "# how many go to 1/main <= 1 with fullco vs non-fullco version of the removal analysis?\n",
    "# actually, don't do this analysis now, not top priority\n",
    "\n",
    "# actual next step - look at 1/main final instead of fraction ≤ 1\n",
    "temp = detabss.loc[(detabss['N'] > 2)&(detabss['inferred_b_-1to1_original'] <= 0.9)].copy(deep=True)\n",
    "for p in np.arange(0,1):\n",
    "    for e in np.arange(0,1):\n",
    "        for l in np.arange(len(floci)):\n",
    "            templ = temp.loc[(temp['main locus'] == floci[l])]\n",
    "            plt.scatter(templ['num_mut'],templ['1/main_final'])\n",
    "            #myreg = linregress(templ['num_mut'],templ['1/main_final'])\n",
    "            #plt.plot(np.arange(3,10),np.arange(3,10)*myreg.slope+myreg.intercept)\n",
    "            mymeds = []\n",
    "            for n in np.arange(3,10):\n",
    "                templn = templ.loc[(templ['num_mut'] == n)]\n",
    "                mymeds = mymeds + [np.median(templn['1/main_final'])]\n",
    "            plt.plot(np.arange(3,10),mymeds)\n",
    "        plt.xlabel('subset size')\n",
    "        plt.ylabel('final SSE b=1 / b=global')\n",
    "        plt.ylim(-0.5,10) #I know this cuts off some points\n",
    "        plt.title(ploidies[p]+'_'+envts[e])\n",
    "        plt.legend(floci)\n",
    "        plt.axhline(y=1,lw=0.5,color='k',zorder=0)\n",
    "    plt.show()\n",
    "\n",
    "# now with full data coefficients\n",
    "# actual next step - look at 1/main final instead of fraction ≤ 1\n",
    "#temp = detabssfc.loc[(detabssfc['N'] > 2)&(detabssfc['inferred_b_-1to1_original'] <= 0.9)].copy(deep=True)\n",
    "#for p in np.arange(0,1):\n",
    "#    for e in np.arange(0,1):\n",
    "#        for l in np.arange(len(floci)):\n",
    "#            templ = temp.loc[(temp['main locus'] == floci[l])]\n",
    "#            plt.scatter(templ['num_mut'],templ['1/main_final'])\n",
    "#            #myreg = linregress(templ['num_mut'],templ['1/main_final'])\n",
    "#            #plt.plot(np.arange(3,10),np.arange(3,10)*myreg.slope+myreg.intercept)\n",
    "#            mymeds = []\n",
    "#            for n in np.arange(3,10):\n",
    "#                templn = templ.loc[(templ['num_mut'] == n)]\n",
    "#                mymeds = mymeds + [np.median(templn['1/main_final'])]\n",
    "#            plt.plot(np.arange(3,10),mymeds)\n",
    "#        plt.xlabel('subset size')\n",
    "#        plt.ylabel('final SSE b=1 / b=global')\n",
    "#        plt.ylim(-0.5,10) #I know this cuts off some points\n",
    "#        plt.title(ploidies[p]+'_'+envts[e])\n",
    "#        plt.legend(floci)\n",
    "#        plt.axhline(y=1,lw=0.5,color='k',zorder=0)\n",
    "#    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
