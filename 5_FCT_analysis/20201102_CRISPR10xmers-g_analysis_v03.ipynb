{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal = genotype all CRISPR 10xmer haploids from multiplex PCR nextseq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from numpy import matlib\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pylab as pl\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import scipy as stats\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import multinomial\n",
    "from scipy.stats import t\n",
    "from scipy.stats import chi2\n",
    "from scipy.linalg import hadamard\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import OrderedDict\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indices, loci, and tables for properly extracting info from \"BC\"\n",
    "INDEX_LIST = ['CTCGTA','CATTGG','AAGGTC','GACTAC','TTCACG','ACACTT','CGAAGG','TCTTGC']\n",
    "indexmap = pd.DataFrame()\n",
    "indexmap['Index'] = INDEX_LIST\n",
    "indexmap['plateset'] = [1,2,3,4,5,6,7,8]\n",
    "LOCUS_LIST = ['BUL2','FAS1','MKT1','NCS2','PMA1','RHO5','SCH9','WHI2','AKL1','RPI1','HSL7','SPT7','FRS1']\n",
    "letters = ['A-E','B-F','C-G','D-H']\n",
    "letters14 = ['A','B','C','D']\n",
    "letters58 = ['E','F','G','H']\n",
    "lettersall = letters14+letters58\n",
    "letterslash = ['A/E','B/F','C/G','D/H']\n",
    "\n",
    "indlet = []\n",
    "indletright = []\n",
    "for i in np.arange(len(indexmap)):\n",
    "    for l in np.arange(len(letters)):\n",
    "        indlet = indlet + [indexmap.loc[i,'plateset'].astype(str)+letters[l]]\n",
    "        if i <= 3:\n",
    "            indletright = indletright + [indexmap.loc[i,'plateset'].astype(str)+letters14[l]]\n",
    "        else:\n",
    "            indletright = indletright + [indexmap.loc[i,'plateset'].astype(str)+letters58[l]]\n",
    "\n",
    "indletdf = pd.DataFrame()\n",
    "indletdf['plateletterlong'] = indlet\n",
    "indletdf['plateletter'] = indletright\n",
    "\n",
    "wellund = []\n",
    "wellright = []\n",
    "for r in np.arange(len(lettersall)):\n",
    "    for c in np.arange(3,11):\n",
    "        wellright = wellright + [lettersall[r]+str(c)]\n",
    "        if c < 10:\n",
    "            wellund = wellund + [lettersall[r]+str(c)+'_']\n",
    "        else:\n",
    "            wellund = wellund + [lettersall[r]+str(c)]\n",
    "\n",
    "wellnamemap = pd.DataFrame()\n",
    "wellnamemap['prewell'] = wellund\n",
    "wellnamemap['platewell'] = wellright\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import filenames to work off of (excluding A-E_D6)\n",
    "\n",
    "filenames = pd.read_csv(\"CRISPR_10xmer-g_array_v02.csv\")\n",
    "\n",
    "\n",
    "# import all data into a dataframe\n",
    "\n",
    "allcounts = pd.DataFrame()\n",
    "allcounts['BC'] = \"\"\n",
    "allcounts['Reads'] = \"\"\n",
    "allcounts['UMI.Count'] = \"\"\n",
    "allcounts['Library'] = \"\"\n",
    "\n",
    "for f in np.arange(len(filenames)):\n",
    "    temptable = pd.read_csv(\"20201102_CRISPR10xmers_g_output_batch/counts/\"+filenames.loc[f,'Library']+\"_counts.csv\")\n",
    "    temptable['Library'] = filenames.loc[f,'Library']\n",
    "    allcounts = allcounts.append(temptable)\n",
    "\n",
    "\n",
    "# Now we want to add in the A/E-D6 reads from the MiSeq run (20201207)\n",
    "# import all data into a dataframe\n",
    "temptable = pd.read_csv(\"20201207_output_miseq/counts/Rxn_51_S67_R1_001.fastq.gz_counts.csv\")\n",
    "temptable['Library'] = \"A-E_D6_S28_R1_001.fastq.gz\"\n",
    "allcounts = allcounts.append(temptable)    \n",
    "    \n",
    "allcounts = allcounts.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to extract the salient features: allele and well identity\n",
    "\n",
    "allcounts[\"Index\"] = allcounts.BC.str[:6]\n",
    "allcounts[\"Locus\"] = allcounts.BC.str[7:11]\n",
    "allcounts[\"Allele\"] = allcounts.BC.str[12:]\n",
    "\n",
    "allcounts = pd.merge(allcounts,indexmap,on='Index',how='left')\n",
    "\n",
    "allcounts['letterset'] = allcounts.Library.str[:3]\n",
    "\n",
    "allcounts['plateletterlong'] = allcounts['plateset'].astype(str)+allcounts['letterset']\n",
    "\n",
    "allcounts = pd.merge(allcounts,indletdf,on='plateletterlong',how='left')\n",
    "\n",
    "allcounts['prewell'] = allcounts.Library.str[4:7]\n",
    "\n",
    "allcounts = pd.merge(allcounts,wellnamemap,on='prewell',how='left')\n",
    "\n",
    "allcounts['Well'] = allcounts['plateletter']+'-'+allcounts['platewell']\n",
    "\n",
    "allcounts = allcounts.drop(columns = ['BC','plateset','letterset','plateletterlong','plateletter','platewell','prewell','Index','UMI.Count','Library'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick check for the number of BUL2 reads associated with each well, since this is the one I'm concerned about\n",
    "#bul2check = allcounts.loc[(allcounts['Locus'] == 'BUL2')].reset_index(drop=True)\n",
    "#export_csv = bul2check.to_csv(r'bul2check.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "locusallelemap = allcounts[['Locus','Allele']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "allcounts_allelelist = allcounts.drop(columns = ['Well','Locus'])\n",
    "allcounts_allelelist = allcounts_allelelist.groupby((['Allele'])).sum()\n",
    "allcounts_allelelist = pd.merge(allcounts_allelelist,locusallelemap,on='Allele',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_csv = allcounts_allelelist.to_csv(r'20201102_10xmers-g_allcounts-allelelist.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of alleles. Anything allele that has ≥ 10 reads in at least one well and ≥ 1% freq in at least one well gets\n",
    "# a name, rest is \"na\" name, lumped together in later stages\n",
    "allelenames = pd.read_csv(\"20201103_allelelist_v4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map allele names to big table\n",
    "allelenames2 = allelenames.drop(columns=['Locus'])\n",
    "allcounts = pd.merge(allcounts,allelenames2,on='Allele',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all wells present\n",
    "well_list = list(OrderedDict.fromkeys(allcounts['Well']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead, import acfreq from csv\n",
    "#acfreq = pd.read_csv(\"20201208_acfreq.csv\")\n",
    "#acfreq = acfreq.drop(columns = ['Unnamed: 0'])\n",
    "#acfreq = acfreq.drop(columns=['Unnamed: 0','Allele name','Locus','Complex'])\n",
    "\n",
    "# do some terraforming to fold in the updated allele list\n",
    "#acfreq = pd.merge(acfreq,allelenames,how=\"left\",on=\"Allele\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list on which we can do the freq calculations and filtering\n",
    "\n",
    "blankslist = pd.read_csv(\"20201104_blankslist.csv\")\n",
    "\n",
    "acfreq = pd.merge(allcounts,blankslist,on='Well',how='left')\n",
    "\n",
    "acfreqmod = acfreq.copy(deep=True)\n",
    "acfreqmod = acfreqmod.loc[(acfreqmod['blank?']=='no')].reset_index(drop=True)\n",
    "acfreqmod['Well_Allele'] = acfreqmod['Well']+'_'+acfreqmod['Allele']\n",
    "\n",
    "acfreqmod['ComplexsepBY'] = \"\"\n",
    "acfreqmod['pool'] = \"\"\n",
    "\n",
    "for i in np.arange(len(acfreqmod)):\n",
    "    if acfreqmod.loc[i,'Allele name'] == 'BY':\n",
    "        acfreqmod.at[i,'ComplexsepBY'] = 'BY'\n",
    "    else:\n",
    "        acfreqmod.at[i,'ComplexsepBY'] = acfreqmod.loc[i,'Complex']\n",
    "    numr = float(acfreqmod.loc[i,'Well'][0])\n",
    "    letr = acfreqmod.loc[i,'Well'][1]\n",
    "    wellr = acfreqmod.loc[i,'Well'][2:]\n",
    "    if numr < 5:\n",
    "        acfreqmod.at[i,'pool'] = letterslash[letters14.index(letr)]+wellr\n",
    "    else:\n",
    "        acfreqmod.at[i,'pool'] = letterslash[letters58.index(letr)]+wellr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to apply our chimera recognition / filtering algorithm\n",
    "\n",
    "# create an empty dataframe to add things to\n",
    "freqstable = pd.DataFrame()\n",
    "\n",
    "# Create a comprehensive list of pools\n",
    "poollist = list(OrderedDict.fromkeys(acfreqmod['pool']))\n",
    "\n",
    "# The outer loop goes pool by pool.\n",
    "for p in np.arange(240,241):\n",
    "#THIS TAKES A WHILE TO RUN\n",
    "#for p in np.arange(len(poollist)):    \n",
    "    \n",
    "    # Isolate all the wells/loci/alleles for a given pool\n",
    "    subt = acfreqmod.loc[(acfreqmod['pool'] == poollist[p])].reset_index(drop=True)\n",
    "    \n",
    "    # Look one locus at a time\n",
    "    for l in np.arange(9,10):\n",
    "    #for l in np.arange(len(LOCUS_LIST)):        \n",
    "        subtl = subt.loc[(subt['Locus'] == LOCUS_LIST[l])].reset_index(drop=True)\n",
    "        \n",
    "        # reformat this info to prep for chimera filtration\n",
    "        s2 = pd.DataFrame()\n",
    "        \n",
    "        # add in the well names\n",
    "        s2['Well'] = list(OrderedDict.fromkeys(subtl['Well']))\n",
    "        \n",
    "        # list of unique allele names\n",
    "        thesealleles = list(OrderedDict.fromkeys(subtl['Allele name']))\n",
    "        \n",
    "        # Populate the table\n",
    "        summer = subtl.groupby(['Well','Allele name'])['Reads'].sum().to_frame().reset_index()\n",
    "        \n",
    "        for a in np.arange(len(thesealleles)):\n",
    "            thisallele = thesealleles[a]\n",
    "            tab2add = summer.loc[(summer['Allele name'] == thisallele)].drop(columns='Allele name').rename(columns={'Reads':thisallele}).reset_index(drop=True)\n",
    "            s2 = pd.merge(s2,tab2add,on='Well',how='left')\n",
    "        \n",
    "        # Replace NaNs with 0 (since not all wells have all alleles)\n",
    "        s2 = s2.fillna(0)\n",
    "        \n",
    "        # For Alex's pipeline, convert to numpy array without headers, well names\n",
    "        data = s2.drop(columns=['Well']).to_numpy(copy=True)\n",
    "        \n",
    "        # HERE BEGINS ALEX'S CODE!\n",
    "        \n",
    "        # Add a pseudocount\n",
    "        data = data + 1\n",
    "        #print(data)\n",
    "\n",
    "        # Get chimera pool frequency\n",
    "        # As opposed to previously, we now have multiple possibilities. We'll always assume that the first column is psWT, but in reality it doesn't matter.\n",
    "        chimera_count = []\n",
    "        chimera_freq = []\n",
    "        for i in range(data.shape[1]):\n",
    "            chimera_count.append(np.sum(data[:,i]))\n",
    "\n",
    "        for i in range(data.shape[1]):\n",
    "            chimera_freq.append(chimera_count[i] / np.sum(chimera_count))\n",
    "\n",
    "\n",
    "        # Ok let's go well by well.\n",
    "        min_pis = 0.5\n",
    "\n",
    "        max_likelihood = -1 * np.Inf\n",
    "        for it in range(100):\n",
    "            test_pi = min_pis + (1-min_pis)/100 * it\n",
    "            probability = 0\n",
    "\n",
    "            # Ok so what is the model?\n",
    "            # We assume that a well is the dominant read\n",
    "            # That means that the probability for observing the dominant read is always 1 * test_pi + (1-test_pi) * chimera_read_probability\n",
    "            # For observing the non-dominant read it's 0*test_pi + (1-test_pi) * chimera_read_probability\n",
    "\n",
    "            for i in range(data.shape[0]):\n",
    "                index_max = np.argmax(data[i])\n",
    "                p_arr = []\n",
    "                for j in range(data.shape[1]):\n",
    "                    if(j == index_max):\n",
    "                        # This is the type this well is most likely to be.\n",
    "                        p_arr.append(test_pi * 1 + (1-test_pi) * chimera_freq[j])\n",
    "                    else:\n",
    "                        p_arr.append((1-test_pi) * chimera_freq[j])\n",
    "\n",
    "                probability = probability + multinomial.logpmf(data[i],n=np.sum(data[i]),p=p_arr)\n",
    "\n",
    "            if(probability > max_likelihood):\n",
    "                max_likelihood = probability\n",
    "                max_test_pi = test_pi\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"Non chimeric rate: \" + str(max_test_pi))\n",
    "\n",
    "        f_superarr = []\n",
    "        for i in range(data.shape[0]):\n",
    "            index_max = np.argmax(data[i])\n",
    "            well_reads = np.sum(data[i])\n",
    "            obs_freq = data[i][0] / well_reads\n",
    "\n",
    "            # Remember that # We assume that a well is the dominant read\n",
    "            # That means that the probability for observing the dominant read is always 1 * test_pi + (1-test_pi) * chimera_read_probability\n",
    "            # For observing the non-dominant read it's 0*test_pi + (1-test_pi) * chimera_read_probability\n",
    "\n",
    "            # f * max_test_pi + (1-max_test_pi) * chimera_freq[j] = data[i][j]/well_reads\n",
    "            f_arr = []\n",
    "            for j in range(data.shape[1]):\n",
    "                f = (data[i][j] / well_reads - (1-max_test_pi) * chimera_freq[j]) / max_test_pi\n",
    "                f_arr.append(f)\n",
    "\n",
    "            # ok is the solution valid?\n",
    "            params = []\n",
    "            if(len(np.where(np.array(f_arr) < 0)[0]) > 0 or len(np.where(np.array(f_arr) > 1)[0]) > 0):\n",
    "                # invalid\n",
    "                # Now search on the boundaries for some valid parameters\n",
    "                # We need to set up to n-1 parameters to be equal to zero and find the solutions in those cases.\n",
    "                max_val = pow(2,data.shape[1])\n",
    "                best_logL = -np.Inf\n",
    "                best_logL_params = []\n",
    "                for j in range(1,max_val-1):\n",
    "                    mask_string = (\"{0:b}\".format(j).zfill(data.shape[1]))\n",
    "                    #print(mask_string)\n",
    "                    enumerated = [place  for place, letter in enumerate(mask_string) if letter == \"1\"]\n",
    "                    #print(enumerated)\n",
    "                    # Inside enumerated are the positions which will not be constrained to zero\n",
    "                    # Based on this solution: lambda = max_test_pi * (data[i][0] + data[i][1]) / (max_test_pi + (1-max_test_pi) * (chimera_freq1 + chimera_freq2))\n",
    "                    lambda_val = max_test_pi * np.sum(data[i][enumerated]) / (max_test_pi + (1-max_test_pi) * np.sum(np.array(chimera_freq)[enumerated]))\n",
    "\n",
    "                    # and:\n",
    "                    # (max_test_pi * data[i][0] / lambda - (1-max_test_pi) * chimera_freq1) / max_test_pi = f1\n",
    "\n",
    "                    f_trials = []\n",
    "                    for k in range(data.shape[1]):\n",
    "                        if(k in enumerated):\n",
    "                            f = round((max_test_pi * data[i][k] / lambda_val - (1-max_test_pi) * chimera_freq[k]) / max_test_pi,15)\n",
    "                            f_trials.append(f)\n",
    "                        else:\n",
    "                            f_trials.append(0)\n",
    "                    # ok now test if that worked\n",
    "                    #print(f_trials)\n",
    "\n",
    "                    if(len(np.where(np.array(f_trials) < 0)[0]) == 0 and len(np.where(np.array(f_trials) > 1)[0]) == 0):\n",
    "                        # valid!\n",
    "                        #print(f_trials)\n",
    "                        # Obtain the likelihood of the data!\n",
    "                        p_arr = np.array(f_trials) * max_test_pi + (1-max_test_pi) * np.array(chimera_freq)\n",
    "                        #print(p_arr)\n",
    "                        log_p = multinomial.logpmf(data[i],n=np.sum(data[i]),p=p_arr)\n",
    "                        #print(log_p)\n",
    "\n",
    "                        if(log_p > best_logL):\n",
    "                            best_logL = log_p\n",
    "                            best_logL_params = f_trials\n",
    "\n",
    "                #print(best_logL_params)\n",
    "                f_superarr.append(best_logL_params) \n",
    "            else:\n",
    "                #print(f_arr)   \n",
    "                f_superarr.append(f_arr)\n",
    "        \n",
    "        \n",
    "        truefreqtable = pd.DataFrame(f_superarr)\n",
    "        \n",
    "        # Before tacking on the truefreqtable to s2, want to add some fields.\n",
    "        \n",
    "        # First, get the raw frequencies for comparison.\n",
    "        len2use = len(s2.columns)\n",
    "        readsums = s2.iloc[:,1:len2use].sum(axis=1)\n",
    "        for a in np.arange(len(thesealleles)):\n",
    "            thisallele = thesealleles[a]\n",
    "            s2[thisallele+'-rfreq'] = s2[thisallele]/readsums\n",
    "        \n",
    "        # Next, add the \"true frequencies\" from the chimera consideration process\n",
    "        truefreqtable.columns = [s + '-pcfreq' for s in thesealleles]\n",
    "        s2 = pd.concat([s2,truefreqtable],axis=1)\n",
    "        \n",
    "        # Calculate out the inferred reads\n",
    "        for a in np.arange(len(thesealleles)):\n",
    "            thisallele = thesealleles[a]\n",
    "            s2[thisallele+'-pcreads'] = readsums * s2[thisallele+'-pcfreq']\n",
    "        \n",
    "        # Let's re-re-format this table in order to make it somewhat tractable.\n",
    "        s3 = pd.DataFrame()\n",
    "        \n",
    "        for a in np.arange(len(thesealleles)):\n",
    "            thisallele = thesealleles[a]\n",
    "            temp = s2[['Well',thisallele,thisallele+'-rfreq',thisallele+'-pcfreq',thisallele+'-pcreads']]\n",
    "            temp = temp.rename(columns={thisallele:'Reads',thisallele+'-rfreq':'Rawfreq',thisallele+'-pcfreq':'pcfreq',thisallele+'-pcreads':'pcReads'})\n",
    "            temp['Allele name'] = thisallele\n",
    "            temp['freq-in-pool'] = s2[thisallele].sum()/s2.iloc[:,1:len2use].sum(axis=0).sum()\n",
    "            s3 = s3.append(temp)\n",
    "        \n",
    "        s3['Non-chimeric rate'] = max_test_pi\n",
    "        s3['Chimeric rate'] = 1 - max_test_pi\n",
    "        s3['Locus'] = LOCUS_LIST[l]\n",
    "        s3['pool'] = poollist[p]\n",
    "        \n",
    "        freqstable = freqstable.append(s3)\n",
    "    print(str(p))\n",
    "\n",
    "freqstable = freqstable.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#export_csv = freqstable.to_csv(r'20210105_freqstable.csv',index=True,header=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the freqstable in case don't want to run all this code again!\n",
    "freqstable = pd.read_csv(\"20210105_freqstable.csv\")\n",
    "freqstable = freqstable.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's time to apply some thresholds, getting the information in the right format for genotyping\n",
    "# First, need to map the \"complexes\" back on.\n",
    "# To do this, need to create a new field, \"Locus-Allele name\"\n",
    "# Also want to collapse the na alleles!\n",
    "naind1 = allelenames.loc[(allelenames['Allele name'] == 'na')].index.values.tolist()\n",
    "allelenames.at[naind1,'Allele'] = 'na'\n",
    "allelenames = allelenames.drop_duplicates().reset_index(drop=True)\n",
    "allelenames['L-A'] = allelenames['Locus']+'-'+allelenames['Allele name']\n",
    "\n",
    "freqstable['L-A'] = freqstable['Locus']+'-'+freqstable['Allele name']\n",
    "\n",
    "# Drop some columns to enable less annoying mapping\n",
    "allelenames2 = allelenames.drop(columns=['Locus','Allele name'])\n",
    "ft2 = pd.merge(freqstable,allelenames2,on='L-A',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get in format for proper genotyping\n",
    "\n",
    "gt = pd.DataFrame()\n",
    "\n",
    "new_WL = list(OrderedDict.fromkeys(ft2['Well']))\n",
    "c_list = ['WT','Mut','Other','na']\n",
    "\n",
    "gt['Well'] = \"\"\n",
    "\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gt[LOCUS_LIST[l]+'-totalreads'] = ''\n",
    "    for c in np.arange(len(c_list)):\n",
    "        gt[LOCUS_LIST[l]+'-'+c_list[c]+'%'] = \"\"\n",
    "    gt[LOCUS_LIST[l]+'-maxcomplex'] = ''\n",
    "    gt[LOCUS_LIST[l]+'-maxcomplex%'] = ''\n",
    "\n",
    "#for w in np.arange(0,1):\n",
    "for w in np.arange(len(new_WL)):\n",
    "    gt.at[w,'Well'] = new_WL[w]\n",
    "    wt = ft2.loc[(ft2['Well'] == new_WL[w])].reset_index(drop=True)\n",
    "    #for l in np.arange(0,1):\n",
    "    for l in np.arange(len(LOCUS_LIST)):\n",
    "        wlt = wt.loc[(wt['Locus'] == LOCUS_LIST[l])].reset_index(drop=True)\n",
    "        wlts = wlt.groupby('Complex').sum().reset_index()\n",
    "        gt.at[w,LOCUS_LIST[l]+'-totalreads'] = wlts['Reads'].sum()\n",
    "        \n",
    "        cmax = ''\n",
    "        cpmax = 0\n",
    "        for c in np.arange(len(c_list)):\n",
    "            if len(wlts.loc[(wlts['Complex'] == c_list[c])]) == 0:\n",
    "                gt.at[w,LOCUS_LIST[l]+'-'+c_list[c]+'%'] = 0\n",
    "            else:\n",
    "                cp = wlts.loc[(wlts['Complex'] == c_list[c])].reset_index(drop=True).loc[0,'pcfreq']\n",
    "                gt.at[w,LOCUS_LIST[l]+'-'+c_list[c]+'%'] = cp\n",
    "                \n",
    "                if cp > cpmax:\n",
    "                    cpmax = cp\n",
    "                    cmax = c_list[c]\n",
    "        \n",
    "        gt.at[w,LOCUS_LIST[l]+'-maxcomplex'] = cmax\n",
    "        gt.at[w,LOCUS_LIST[l]+'-maxcomplex%'] = cpmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now set a threshold on mixednesss\n",
    "gt2 = gt.copy(deep=True)\n",
    "\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gt2[LOCUS_LIST[l]+'_threshterm'] = ''\n",
    "    \n",
    "for w in np.arange(len(gt2)):\n",
    "    for l in np.arange(len(LOCUS_LIST)):\n",
    "        wl_maxp = gt2.loc[w,LOCUS_LIST[l]+'-maxcomplex%']\n",
    "        # this is the new way I'm trying that subtracts out na %\n",
    "        wl_maxc = gt2.loc[w,LOCUS_LIST[l]+'-maxcomplex']\n",
    "        wl_nap = gt2.loc[w,LOCUS_LIST[l]+'-na%']\n",
    "        \n",
    "        if wl_maxc == 'na':\n",
    "            gt2.at[w,LOCUS_LIST[l]+'_threshterm'] = 1 - wl_maxp\n",
    "        \n",
    "        else:\n",
    "            gt2.at[w,LOCUS_LIST[l]+'_threshterm'] = 1 - wl_maxp - wl_nap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the threshterms, let's figure out the threshold for each locus.\n",
    "# Note that this likely won't work with FRS1 given its unique properties.\n",
    "\n",
    "threshtable = pd.DataFrame()\n",
    "threshtable['Locus'] = LOCUS_LIST\n",
    "threshtable['threshold'] = ''\n",
    "\n",
    "#for l in np.arange(0,1):\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    curve = sorted(gt2[LOCUS_LIST[l]+'_threshterm'].values.tolist())\n",
    "\n",
    "    # Code copied from https://stackoverflow.com/a/37121355\n",
    "    nPoints = len(curve)\n",
    "    allCoord = np.vstack((range(nPoints), curve)).T\n",
    "    np.array([range(nPoints), curve])\n",
    "    firstPoint = allCoord[0]\n",
    "    lineVec = allCoord[-1] - allCoord[0]\n",
    "    lineVecNorm = lineVec / np.sqrt(np.sum(lineVec**2))\n",
    "    vecFromFirst = allCoord - firstPoint\n",
    "    scalarProduct = np.sum(vecFromFirst * np.matlib.repmat(lineVecNorm, nPoints, 1), axis=1)\n",
    "    vecFromFirstParallel = np.outer(scalarProduct, lineVecNorm)\n",
    "    vecToLine = vecFromFirst - vecFromFirstParallel\n",
    "    distToLine = np.sqrt(np.sum(vecToLine ** 2, axis=1))\n",
    "    idxOfBestPoint = np.argmax(distToLine)\n",
    "    \n",
    "    # Now take that index and convert it into a % cutoff\n",
    "    lthresh = curve[idxOfBestPoint]\n",
    "    threshtable.at[l,'threshold'] = lthresh\n",
    "    print('threshold = '+str(lthresh))\n",
    "    print('excludes '+str(len(curve) - idxOfBestPoint)+' wells')\n",
    "    plt.plot(curve)\n",
    "    plt.title(LOCUS_LIST[l])\n",
    "    plt.plot(np.linspace(0,1892),[lthresh]*len(np.linspace(0,1892)),alpha=0.5)\n",
    "    plt.show()\n",
    "    print('*********************')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get genotypes based on the threshold\n",
    "mthresh = threshtable['threshold'].tolist()\n",
    "            \n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gt2[LOCUS_LIST[l]+'_g'] = ''\n",
    "    \n",
    "for w in np.arange(len(gt2)):\n",
    "    for l in np.arange(len(LOCUS_LIST)):\n",
    "        wl_maxp = gt2.loc[w,LOCUS_LIST[l]+'-maxcomplex%']\n",
    "        # this is the new way I'm trying that subtracts out na %\n",
    "        wl_maxc = gt2.loc[w,LOCUS_LIST[l]+'-maxcomplex']\n",
    "        wl_nap = gt2.loc[w,LOCUS_LIST[l]+'-na%']\n",
    "        \n",
    "        if wl_maxc == 'na':\n",
    "            if 1 - wl_maxp < mthresh[l]:\n",
    "                gt2.at[w,LOCUS_LIST[l]+'_g'] = gt2.loc[w,LOCUS_LIST[l]+'-maxcomplex']\n",
    "            else:\n",
    "                #gt2.at[w,LOCUS_LIST[l]+'_g'] = 'mixed @ '+str(mthresh[l])\n",
    "                gt2.at[w,LOCUS_LIST[l]+'_g'] = 'mixed'\n",
    "        else:\n",
    "            if 1 - wl_maxp - wl_nap < mthresh[l]:\n",
    "                gt2.at[w,LOCUS_LIST[l]+'_g'] = gt2.loc[w,LOCUS_LIST[l]+'-maxcomplex']\n",
    "            # this is the old way I tried\n",
    "            #if 1 - wl_maxp < mthresh:\n",
    "            #    gt2.at[w,LOCUS_LIST[l]+'_g'] = gt2.loc[w,LOCUS_LIST[l]+'-maxcomplex']\n",
    "            else:\n",
    "                #gt2.at[w,LOCUS_LIST[l]+'_g'] = 'mixed @ '+str(mthresh[l])\n",
    "                gt2.at[w,LOCUS_LIST[l]+'_g'] = 'mixed'\n",
    "#export_csv = gt2.to_csv(r'20210106_gt2.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have another trove of single-locus non-pooled sequencing data from our Dec 7 2020 MiSeq run\n",
    "# Let's use that to supplement our assessment from the main body of data.\n",
    "\n",
    "# Previously, I've parsed the fastq files into counts files. That's very good.\n",
    "# I need to import these counts files and assign them to well names and synthesize into a table\n",
    "# for quick recall and mapping.\n",
    "\n",
    "# import filenames to work off of\n",
    "\n",
    "filenames = pd.read_csv(\"CRISPR_10xmer-g_array_singlelociguys.csv\")\n",
    "\n",
    "# import all data into a dataframe\n",
    "\n",
    "singlecounts = pd.DataFrame()\n",
    "singlecounts['BC'] = \"\"\n",
    "singlecounts['Reads'] = \"\"\n",
    "singlecounts['UMI.Count'] = \"\"\n",
    "singlecounts['Library'] = \"\"\n",
    "singlecounts['Well'] = \"\"\n",
    "singlecounts['Locus'] = \"\"\n",
    "\n",
    "for f in np.arange(len(filenames)):\n",
    "    temptable = pd.read_csv(\"20201207_output_miseq/counts/\"+filenames.loc[f,'Library']+\"_counts.csv\")\n",
    "    temptable['Library'] = filenames.loc[f,'Library']\n",
    "    temptable['Well'] = filenames.loc[f,'Well']\n",
    "    temptable['Locus'] = filenames.loc[f,'Locus']\n",
    "    singlecounts = singlecounts.append(temptable)\n",
    "    \n",
    "singlecounts = singlecounts.reset_index(drop=True)\n",
    "\n",
    "# Parse the barcode name for locus\n",
    "singlecounts[\"parsedLocus\"] = singlecounts.BC.str[7:11]\n",
    "singlecounts[\"Allele\"] = singlecounts.BC.str[12:]\n",
    "\n",
    "# Delete all those rows where the parsed locus name matches the supposed locus\n",
    "singlecounts['locusmatch?'] = singlecounts['Locus'] == singlecounts['parsedLocus']\n",
    "singlecounts = singlecounts.loc[(singlecounts['locusmatch?'] == True)].reset_index(drop=True)\n",
    "\n",
    "# Delete useless fields\n",
    "singlecounts = singlecounts.drop(columns=['BC','UMI.Count','Library','parsedLocus','locusmatch?'])\n",
    "\n",
    "# Re-import allelelist to re-obtain the na alleles\n",
    "allelenames = pd.read_csv(\"20201103_allelelist_v4.csv\")\n",
    "an = allelenames.drop(columns = ['Locus'])\n",
    "\n",
    "# Map the allele list to singlecounts\n",
    "singlecounts = pd.merge(singlecounts,an,on='Allele',how='left')\n",
    "\n",
    "# add in frequencies for each well-locus\n",
    "scwl = sorted(list(OrderedDict.fromkeys(singlecounts['Well'])))\n",
    "scll = sorted(list(OrderedDict.fromkeys(singlecounts['Locus'])))\n",
    "\n",
    "sc2 = pd.DataFrame()\n",
    "\n",
    "for w in np.arange(len(scwl)):\n",
    "    for l in np.arange(len(scll)):\n",
    "        tt = singlecounts.loc[(singlecounts['Well'] == scwl[w])&(singlecounts['Locus'] == scll[l])].reset_index(drop=True)\n",
    "        ttreadsum = tt['Reads'].sum()\n",
    "        tt['singlelocus-freq'] = tt['Reads']/ttreadsum\n",
    "        sc2 = sc2.append(tt)\n",
    "\n",
    "# Now we want to get things in the same format as we have with the pooled genotypes, such\n",
    "# that we can merge.\n",
    "\n",
    "# Now get in format for proper genotyping\n",
    "\n",
    "sc3 = pd.DataFrame()\n",
    "\n",
    "sc3['Well'] = \"\"\n",
    "\n",
    "for l in np.arange(len(scll)):\n",
    "    sc3[scll[l]+'-singlelocus-totalreads'] = ''\n",
    "    for c in np.arange(len(c_list)):\n",
    "        sc3[scll[l]+'-singlelocus-'+c_list[c]+'%'] = \"\"\n",
    "    sc3[scll[l]+'-singlelocus-maxcomplex'] = ''\n",
    "    sc3[scll[l]+'-singlelocus-maxcomplex%'] = ''\n",
    "\n",
    "#for w in np.arange(0,1):\n",
    "for w in np.arange(len(scwl)):\n",
    "    sc3.at[w,'Well'] = scwl[w]\n",
    "    wt = sc2.loc[(sc2['Well'] == scwl[w])].reset_index(drop=True)\n",
    "    #for l in np.arange(0,1):\n",
    "    for l in np.arange(len(scll)):\n",
    "        wlt = wt.loc[(wt['Locus'] == scll[l])].reset_index(drop=True)\n",
    "        wlts = wlt.groupby('Complex').sum().reset_index()\n",
    "        sc3.at[w,scll[l]+'-singlelocus-totalreads'] = wlts['Reads'].sum()\n",
    "        \n",
    "        cmax = ''\n",
    "        cpmax = 0\n",
    "        for c in np.arange(len(c_list)):\n",
    "            if len(wlts.loc[(wlts['Complex'] == c_list[c])]) == 0:\n",
    "                sc3.at[w,scll[l]+'-singlelocus-'+c_list[c]+'%'] = 0\n",
    "            else:\n",
    "                cp = wlts.loc[(wlts['Complex'] == c_list[c])].reset_index(drop=True).loc[0,'singlelocus-freq']\n",
    "                sc3.at[w,scll[l]+'-singlelocus-'+c_list[c]+'%'] = cp\n",
    "                \n",
    "                if cp > cpmax:\n",
    "                    cpmax = cp\n",
    "                    cmax = c_list[c]\n",
    "        \n",
    "        sc3.at[w,scll[l]+'-singlelocus-maxcomplex'] = cmax\n",
    "        sc3.at[w,scll[l]+'-singlelocus-maxcomplex%'] = cpmax\n",
    "\n",
    "        \n",
    "# Do the threshold-based fields\n",
    "for l in np.arange(len(scll)):\n",
    "    sc3[scll[l]+'-singlelocus_threshterm'] = ''\n",
    "    \n",
    "for w in np.arange(len(sc3)):\n",
    "    for l in np.arange(len(scll)):\n",
    "        wl_maxp = sc3.loc[w,scll[l]+'-singlelocus-maxcomplex%']\n",
    "        if wl_maxp != 0:\n",
    "            wl_maxc = sc3.loc[w,scll[l]+'-singlelocus-maxcomplex']\n",
    "            wl_nap = sc3.loc[w,scll[l]+'-singlelocus-na%']\n",
    "\n",
    "            if wl_maxc == 'na':\n",
    "                sc3.at[w,scll[l]+'-singlelocus_threshterm'] = 1 - wl_maxp\n",
    "\n",
    "            else:\n",
    "                sc3.at[w,scll[l]+'-singlelocus_threshterm'] = 1 - wl_maxp - wl_nap\n",
    "\n",
    "for l in np.arange(len(scll)):\n",
    "    sc3[scll[l]+'-singlelocus_g'] = ''\n",
    "\n",
    "mthresh = threshtable.loc[(threshtable['Locus'].isin(scll))].reset_index(drop=True)['threshold'].tolist()\n",
    "\n",
    "for w in np.arange(len(sc3)):\n",
    "    for l in np.arange(len(scll)):\n",
    "        wl_maxp = sc3.loc[w,scll[l]+'-singlelocus-maxcomplex%']\n",
    "        if wl_maxp != 0:\n",
    "            wl_maxc = sc3.loc[w,scll[l]+'-singlelocus-maxcomplex']\n",
    "            wl_nap = sc3.loc[w,scll[l]+'-singlelocus-na%']\n",
    "\n",
    "            if wl_maxc == 'na':\n",
    "                if 1 - wl_maxp < mthresh[l]:\n",
    "                    sc3.at[w,scll[l]+'-singlelocus_g'] = sc3.loc[w,scll[l]+'-singlelocus-maxcomplex']\n",
    "                else:\n",
    "                    sc3.at[w,scll[l]+'-singlelocus_g'] = 'mixed'\n",
    "            else:\n",
    "                if 1 - wl_maxp - wl_nap < mthresh[l]:\n",
    "                    sc3.at[w,scll[l]+'-singlelocus_g'] = sc3.loc[w,scll[l]+'-singlelocus-maxcomplex']\n",
    "                else:\n",
    "                    sc3.at[w,scll[l]+'-singlelocus_g'] = 'mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the single locus data onto the bigger data table\n",
    "gt3 = pd.merge(gt2,sc3,on='Well',how='left')\n",
    "for l in np.arange(len(scll)):\n",
    "    gt3[scll[l]+\"matches single?\"] = \"\"\n",
    "    for w in np.arange(len(gt3)):\n",
    "        if gt3.loc[w,scll[l]+'-singlelocus-maxcomplex'] in c_list:\n",
    "            gt3.at[w,scll[l]+\"matches single?\"] = gt3.loc[w,scll[l]+'_g'] == gt3.loc[w,scll[l]+'-singlelocus_g']\n",
    "\n",
    "# We're gonna go ahead and assume the single-locus data is definitive over the pool data for a given locus\n",
    "# So that means we're gonna create new fields for updated genotypes, replacing pool genotypes with\n",
    "# single locus genotypes where possible.\n",
    "\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gt3[LOCUS_LIST[l]+'_g-update'] = gt3[LOCUS_LIST[l]+'_g']\n",
    "    if LOCUS_LIST[l] in scll:\n",
    "        temp = gt3.loc[(gt3[LOCUS_LIST[l]+\"matches single?\"] == False)]\n",
    "        tempind = temp.index.values.tolist()\n",
    "        for i in np.arange(len(tempind)):\n",
    "            gt3.at[tempind[i],LOCUS_LIST[l]+'_g-update'] = temp.loc[tempind[i],LOCUS_LIST[l]+'-singlelocus_g']\n",
    "\n",
    "# Now we want these in binary format\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gt3[LOCUS_LIST[l]+'_g-update_bin'] = \"\"\n",
    "    for w in np.arange(len(gt3)):\n",
    "        if gt3.loc[w,LOCUS_LIST[l]+'_g-update'] == 'WT':\n",
    "            gt3.at[w,LOCUS_LIST[l]+'_g-update_bin'] = '0'\n",
    "        elif gt3.loc[w,LOCUS_LIST[l]+'_g-update'] == 'Mut':\n",
    "            gt3.at[w,LOCUS_LIST[l]+'_g-update_bin'] = '1'\n",
    "        else:\n",
    "            gt3.at[w,LOCUS_LIST[l]+'_g-update_bin'] = '2'\n",
    "\n",
    "# And we want to concatenate to form the full genotypes for the 10xmer and 3 extra loci\n",
    "gcollist = []\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gcollist = gcollist + [LOCUS_LIST[l]+'_g-update_bin']\n",
    "gt3['full_g_bin_13'] = gt3[gcollist].agg(''.join, axis=1)\n",
    "gt3['full_g_bin_10'] = gt3[gcollist[:10]].agg(''.join, axis=1)\n",
    "\n",
    "# Map on the expected genotypes\n",
    "expectations = pd.read_csv('20201104_10xmer-g_expectations.csv').applymap(str)\n",
    "gt4 = pd.merge(gt3,expectations,on='Well',how='left')\n",
    "\n",
    "gexplist = []\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gexplist = gexplist + [LOCUS_LIST[l]+'_g_exp']\n",
    "\n",
    "gt4['full_g_bin_10_exp'] = gt4[gexplist[:10]].agg(''.join,axis=1)\n",
    "\n",
    "# Look for matches\n",
    "gt4['full_g_match?'] = gt4['full_g_bin_10'] == gt4['full_g_bin_10_exp']\n",
    "\n",
    "#export_csv = gt4.to_csv(r'20200107_gt4.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ratios of loci within wells as a possible way to check for aneuploidies\n",
    "\n",
    "fig, axes = plt.subplots(nrows=13, ncols=13, sharex='col', sharey='row',figsize=(12,12))\n",
    "\n",
    "for l1 in np.arange(len(LOCUS_LIST)):\n",
    "    for l2 in np.arange(len(LOCUS_LIST)):\n",
    "        axes[l1][l2].scatter(gt4[LOCUS_LIST[l2]+'-totalreads'],\n",
    "                             gt4[LOCUS_LIST[l1]+'-totalreads'],\n",
    "                             s=0.5)\n",
    "        axes[l1][l2].set_xlabel(LOCUS_LIST[l2])\n",
    "        axes[l1][l2].set_ylabel(LOCUS_LIST[l1])\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "\n",
    "delax = []\n",
    "for i in np.arange(len(LOCUS_LIST)):\n",
    "    for j in np.arange(len(LOCUS_LIST)):\n",
    "        if j >= i:\n",
    "            delax = delax + [[i,j]]\n",
    "for pair in np.arange(len(delax)):\n",
    "    fig.delaxes(axes[delax[pair][0]][delax[pair][1]])\n",
    "#plt.savefig('20210118_readratios_v01.jpg',bbox_inches='tight',dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to create a csv mapping barcodes and genotypes to each other for Alex to use in his pipeline for\n",
    "# fitness, epistasis estimation\n",
    "\n",
    "# Import the bc well map\n",
    "bcwellmap = pd.read_csv('CRISPR-10xmer-barcodes-wells-map.csv')\n",
    "\n",
    "# Create a version of gt4 that is just well names and genotypes at the 13 loci and the match? column\n",
    "gformapping = gt4[['Well','full_g_match?']].copy(deep=True)\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    gformapping[LOCUS_LIST[l]+'_g-update_bin'] = gt4[LOCUS_LIST[l]+'_g-update_bin']\n",
    "\n",
    "gformapping = gformapping.copy(deep=True)\n",
    "gformapping = gformapping.loc[(gformapping['full_g_match?'] == True)].reset_index(drop=True)\n",
    "\n",
    "bcwellgmap = pd.merge(gformapping,bcwellmap,on='Well',how='left')\n",
    "bcwellgmap = bcwellgmap.drop(columns=['Well','full_g_match?'])\n",
    "\n",
    "#export_csv = bcwellgmap.to_csv(r'20210112_10xmer-bc-genotype-map.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about an hour to run, watch out!\n",
    "#acfreq = pd.DataFrame()\n",
    "\n",
    "#for w in np.arange(len(well_list)):\n",
    "#    for l in np.arange(len(LOCUS_LIST)):\n",
    "        temptab = allcounts.loc[(allcounts['Well'] == well_list[w])&(allcounts['Locus'] == LOCUS_LIST[l])]\n",
    "        temptab['freq'] = temptab['Reads']/sum(temptab['Reads'])\n",
    "        acfreq = acfreq.append(temptab)\n",
    "    print(well_list[w])\n",
    "\n",
    "acfreq = acfreq.reset_index(drop=True)\n",
    "\n",
    "export_csv = acfreq.to_csv(r'20201208_acfreq.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to get things in the right format now\n",
    "\n",
    "# create a shell dataframe\n",
    "genotypes = pd.DataFrame()\n",
    "genotypes['Well'] = ''\n",
    "\n",
    "#define alleles of interest, complexes of interest\n",
    "aoi = ['psWT','Mut','BY']\n",
    "coi = ['WT','Mut','na']\n",
    "\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    genotypes[LOCUS_LIST[l]+'_totalreads'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_topallele'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_topallelename'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_topfreq'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_topcount'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_psWTfreq'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_psWTcount'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_Mutfreq'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_Mutcount'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_BYfreq'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_BYcount'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_WTcomplexfreq'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_WTcomplexcount'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_Mutcomplexfreq'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_Mutcomplexcount'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_nacomplexfreq'] = ''\n",
    "    genotypes[LOCUS_LIST[l]+'_nacomplexcount'] = ''\n",
    "\n",
    "    #genotypes[LOCUS_LIST[l]+'_2ndallele'] = ''\n",
    "    #genotypes[LOCUS_LIST[l]+'_2ndfreq'] = ''\n",
    "    #genotypes[LOCUS_LIST[l]+'_2ndcount'] = ''\n",
    "    #genotypes[LOCUS_LIST[l]+'_errfreq'] = ''\n",
    "\n",
    "    \n",
    "# look at each locus and parse the info\n",
    "for w in np.arange(len(well_list)):\n",
    "    welltable = acfreq.loc[(acfreq['Well'] == well_list[w])].reset_index(drop=True)\n",
    "    rowlist = [well_list[w]]\n",
    "    for l in np.arange(len(LOCUS_LIST)):\n",
    "        subtable = welltable.loc[(welltable['Locus'] == LOCUS_LIST[l])].sort_values(by=['freq'],ascending=False).reset_index(drop=True)\n",
    "        # get the total reads\n",
    "        rowlist = rowlist + [subtable['Reads'].sum()]\n",
    "        # get the \"topallele\" stats in gear\n",
    "        if len(subtable) > 0:\n",
    "            topa = subtable.loc[0,'Allele']\n",
    "            topan = subtable.loc[0,'Allele name']\n",
    "            topf = subtable.loc[0,'freq']\n",
    "            topr = subtable.loc[0,'Reads']\n",
    "            rowlist = rowlist + [topa,topan,topf,topr]\n",
    "        else:\n",
    "            rowlist = rowlist + [np.nan,np.nan,np.nan,np.nan]\n",
    "        # get the stats for the alleles of interest\n",
    "        for a in np.arange(len(aoi)):\n",
    "            aoitable = subtable.loc[(subtable['Allele name'] == aoi[a])].reset_index(drop=True)\n",
    "            if len(aoitable) > 0:\n",
    "                rowlist = rowlist + [aoitable.loc[0,'freq'],aoitable.loc[0,'Reads']]\n",
    "            else:\n",
    "                rowlist = rowlist + [np.nan,np.nan]\n",
    "        # get the stats for the complexes of interest\n",
    "        ctable = subtable.groupby((['Complex'])).sum()\n",
    "        for c in np.arange(len(coi)):\n",
    "            if len(subtable.loc[(subtable['Complex'] == coi[c])]) > 0:\n",
    "                rowlist = rowlist + [ctable.loc[coi[c],'freq'],ctable.loc[coi[c],'Reads']]\n",
    "            else:\n",
    "                rowlist = rowlist + [np.nan,np.nan]\n",
    "        \n",
    "    genotypes = genotypes.append(pd.DataFrame([rowlist],columns=list(genotypes)),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some useful columns to genotypes table\n",
    "blankslist = pd.read_csv(\"20201104_blankslist.csv\")\n",
    "expectations = pd.read_csv('20201104_10xmer-g_expectations.csv')\n",
    "\n",
    "genotypes = pd.merge(genotypes,blankslist,on='Well',how='left')\n",
    "genotypes = pd.merge(genotypes,expectations,on='Well',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a genotype guess set of columns for ease, all in one place\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    genotypes[LOCUS_LIST[l]+'_g'] = genotypes[LOCUS_LIST[l]+'_topallelename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's export and play with genotypes\n",
    "export_csv = genotypes.to_csv(r'20201115_genotypes.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to remove suspected chimeric reads.\n",
    "\n",
    "# This will require some initial field additions to the acfreq table.\n",
    "# First, I want to add a field that allows us to see not just the \"Complex\" but also the \"ComplexsepBY\".\n",
    "# Then, add a field that signifies the pool to which each well belongs.\n",
    "# Then, add a field saying whether a well is blank or not, and DROP blank wells.\n",
    "\n",
    "acfreqmod = acfreq.copy(deep=True)\n",
    "acfreqmod['ComplexsepBY'] = \"\"\n",
    "acfreqmod['pool'] = \"\"\n",
    "\n",
    "for i in np.arange(len(acfreqmod)):\n",
    "    if acfreqmod.loc[i,'Allele name'] == 'BY':\n",
    "        acfreqmod.at[i,'ComplexsepBY'] = 'BY'\n",
    "    else:\n",
    "        acfreqmod.at[i,'ComplexsepBY'] = acfreqmod.loc[i,'Complex']\n",
    "    numr = float(acfreqmod.loc[i,'Well'][0])\n",
    "    letr = acfreqmod.loc[i,'Well'][1]\n",
    "    wellr = acfreqmod.loc[i,'Well'][2:]\n",
    "    if numr < 5:\n",
    "        acfreqmod.at[i,'pool'] = letterslash[letters14.index(letr)]+wellr\n",
    "    else:\n",
    "        acfreqmod.at[i,'pool'] = letterslash[letters58.index(letr)]+wellr\n",
    "\n",
    "acfreqmod = pd.merge(acfreqmod,blankslist,on='Well',how='left')\n",
    "acfreqmod = acfreqmod.loc[(acfreqmod['blank?']=='no')].reset_index(drop=True)\n",
    "acfreqmod['Well_Allele'] = acfreqmod['Well']+'_'+acfreqmod['Allele']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_csv = acfreqmod.to_csv(r'20201219_acfreqmod.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing our read removal algorithm, we create test sets of different pools\n",
    "\n",
    "opptab = pd.DataFrame.from_dict({'ComplexsepBY': ['WT','Mut'], 'Oppcomp': ['Mut','WT']})\n",
    "\n",
    "# Create a comprehensive list of pools\n",
    "poollist = list(OrderedDict.fromkeys(acfreqmod['pool']))\n",
    "\n",
    "# The outer loop goes pool by pool.\n",
    "for p in np.arange(244):\n",
    "#for p in np.arange(len(poollist)):\n",
    "    \n",
    "    # Isolate all the wells/loci/alleles for a given pool\n",
    "    subt = acfreqmod.loc[(acfreqmod['pool'] == poollist[p])].reset_index(drop=True)\n",
    "    \n",
    "    # Look one locus at a time\n",
    "    for l in np.arange(9,10):\n",
    "    #for l in np.arange(len(LOCUS_LIST)):\n",
    "        \n",
    "        subtl = subt.loc[(subt['Locus'] == LOCUS_LIST[l])].reset_index(drop=True)\n",
    "        \n",
    "        # Create a new table (\"pie\") to house frequencies of each allele at that locus\n",
    "        pie = subtl.drop(columns=['Reads','Well','freq','blank?']).drop_duplicates().reset_index(drop=True)\n",
    "        readr = subtl.groupby('Allele')['Reads'].sum().to_frame()\n",
    "        pie = pd.merge(pie,readr,on='Allele',how='left')\n",
    "        pie['poolfreq_allele'] = pie['Reads']/sum(pie['Reads'])\n",
    "        \n",
    "        piec = pie.groupby('ComplexsepBY')['poolfreq_allele'].sum().to_frame().reset_index()\n",
    "        \n",
    "        # Figure out the \"Complex\" composition of each well at this locus\n",
    "        # Need to format complextable with complexes absent from the wells as placeholders still.\n",
    "        subtl_wells = sorted(list(OrderedDict.fromkeys(subtl['Well'])))\n",
    "        complexes = ['WT','Mut','BY','Other','na']\n",
    "        completer = pd.DataFrame()\n",
    "        completer['Well'] = \"\"\n",
    "        completer['ComplexsepBY'] = \"\"\n",
    "        for w in np.arange(len(subtl_wells)):\n",
    "            subcompleter = pd.DataFrame()\n",
    "            subcompleter['ComplexsepBY'] = complexes\n",
    "            subcompleter['Well'] = subtl_wells[w]\n",
    "            completer = completer.append(subcompleter)\n",
    "            \n",
    "        complextable = subtl.groupby(['Well','ComplexsepBY']).sum().reset_index()\n",
    "        complextable = pd.merge(completer,complextable,on=['Well','ComplexsepBY'],how='left')\n",
    "        \n",
    "        nanind = complextable[pd.isnull(complextable['Reads'])].index.values\n",
    "        complextable.at[nanind,'Reads'] = 0\n",
    "        complextable.at[nanind,'freq'] = 0\n",
    "        \n",
    "        # Create a table where we store the info on the leading (max_freq) complex for each well\n",
    "        lc = subtl.groupby(['Well','ComplexsepBY']).sum().reset_index()\n",
    "        lc = lc[lc.groupby(['Well'])['freq'].transform(max) == lc['freq']]\n",
    "        \n",
    "        # For the purpose of calculating the % of chimeric reads, use only those for which lead complex is WT or Mut\n",
    "        lc_forav = lc.loc[(lc['ComplexsepBY'] == 'WT')].append(lc.loc[(lc['ComplexsepBY'] == 'Mut')])\n",
    "        \n",
    "        # Create a field to hold the opposite complex name\n",
    "        lc_forav = pd.merge(lc_forav,opptab,on='ComplexsepBY',how='left')\n",
    "        \n",
    "        # Map the frequencies of the Oppcomp onto the lc_forav table\n",
    "        lc_forav = pd.merge(lc_forav,complextable,left_on=['Well','Oppcomp'],right_on=['Well','ComplexsepBY'],how='left',\n",
    "                            suffixes=('','_opp')).drop(columns=['Oppcomp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time for the big loop to make the read removal happen.\n",
    "\n",
    "# This is a df for later...\n",
    "opptab = pd.DataFrame.from_dict({'ComplexsepBY': ['WT','Mut'], 'Oppcomp': ['Mut','WT']})\n",
    "\n",
    "# And this is the threshold at which we will say that a well has too many reads (by %) that are \"actually\" bad.\n",
    "# Here, \"actually bad\" means something slightly different than \"chimeric.\" It means \"having an undesirable\n",
    "# phenotype.\" So there may be things we don't want to use for the purposes of the algorithm below (e.g.,\n",
    "# a well that's supposed to be psWT that's actually mostly BY). The divergence between this definition of \"bad\"\n",
    "# and the deeper definition of \"chimeric\" buried in the algorithm might result in some unsavory inconsistencies\n",
    "# or loops, so keep an eye out.\n",
    "malthresh = 0.05\n",
    "\n",
    "# Create a comprehensive list of pools\n",
    "poollist = list(OrderedDict.fromkeys(acfreqmod['pool']))\n",
    "\n",
    "# Create a place to put all information generated by the algorithm that we might want to refer back to.\n",
    "# This will contain more info than needed I expect, historical info for each iteration included,\n",
    "# so make sure to filter on that as appropriate during analysis.\n",
    "piestore = pd.DataFrame()\n",
    "freqstore = pd.DataFrame()\n",
    "\n",
    "# The outer loop goes pool by pool.\n",
    "for p in np.arange(1):\n",
    "#for p in np.arange(len(poollist)):\n",
    "    \n",
    "    # Isolate all the wells/loci/alleles for a given pool\n",
    "    subt = acfreqmod.loc[(acfreqmod['pool'] == poollist[p])].reset_index(drop=True)\n",
    "    \n",
    "    # Look one locus at a time\n",
    "    for l in np.arange(8,9):\n",
    "    #for l in np.arange(len(LOCUS_LIST)):\n",
    "        \n",
    "        loopflag = 0\n",
    "        itcount = 0\n",
    "        \n",
    "        subtl = subt.loc[(subt['Locus'] == LOCUS_LIST[l])].reset_index(drop=True)\n",
    "        \n",
    "        # Create a new table (\"pie\") to house frequencies of each allele at that locus\n",
    "        pie = subtl.drop(columns=['Reads','Well','freq','blank?']).drop_duplicates().reset_index(drop=True)\n",
    "        readr = subtl.groupby('Allele')['Reads'].sum().to_frame()\n",
    "        pie = pd.merge(pie,readr,on='Allele',how='left')\n",
    "        pie['poolfreq_allele'] = pie['Reads']/sum(pie['Reads'])\n",
    "        \n",
    "        piestore = piestore.append(pie)\n",
    "        \n",
    "        piec = pie.groupby('ComplexsepBY')['poolfreq_allele'].sum().to_frame().reset_index()\n",
    "        \n",
    "        # Figure out the \"Complex\" composition of each well at this locus\n",
    "        # Need to format complextable with complexes absent from the wells as placeholders still.\n",
    "        subtl_wells = sorted(list(OrderedDict.fromkeys(subtl['Well'])))\n",
    "        complexes = ['WT','Mut','BY','Other','na']\n",
    "        completer = pd.DataFrame()\n",
    "        completer['Well'] = \"\"\n",
    "        completer['ComplexsepBY'] = \"\"\n",
    "        for w in np.arange(len(subtl_wells)):\n",
    "            subcompleter = pd.DataFrame()\n",
    "            subcompleter['ComplexsepBY'] = complexes\n",
    "            subcompleter['Well'] = subtl_wells[w]\n",
    "            completer = completer.append(subcompleter)\n",
    "            \n",
    "        complextable = subtl.groupby(['Well','ComplexsepBY']).sum().reset_index()\n",
    "        complextable = pd.merge(completer,complextable,on=['Well','ComplexsepBY'],how='left')\n",
    "        \n",
    "        nanind = complextable[pd.isnull(complextable['Reads'])].index.values\n",
    "        complextable.at[nanind,'Reads'] = 0\n",
    "        complextable.at[nanind,'freq'] = 0\n",
    "        \n",
    "        # Create a table where we store the info on the leading (max_freq) complex for each well\n",
    "        lc = subtl.groupby(['Well','ComplexsepBY']).sum().reset_index()\n",
    "        lc = lc[lc.groupby(['Well'])['freq'].transform(max) == lc['freq']]\n",
    "        \n",
    "        # For the purpose of calculating the % of chimeric reads, use only those for which lead complex is WT or Mut\n",
    "        lc_forav = lc.loc[(lc['ComplexsepBY'] == 'WT')].append(lc.loc[(lc['ComplexsepBY'] == 'Mut')])\n",
    "        \n",
    "        # Create a field to hold the opposite complex name\n",
    "        lc_forav = pd.merge(lc_forav,opptab,on='ComplexsepBY',how='left')\n",
    "        \n",
    "        # Map the frequencies of the Oppcomp onto the lc_forav table\n",
    "        lc_forav = pd.merge(lc_forav,complextable,left_on=['Well','Oppcomp'],right_on=['Well','ComplexsepBY'],how='left',\n",
    "                            suffixes=('','_opp')).drop(columns=['Oppcomp'])\n",
    "        \n",
    "        # Replace NaNs with 0\n",
    "        #lc_forav[pd.isnull(lc_forav['Reads_opp'])] = 0\n",
    "        \n",
    "        # Based on the pool-wide \"pie\" of alleles, estimate fraction of chimeric reads in each well\n",
    "        lc_forav['chim_freq'] = \"\"\n",
    "        for w in np.arange(len(lc_forav)):\n",
    "            if len(piec.loc[(piec['ComplexsepBY'] == lc_forav.loc[w,'ComplexsepBY_opp']),'poolfreq_allele']) > 0:\n",
    "                lc_forav.at[w,'chim_freq'] = lc_forav.loc[w,'freq_opp'] / piec.loc[(piec['ComplexsepBY'] == lc_forav.loc[w,'ComplexsepBY_opp']),'poolfreq_allele'].reset_index(drop=True)[0]\n",
    "            else:\n",
    "                lc_forav.at[w,'chim_freq'] = 0\n",
    "            \n",
    "        \n",
    "        # Now take the average of the chim_freq across the wells to treat as the \"true\" chimera frequency across the pool\n",
    "        # IMPORTANT: Changed this to median to be robust to weird small # / small # stuff. Might need more stable solution later.\n",
    "        chim_avg = lc_forav['chim_freq'].median()\n",
    "        \n",
    "        # Now we want to look at each well in turn\n",
    "        judgmentbank = []\n",
    "        freqotherlist = []\n",
    "        freqstoreint = pd.DataFrame()\n",
    "        #for w in np.arange(5,6):\n",
    "        for w in np.arange(len(subtl_wells)):\n",
    "            subtlw = subtl.loc[(subtl['Well'] == subtl_wells[w])].reset_index(drop=True)\n",
    "            \n",
    "            # Get total reads at that locus for that well\n",
    "            totreadslw = subtlw['Reads'].sum()\n",
    "            \n",
    "            # Using the average est. chimera freq and pie, calculate how many reads that is for each allele\n",
    "            pie['toremove_'+subtl_wells[w]] = chim_avg * totreadslw * pie['poolfreq_allele']\n",
    "            piew = pie[['Allele','poolfreq_allele','toremove_'+subtl_wells[w]]]\n",
    "            piew = piew.rename(columns={'toremove_'+subtl_wells[w]:'toremove'})\n",
    "            \n",
    "            # Merge that new DataFrame with the well-locus subtable\n",
    "            subtlw = pd.merge(subtlw,piew,on='Allele',how='left')\n",
    "            \n",
    "            # Subtract these putatively chimeric reads from the raw reads for each allele,\n",
    "            # being sure not to go negative.\n",
    "            subtlw['Reads_adj'] = subtlw['Reads'] - subtlw['toremove']\n",
    "            nanind2 = subtlw[subtlw['Reads_adj'] < 0].index.values\n",
    "            subtlw.at[nanind2,'Reads_adj'] = 0\n",
    "            \n",
    "            # Calculate the old and new % of reads at each locus in the well\n",
    "            subtlw['wellfreq_allele'] = subtlw['Reads']/sum(subtlw['Reads'])\n",
    "            subtlw['wellfreq_allele_adj'] = subtlw['Reads_adj']/sum(subtlw['Reads_adj'])\n",
    "            \n",
    "            # for storage purposes\n",
    "            subtlw['iteration'] = itcount\n",
    "            \n",
    "            # Evaluate whether this given well is \"bad\" and store that info\n",
    "            lcw = subtlw.groupby(['Complex']).sum().reset_index()\n",
    "            lcw = lcw.sort_values(by=['wellfreq_allele_adj'],ascending=False).reset_index(drop=True)\n",
    "            freqother = 1 - lcw.loc[0,'wellfreq_allele_adj']\n",
    "            if freqother > malthresh:\n",
    "                judgmentbank = judgmentbank + ['bad (mixed)']\n",
    "                subtlw['judgment'] = 'bad (mixed)'\n",
    "                subtlw['judgment_freqother'] = freqother\n",
    "            else:\n",
    "                judgmentbank = judgmentbank + ['good']\n",
    "                subtlw['judgment'] = 'good'\n",
    "                subtlw['judgment_freqother'] = freqother\n",
    "            freqotherlist = freqotherlist + [freqother]\n",
    "            freqstoreint = freqstoreint.append(subtlw)\n",
    "                \n",
    "        # Ask if any are bad (mixed). If not, proceed to the next pool-locus.\n",
    "        # If so, repeat the process for this pool-locus, but remove the one (worst) problem well from the pool\n",
    "        # in the part of the algorithm that figures out the average chimerism.\n",
    "        \n",
    "        if judgmentbank.count('bad (mixed)') == 0:\n",
    "            loopflag = 1\n",
    "            freqstoreint['final_it?'] = \"yes\"\n",
    "            freqstore = freqstore.append(freqstoreint)\n",
    "            \n",
    "        while loopflag == 0:\n",
    "            \n",
    "            freqstoreint['final_it?'] = \"no\"\n",
    "            freqstore = freqstore.append(freqstoreint)\n",
    "            \n",
    "            itcount = itcount+1\n",
    "            \n",
    "            dfh = pd.DataFrame()\n",
    "            dfh['Well'] = subtl_wells\n",
    "            dfh['judge'] = judgmentbank\n",
    "            dfh['freq-not-lead-complex'] = freqotherlist\n",
    "            dfhbad = dfh.loc[(dfh['judge'] == 'bad (mixed)')].sort_values(by=['freq-not-lead-complex'],ascending=False).reset_index(drop=True)\n",
    "            subtl_wells = subtl_wells.remove(dfhbad.loc[0,'Well'])\n",
    "             \n",
    "            pie = pie[['Locus','Allele','Allele name','Complex','ComplexsepBY','pool','Reads','poolfreq_allele']]\n",
    "            \n",
    "            lc_forav = lc_forav.loc[(lc_forav['Well'] != dfhbad.loc[0,'Well'])].reset_index(drop=True)\n",
    "\n",
    "            # Now take the average of the chim_freq across the wells to treat as the \"true\" chimera frequency across the pool\n",
    "            # IMPORTANT: As above, using median for now, may want to change approach in future.\n",
    "            chim_avg = lc_forav['chim_freq'].median()\n",
    "\n",
    "            # Now we want to look at each well in turn\n",
    "            judgmentbank = []\n",
    "            freqotherlist = []\n",
    "            freqstoreint = pd.DataFrame()\n",
    "            #for w in np.arange(1,2):\n",
    "            for w in np.arange(len(subtl_wells)):\n",
    "                subtlw = subtl.loc[(subtl['Well'] == subtl_wells[w])].reset_index(drop=True)\n",
    "\n",
    "                # Get total reads at that locus for that well\n",
    "                totreadslw = subtlw['Reads'].sum()\n",
    "\n",
    "                # Using the average est. chimera freq and pie, calculate how many reads that is for each allele\n",
    "                pie['toremove_'+subtl_wells[w]] = chim_avg * totreadslw * pie['poolfreq_allele']\n",
    "                piew = pie[['Allele','poolfreq_allele','toremove_'+subtl_wells[w]]]\n",
    "                piew = piew.rename(columns={'toremove_'+subtl_wells[w]:'toremove'})\n",
    "\n",
    "                # Merge that new DataFrame with the well-locus subtable\n",
    "                subtlw = pd.merge(subtlw,piew,on='Allele',how='left')\n",
    "\n",
    "                # Subtract these putatively chimeric reads from the raw reads for each allele,\n",
    "                # being sure not to go negative.\n",
    "                subtlw['Reads_adj'] = subtlw['Reads'] - subtlw['toremove']\n",
    "                nanind2 = subtlw[subtlw['Reads_adj'] < 0].index.values\n",
    "                subtlw.at[nanind2,'Reads_adj'] = 0\n",
    "\n",
    "                # Calculate the old and new % of reads at each locus in the well\n",
    "                subtlw['wellfreq_allele'] = subtlw['Reads']/sum(subtlw['Reads'])\n",
    "                subtlw['wellfreq_allele_adj'] = subtlw['Reads_adj']/sum(subtlw['Reads_adj'])\n",
    "\n",
    "                # for storage purposes\n",
    "                subtlw['iteration'] = itcount\n",
    "\n",
    "                # Evaluate whether this given well is \"bad\" and store that info\n",
    "                lcw = subtlw.groupby(['Complex']).sum().reset_index()\n",
    "                lcw = lcw.sort_values(by=['wellfreq_allele_adj'],ascending=False).reset_index(drop=True)\n",
    "                freqother = 1 - lcw.loc[0,'wellfreq_allele_adj']\n",
    "                if freqother > malthresh:\n",
    "                    judgmentbank = judgmentbank + ['bad (mixed)']\n",
    "                    subtlw['judgment'] = 'bad (mixed)'\n",
    "                    subtlw['judgment_freqother'] = freqother\n",
    "                else:\n",
    "                    judgmentbank = judgmentbank + ['good']\n",
    "                    subtlw['judgment'] = 'good'\n",
    "                    subtlw['judgment_freqother'] = freqother\n",
    "                freqotherlist = freqotherlist + [freqother]\n",
    "                freqstoreint = freqstoreint.append(subtlw)\n",
    "                \n",
    "            if judgmentbank.count('bad (mixed)') == 0:\n",
    "                freqstoreint['final_it?'] = \"yes\"\n",
    "                freqstore = freqstore.append(freqstoreint)\n",
    "                loopflag = 1\n",
    "                \n",
    "            while loopflag == 0:\n",
    "\n",
    "                freqstoreint['final_it?'] = \"no\"\n",
    "                freqstore = freqstore.append(freqstoreint)\n",
    "\n",
    "                itcount = itcount+1\n",
    "\n",
    "                dfh = pd.DataFrame()\n",
    "                dfh['Well'] = subtl_wells\n",
    "                dfh['judge'] = judgmentbank\n",
    "                dfh['freq-not-lead-complex'] = freqotherlist\n",
    "                dfhbad = dfh.loc[(dfh['judge'] == 'bad (mixed)')].sort_values(by=['freq-not-lead-complex'],ascending=False).reset_index(drop=True)\n",
    "                subtl_wells = subtl_wells.remove(dfhbad.loc[0,'Well'])\n",
    "\n",
    "                pie = pie[['Locus','Allele','Allele name','Complex','ComplexsepBY','pool','Reads','poolfreq_allele']]\n",
    "\n",
    "                lc_forav = lc_forav.loc[(lc_forav['Well'] != dfhbad.loc[0,'Well'])].reset_index(drop=True)\n",
    "\n",
    "                # Now take the average of the chim_freq across the wells to treat as the \"true\" chimera frequency across the pool\n",
    "                # IMPORTANT: As above, using median for now, may want to change approach in future.\n",
    "                chim_avg = lc_forav['chim_freq'].median()\n",
    "\n",
    "                # Now we want to look at each well in turn\n",
    "                judgmentbank = []\n",
    "                freqotherlist = []\n",
    "                freqstoreint = pd.DataFrame()\n",
    "                #for w in np.arange(1,2):\n",
    "                for w in np.arange(len(subtl_wells)):\n",
    "                    subtlw = subtl.loc[(subtl['Well'] == subtl_wells[w])].reset_index(drop=True)\n",
    "\n",
    "                    # Get total reads at that locus for that well\n",
    "                    totreadslw = subtlw['Reads'].sum()\n",
    "\n",
    "                    # Using the average est. chimera freq and pie, calculate how many reads that is for each allele\n",
    "                    pie['toremove_'+subtl_wells[w]] = chim_avg * totreadslw * pie['poolfreq_allele']\n",
    "                    piew = pie[['Allele','poolfreq_allele','toremove_'+subtl_wells[w]]]\n",
    "                    piew = piew.rename(columns={'toremove_'+subtl_wells[w]:'toremove'})\n",
    "\n",
    "                    # Merge that new DataFrame with the well-locus subtable\n",
    "                    subtlw = pd.merge(subtlw,piew,on='Allele',how='left')\n",
    "\n",
    "                    # Subtract these putatively chimeric reads from the raw reads for each allele,\n",
    "                    # being sure not to go negative.\n",
    "                    subtlw['Reads_adj'] = subtlw['Reads'] - subtlw['toremove']\n",
    "                    nanind2 = subtlw[subtlw['Reads_adj'] < 0].index.values\n",
    "                    subtlw.at[nanind,'Reads_adj'] = 0\n",
    "\n",
    "                    # Calculate the old and new % of reads at each locus in the well\n",
    "                    subtlw['wellfreq_allele'] = subtlw['Reads']/sum(subtlw['Reads'])\n",
    "                    subtlw['wellfreq_allele_adj'] = subtlw['Reads_adj']/sum(subtlw['Reads_adj'])\n",
    "\n",
    "                    # for storage purposes\n",
    "                    subtlw['iteration'] = itcount\n",
    "\n",
    "                    # Evaluate whether this given well is \"bad\" and store that info\n",
    "                    lcw = subtlw.groupby(['Complex']).sum().reset_index()\n",
    "                    lcw = lcw.sort_values(by=['wellfreq_allele_adj'],ascending=False).reset_index(drop=True)\n",
    "                    freqother = 1 - lcw.loc[0,'wellfreq_allele_adj']\n",
    "                    if freqother > malthresh:\n",
    "                        judgmentbank = judgmentbank + ['bad (mixed)']\n",
    "                        subtlw['judgment'] = 'bad (mixed)'\n",
    "                        subtlw['judgment_freqother'] = freqother\n",
    "                    else:\n",
    "                        judgmentbank = judgmentbank + ['good']\n",
    "                        subtlw['judgment'] = 'good'\n",
    "                        subtlw['judgment_freqother'] = freqother\n",
    "                    freqotherlist = freqotherlist + [freqother]\n",
    "                    freqstoreint = freqstoreint.append(subtlw)\n",
    "\n",
    "                if judgmentbank.count('bad (mixed)') == 0:\n",
    "                    freqstoreint['final_it?'] = \"yes\"\n",
    "                    freqstore = freqstore.append(freqstoreint)\n",
    "                    loopflag = 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I believe the below may all be deprecated, but forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_forav = lc.loc[(lc['ComplexsepBY'] == 'WT')].append(lc.loc[(lc['ComplexsepBY'] == 'Mut')])\n",
    "        \n",
    "# Create a field to hold the opposite complex name\n",
    "lc_forav = pd.merge(lc_forav,opptab,on='ComplexsepBY',how='left')\n",
    "\n",
    "# Map the frequencies of the Oppcomp onto the lc_forav table\n",
    "lc_forav = pd.merge(lc_forav,complextable,left_on=['Well','Oppcomp'],right_on=['Well','ComplexsepBY'],how='left',\n",
    "                    suffixes=('','_opp')).drop(columns=['Oppcomp'])\n",
    "\n",
    "lc_forav\n",
    "lc_forav['chim_freq'] = \"\"\n",
    "lc_forav.loc[w,'freq_opp']\n",
    "len(piec.loc[(piec['ComplexsepBY'] == lc_forav.loc[w,'ComplexsepBY_opp']),'poolfreq_allele'])\n",
    "#for w in np.arange(len(lc_forav)):\n",
    "#    lc_forav.at[w,'chim_freq'] = lc_forav.loc[w,'freq_opp'] / piec.loc[(piec['ComplexsepBY'] == lc_forav.loc[w,'ComplexsepBY_opp']),'poolfreq_allele'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# before any further read pruning, let's evaluate the chimera test data set\n",
    "\n",
    "# first, import the chimera test libraries\n",
    "# import filenames to work off of\n",
    "\n",
    "filenamesc = pd.read_csv(\"CRISPR_10xmer-g-followup_array_v01_chim.csv\")\n",
    "\n",
    "\n",
    "# import all data into a dataframe\n",
    "\n",
    "allcountsc = pd.DataFrame()\n",
    "allcountsc['BC'] = \"\"\n",
    "allcountsc['Reads'] = \"\"\n",
    "allcountsc['UMI.Count'] = \"\"\n",
    "allcountsc['Library'] = \"\"\n",
    "\n",
    "for f in np.arange(len(filenamesc)):\n",
    "    temptable = pd.read_csv(\"20201207_output_miseq/counts/\"+filenamesc.loc[f,'Library']+\"_counts.csv\")\n",
    "    temptable['Library'] = filenamesc.loc[f,'Library']\n",
    "    allcountsc = allcountsc.append(temptable)\n",
    "    \n",
    "allcountsc = allcountsc.reset_index(drop=True)\n",
    "\n",
    "# Now time to extract the allele identity\n",
    "\n",
    "allcountsc[\"Index\"] = allcountsc.BC.str[:6]\n",
    "allcountsc[\"Locus\"] = allcountsc.BC.str[7:11]\n",
    "allcountsc[\"Allele\"] = allcountsc.BC.str[12:]\n",
    "\n",
    "# Map on the allele name\n",
    "allcountsc = pd.merge(allcountsc,allelenames2,on='Allele',how='left')\n",
    "\n",
    "# Make a column with Library-Index as a proxy for \"Well\" in the allele freq calculation\n",
    "allcountsc['Library-Index'] = allcountsc['Library'] + \"-\" + allcountsc['Index']\n",
    "\n",
    "# make a list of all the Library-Index pairs\n",
    "li_list = list(OrderedDict.fromkeys(allcountsc['Library-Index']))\n",
    "\n",
    "# Calculate allele frequencies\n",
    "acfreqc = pd.DataFrame()\n",
    "\n",
    "for li in np.arange(len(li_list)):\n",
    "    for l in np.arange(len(LOCUS_LIST)):\n",
    "        temptab = allcountsc.loc[(allcountsc['Library-Index'] == li_list[li])&(allcountsc['Locus'] == LOCUS_LIST[l])]\n",
    "        temptab['freq'] = temptab['Reads']/sum(temptab['Reads'])\n",
    "        acfreqc = acfreqc.append(temptab)\n",
    "    #print(li_list[li])\n",
    "\n",
    "acfreqc = acfreqc.reset_index(drop=True)\n",
    "\n",
    "#export_csv = acfreqc.to_csv(r'20201207_acfreqc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to get a sense of the balance between alleles for each locus\n",
    "for l in np.arange(len(LOCUS_LIST)):\n",
    "    temptab = acfreq.loc[(acfreq['Locus'] == LOCUS_LIST[l])]\n",
    "    temptab = temptab.drop(columns=['Complex','Locus','freq','Well','Allele'])\n",
    "    temptab = temptab.groupby((['Allele name'])).sum()\n",
    "    print(LOCUS_LIST[l])\n",
    "    print(temptab)\n",
    "# looks like Mut and psWT alleles predominate, but some loci where significant fraction is one of the inc alleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's filter the table, screening out alleles that take up less than X% of a given locus's reads in a given well\n",
    "\n",
    "percthresh = 0.01\n",
    "\n",
    "acfreq_thresh = acfreq.loc[(acfreq['freq'] >= percthresh)].reset_index(drop=True)\n",
    "\n",
    "#export_csv = acfreq_thresh.to_csv(r'20201103_acfreq_thresh.csv')\n",
    "\n",
    "# Now let's pull out the read counts for each well and locus to see what we can see\n",
    "counttable = acfreq.copy(deep=True).drop(columns=['Allele','Allele name','freq'])\n",
    "counttable = counttable.groupby(['Well','Locus']).sum()\n",
    "\n",
    "#export_csv = counttable.to_csv(r'20201103_well-locus_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking out the regextable for export (2021.09.12)\n",
    "regex_table = pd.DataFrame()\n",
    "regex_table['locus']=['BUL2','FAS1','MKT1','NCS2','PMA1','RHO5','SCH9','WHI2','AKL1','RPI1','HSL7','SPT7','FRS1']\n",
    "regex_table['left']=['CAACACAA','TACCAGGA','ATAATTGT','ATCCTGAA','TGTTTGTC','GTGTGATA','TCATTTCT','ATTTAATG','CTTGATAT','ATGGAAAG','ATCTGAAT','CAACCAAT','GCCAACCA']\n",
    "regex_table['right']=['CTAACGTT','TAATTTAG','CATTATGT','CAGAATCT','TAATAGCA','AAAACGTC','GCCATTAA','TTCACCAA','AAGGTAGT','ATTACTAC','GATTTGCC','CATCCCTG','TCAACGGA']\n",
    "regex_table['bp'] = [20,23,21,20,20,21,21,21,22,20,21,20,20]\n",
    "export_csv = regex_table.to_csv(r'CRISPRgenotyping_regextable.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
